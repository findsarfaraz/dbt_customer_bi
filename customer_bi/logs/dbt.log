[0m01:24:25.703184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30950b4c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30968e8190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3094043c50>]}


============================== 01:24:25.706500 | 1a1b66f4-df3f-419d-9de4-5e1c997b313a ==============================
[0m01:24:25.706500 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:24:25.707058 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'cache_selected_only': 'False', 'debug': 'False', 'log_format': 'default', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'fail_fast': 'False', 'target_path': 'None', 'log_cache_events': 'False', 'warn_error': 'None', 'profiles_dir': '/home/ubuntu/.dbt', 'quiet': 'False', 'empty': 'None', 'indirect_selection': 'eager', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt debug', 'write_json': 'True', 'use_colors': 'True', 'no_print': 'None', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'version_check': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m01:24:25.720901 [info ] [MainThread]: dbt version: 1.11.2
[0m01:24:25.721311 [info ] [MainThread]: python version: 3.13.11
[0m01:24:25.721707 [info ] [MainThread]: python path: /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/bin/python3
[0m01:24:25.722053 [info ] [MainThread]: os info: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
[0m01:24:26.497005 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:24:26.497465 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:24:26.497741 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:24:27.230543 [info ] [MainThread]: Using profiles dir at /home/ubuntu/.dbt
[0m01:24:27.230967 [info ] [MainThread]: Using profiles.yml file at /home/ubuntu/.dbt/profiles.yml
[0m01:24:27.231278 [info ] [MainThread]: Using dbt_project.yml file at /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/dbt_project.yml
[0m01:24:27.231520 [info ] [MainThread]: adapter type: databricks
[0m01:24:27.231765 [info ] [MainThread]: adapter version: 1.11.4
[0m01:24:27.310654 [info ] [MainThread]: Configuration:
[0m01:24:27.311185 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:24:27.311507 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:24:27.311777 [info ] [MainThread]: Required dependencies:
[0m01:24:27.312044 [debug] [MainThread]: Executing "git --help"
[0m01:24:27.316157 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:24:27.316501 [debug] [MainThread]: STDERR: "b''"
[0m01:24:27.316718 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:24:27.316974 [info ] [MainThread]: Connection:
[0m01:24:27.317234 [info ] [MainThread]:   host: adb-3183350029643709.9.azuredatabricks.net
[0m01:24:27.317521 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/a418758e84eae08c
[0m01:24:27.317752 [info ] [MainThread]:   catalog: dev
[0m01:24:27.318030 [info ] [MainThread]:   schema: customer_bi
[0m01:24:27.318414 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:24:27.380670 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:24:27.381279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1a1b66f4-df3f-419d-9de4-5e1c997b313a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f306e578e90>]}
[0m01:24:27.381742 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m01:24:27.381991 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m01:24:27.382211 [debug] [MainThread]: Using databricks connection "debug"
[0m01:24:27.382426 [debug] [MainThread]: On debug: select 1 as id
[0m01:24:27.382612 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:24:29.044642 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0eff0-8141-1a55-8821-a66eea3e8cee) - Created
[0m01:24:35.889102 [debug] [MainThread]: SQL status: OK in 8.510 seconds
[0m01:24:35.891801 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0eff0-8141-1a55-8821-a66eea3e8cee, command-id=01f0eff0-8187-1295-a4b9-f0cb259ba1ac) - Closing
[0m01:24:36.217940 [debug] [MainThread]: On debug: Close
[0m01:24:36.218887 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0eff0-8141-1a55-8821-a66eea3e8cee) - Closing
[0m01:24:36.498579 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m01:24:36.499625 [info ] [MainThread]: [32mAll checks passed![0m
[0m01:24:36.501276 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 10.868009, "process_in_blocks": "197936", "process_kernel_time": 0.343193, "process_mem_max_rss": "250060", "process_out_blocks": "16", "process_user_time": 4.6794}
[0m01:24:36.501958 [debug] [MainThread]: Command `dbt debug` succeeded at 01:24:36.501784 after 10.87 seconds
[0m01:24:36.502462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f306e612580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f306e612ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f306e78b050>]}
[0m01:24:36.502938 [debug] [MainThread]: Flushing usage events
[0m01:24:37.306508 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:41:55.163040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe64ce8cc20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe64e8b4190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe64bfdfc50>]}


============================== 01:41:55.174162 | 1a8ce117-2824-4eea-a9f4-3598ed1da52c ==============================
[0m01:41:55.174162 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:41:55.175107 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt test', 'use_colors': 'True', 'introspect': 'True', 'empty': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': '/home/ubuntu/.dbt', 'indirect_selection': 'eager', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'write_json': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'static_parser': 'True', 'version_check': 'True', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'log_format': 'default', 'target_path': 'None', 'printer_width': '80', 'debug': 'False', 'partial_parse': 'True'}
[0m01:41:56.218563 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:41:56.219069 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:41:56.219373 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:41:57.237702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe64cce0fc0>]}
[0m01:41:57.299306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6263e2140>]}
[0m01:41:57.299962 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:41:57.390776 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:41:57.391539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6265ae050>]}
[0m01:41:57.406868 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m01:41:57.407553 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m01:41:57.408038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6263da6c0>]}
[0m01:41:59.155025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe625dea5f0>]}
[0m01:41:59.212275 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:41:59.215489 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:41:59.377170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6260b5300>]}
[0m01:41:59.378125 [info ] [MainThread]: Found 2 models, 4 data tests, 731 macros
[0m01:41:59.378655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe624535a90>]}
[0m01:41:59.381309 [info ] [MainThread]: 
[0m01:41:59.381732 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:41:59.382088 [info ] [MainThread]: 
[0m01:41:59.382682 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:41:59.382998 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:41:59.391192 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m01:41:59.391869 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m01:41:59.407664 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m01:41:59.408864 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m01:41:59.409412 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:42:00.991688 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff2-f449-13d2-999d-23d3a71a469b) - Created
[0m01:42:08.272078 [debug] [ThreadPool]: SQL status: OK in 8.860 seconds
[0m01:42:08.304853 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eff2-f449-13d2-999d-23d3a71a469b, command-id=01f0eff2-f490-1c55-8589-ea2f8dfad33b) - Closing
[0m01:42:08.743168 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m01:42:08.743704 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff2-f449-13d2-999d-23d3a71a469b) - Closing
[0m01:42:09.058286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe624f399c0>]}
[0m01:42:09.067540 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:42:09.068128 [info ] [Thread-2 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m01:42:09.068848 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m01:42:09.069273 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710'
[0m01:42:09.069634 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:42:09.086430 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:42:09.090558 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:42:09.104215 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:42:09.106063 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:42:09.106441 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:42:09.106710 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:42:09.981967 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-f9b7-1596-9581-4b5ab053b439) - Created
[0m01:42:10.970664 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff2-f9e5-1bcd-b879-1d2de3a0c86c
[0m01:42:10.971476 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m01:42:10.971812 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-f9b7-1596-9581-4b5ab053b439) - Closing
[0m01:42:11.237261 [debug] [Thread-2 (]: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:42:11.237799 [error] [Thread-2 (]: 1 of 4 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 2.17s]
[0m01:42:11.238296 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:42:11.238635 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:42:11.239057 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql.
[0m01:42:11.239435 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m01:42:11.240494 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m01:42:11.240773 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778'
[0m01:42:11.241020 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:42:11.244137 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:42:11.244725 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:42:11.247021 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:42:11.247608 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:42:11.248019 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:42:11.248317 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:42:12.175921 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-fb04-179d-b751-2f4b632e0ff7) - Created
[0m01:42:12.894581 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff2-fb35-1045-a2e7-ab3f55fa1ad9
[0m01:42:12.896624 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: Close
[0m01:42:12.897443 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-fb04-179d-b751-2f4b632e0ff7) - Closing
[0m01:42:13.182336 [debug] [Thread-2 (]: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:42:13.183097 [error] [Thread-2 (]: 2 of 4 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 1.94s]
[0m01:42:13.183739 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:42:13.184150 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:42:13.184962 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql.
[0m01:42:13.184515 [info ] [Thread-2 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m01:42:13.185782 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m01:42:13.186103 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321'
[0m01:42:13.186352 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:42:13.190759 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:42:13.191406 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:42:13.193916 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:42:13.194397 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:42:13.194798 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:42:13.195099 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:42:14.068715 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-fc28-1e61-b74b-044ba6f549c0) - Created
[0m01:42:14.719815 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff2-fc55-1350-b790-0faaa83c558e
[0m01:42:14.721011 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: Close
[0m01:42:14.721622 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-fc28-1e61-b74b-044ba6f549c0) - Closing
[0m01:42:15.012613 [debug] [Thread-2 (]: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:42:15.014125 [error] [Thread-2 (]: 3 of 4 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 1.83s]
[0m01:42:15.015668 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:42:15.016641 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:42:15.017517 [info ] [Thread-2 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m01:42:15.018912 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql.
[0m01:42:15.020413 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m01:42:15.021875 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493'
[0m01:42:15.022833 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:42:15.036822 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:42:15.038776 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:42:15.043100 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:42:15.043840 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:42:15.044376 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:42:15.044786 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:42:15.948100 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-fd44-10c1-8137-f5fa44a1eb9b) - Created
[0m01:42:16.939779 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff2-fd73-1b64-9fa9-d40d2aa0b67e
[0m01:42:16.941026 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m01:42:16.941610 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-fd44-10c1-8137-f5fa44a1eb9b) - Closing
[0m01:42:17.221827 [debug] [Thread-2 (]: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:42:17.222730 [error] [Thread-2 (]: 4 of 4 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 2.20s]
[0m01:42:17.223602 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:42:17.224241 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql.
[0m01:42:17.226429 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:42:17.226896 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:42:17.227539 [info ] [MainThread]: 
[0m01:42:17.228030 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 17.85 seconds (17.85s).
[0m01:42:17.229535 [debug] [MainThread]: Command end result
[0m01:42:17.293181 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:42:17.295971 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:42:17.304935 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m01:42:17.305528 [info ] [MainThread]: 
[0m01:42:17.306158 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m01:42:17.306653 [info ] [MainThread]: 
[0m01:42:17.307268 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:42:17.307815 [error] [MainThread]:   Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:42:17.308273 [info ] [MainThread]: 
[0m01:42:17.308822 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:42:17.309305 [info ] [MainThread]: 
[0m01:42:17.309879 [error] [MainThread]: [31mFailure in test not_null_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:42:17.310471 [error] [MainThread]:   Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:42:17.310961 [info ] [MainThread]: 
[0m01:42:17.311494 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:42:17.311958 [info ] [MainThread]: 
[0m01:42:17.312682 [error] [MainThread]: [31mFailure in test unique_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:42:17.322009 [error] [MainThread]:   Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:42:17.322681 [info ] [MainThread]: 
[0m01:42:17.323205 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:42:17.323576 [info ] [MainThread]: 
[0m01:42:17.324092 [error] [MainThread]: [31mFailure in test unique_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:42:17.324576 [error] [MainThread]:   Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:42:17.325036 [info ] [MainThread]: 
[0m01:42:17.325542 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:42:17.326011 [info ] [MainThread]: 
[0m01:42:17.326498 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=4 SKIP=0 NO-OP=0 TOTAL=4
[0m01:42:17.327506 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 22.305647, "process_in_blocks": "301008", "process_kernel_time": 1.267319, "process_mem_max_rss": "295780", "process_out_blocks": "6616", "process_user_time": 6.970256}
[0m01:42:17.328129 [debug] [MainThread]: Command `dbt test` failed at 01:42:17.327996 after 22.31 seconds
[0m01:42:17.328665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe625e8f570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe624f27890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe64e4f9e50>]}
[0m01:42:17.329182 [debug] [MainThread]: Flushing usage events
[0m01:42:18.147471 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:43:01.946289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadef360c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadf0bf0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadee2e7c50>]}


============================== 01:43:01.951786 | 8a90f654-3fce-421a-a3a3-67401a804534 ==============================
[0m01:43:01.951786 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:43:01.952354 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'introspect': 'True', 'use_colors': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'warn_error': 'None', 'no_print': 'None', 'version_check': 'True', 'quiet': 'False', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'profiles_dir': '/home/ubuntu/.dbt', 'partial_parse': 'True', 'empty': 'None', 'static_parser': 'True', 'fail_fast': 'False', 'log_cache_events': 'False', 'log_format': 'default', 'target_path': 'None'}
[0m01:43:01.968676 [info ] [MainThread]: dbt version: 1.11.2
[0m01:43:01.969274 [info ] [MainThread]: python version: 3.13.11
[0m01:43:01.969633 [info ] [MainThread]: python path: /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/bin/python3
[0m01:43:01.969910 [info ] [MainThread]: os info: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
[0m01:43:02.771463 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:43:02.771926 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:43:02.772275 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:43:03.486704 [info ] [MainThread]: Using profiles dir at /home/ubuntu/.dbt
[0m01:43:03.487191 [info ] [MainThread]: Using profiles.yml file at /home/ubuntu/.dbt/profiles.yml
[0m01:43:03.487480 [info ] [MainThread]: Using dbt_project.yml file at /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/dbt_project.yml
[0m01:43:03.487740 [info ] [MainThread]: adapter type: databricks
[0m01:43:03.487960 [info ] [MainThread]: adapter version: 1.11.4
[0m01:43:03.580437 [info ] [MainThread]: Configuration:
[0m01:43:03.581012 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:43:03.581341 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:43:03.581623 [info ] [MainThread]: Required dependencies:
[0m01:43:03.581927 [debug] [MainThread]: Executing "git --help"
[0m01:43:03.590394 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:43:03.591113 [debug] [MainThread]: STDERR: "b''"
[0m01:43:03.591535 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:43:03.592007 [info ] [MainThread]: Connection:
[0m01:43:03.592437 [info ] [MainThread]:   host: adb-3183350029643709.9.azuredatabricks.net
[0m01:43:03.592848 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/a418758e84eae08c
[0m01:43:03.593228 [info ] [MainThread]:   catalog: dev
[0m01:43:03.593599 [info ] [MainThread]:   schema: customer_bi
[0m01:43:03.594236 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:43:03.678448 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:43:03.679039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '8a90f654-3fce-421a-a3a3-67401a804534', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadcc8e0e90>]}
[0m01:43:03.679547 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m01:43:03.679824 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m01:43:03.680069 [debug] [MainThread]: Using databricks connection "debug"
[0m01:43:03.680378 [debug] [MainThread]: On debug: select 1 as id
[0m01:43:03.680603 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:43:04.631881 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0eff3-1a44-1eee-8376-cb3134049fec) - Created
[0m01:43:05.490347 [debug] [MainThread]: SQL status: OK in 1.810 seconds
[0m01:43:05.491452 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0eff3-1a44-1eee-8376-cb3134049fec, command-id=01f0eff3-1a79-18bc-a835-62e3944964e2) - Closing
[0m01:43:05.491892 [debug] [MainThread]: On debug: Close
[0m01:43:05.492168 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0eff3-1a44-1eee-8376-cb3134049fec) - Closing
[0m01:43:05.790801 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m01:43:05.791958 [info ] [MainThread]: [32mAll checks passed![0m
[0m01:43:05.793726 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 3.905991, "process_in_blocks": "288816", "process_kernel_time": 0.361503, "process_mem_max_rss": "252020", "process_out_blocks": "24", "process_user_time": 2.911572}
[0m01:43:05.794776 [debug] [MainThread]: Command `dbt debug` succeeded at 01:43:05.794594 after 3.91 seconds
[0m01:43:05.795444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadcc97a580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadcc97aad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadccaf3050>]}
[0m01:43:05.796111 [debug] [MainThread]: Flushing usage events
[0m01:43:06.599701 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:43:19.710098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c02f74c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c047ac190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c01f07c50>]}


============================== 01:43:19.714192 | 2aa50040-de88-4101-807a-338281e0cc50 ==============================
[0m01:43:19.714192 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:43:19.715027 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'version_check': 'True', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'log_format': 'default', 'indirect_selection': 'eager', 'static_parser': 'True', 'quiet': 'False', 'warn_error': 'None', 'empty': 'None', 'invocation_command': 'dbt test', 'no_print': 'None', 'cache_selected_only': 'False', 'use_colors': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'write_json': 'True', 'partial_parse': 'True', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'target_path': 'None'}
[0m01:43:21.494791 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:43:21.495626 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:43:21.496222 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:43:22.854478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c02bfcfc0>]}
[0m01:43:23.011140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3be04be140>]}
[0m01:43:23.012313 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:43:23.237197 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:43:23.238240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3be0686050>]}
[0m01:43:23.286517 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m01:43:23.850696 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:43:23.851525 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m01:43:23.852083 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:43:24.044866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3be0174c80>]}
[0m01:43:24.451116 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:43:24.465225 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:43:24.550857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3bdbeaa190>]}
[0m01:43:24.551993 [info ] [MainThread]: Found 2 models, 4 data tests, 731 macros
[0m01:43:24.562261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3bdbe346d0>]}
[0m01:43:24.566406 [info ] [MainThread]: 
[0m01:43:24.577947 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:43:24.578816 [info ] [MainThread]: 
[0m01:43:24.580167 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:43:24.580901 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:43:24.610879 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m01:43:24.612414 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m01:43:24.672835 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m01:43:24.675958 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m01:43:24.684262 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:43:25.618265 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff3-26ce-18f7-8a4b-4bab226a5d1b) - Created
[0m01:43:26.362442 [debug] [ThreadPool]: SQL status: OK in 1.680 seconds
[0m01:43:26.372791 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eff3-26ce-18f7-8a4b-4bab226a5d1b, command-id=01f0eff3-26fd-1b88-953f-72291db689f1) - Closing
[0m01:43:26.373791 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m01:43:26.374337 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff3-26ce-18f7-8a4b-4bab226a5d1b) - Closing
[0m01:43:26.657094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3be018ca10>]}
[0m01:43:26.663388 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:43:26.664173 [info ] [Thread-2 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m01:43:26.665168 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m01:43:26.665764 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710'
[0m01:43:26.666293 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:43:26.704640 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:43:26.706360 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:43:26.742702 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:43:26.744651 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:43:26.745483 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:43:26.746031 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:43:27.571674 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-27ff-1a54-9316-4c2d0f5dc52a) - Created
[0m01:43:28.215888 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-2824-1fab-95a8-f99cb9e5ecc9
[0m01:43:28.216743 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m01:43:28.217130 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-27ff-1a54-9316-4c2d0f5dc52a) - Closing
[0m01:43:28.522718 [debug] [Thread-2 (]: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:43:28.523702 [error] [Thread-2 (]: 1 of 4 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 1.86s]
[0m01:43:28.524311 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:43:28.524691 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:43:28.525226 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql.
[0m01:43:28.525690 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m01:43:28.527708 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m01:43:28.528155 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778'
[0m01:43:28.528568 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:43:28.534768 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:43:28.535655 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:43:28.542253 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:43:28.543011 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:43:28.543449 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:43:28.543790 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:43:29.440616 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-2912-19af-95ba-655fc3f35421) - Created
[0m01:43:30.091444 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-2941-1c48-b978-87d45bd5aec3
[0m01:43:30.092518 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: Close
[0m01:43:30.093083 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-2912-19af-95ba-655fc3f35421) - Closing
[0m01:43:30.409868 [debug] [Thread-2 (]: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:43:30.410617 [error] [Thread-2 (]: 2 of 4 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 1.88s]
[0m01:43:30.411214 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:43:30.411591 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:43:30.411896 [info ] [Thread-2 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m01:43:30.412442 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql.
[0m01:43:30.413171 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m01:43:30.413898 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321'
[0m01:43:30.414380 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:43:30.421574 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:43:30.422337 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:43:30.426711 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:43:30.427556 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:43:30.428185 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:43:30.428717 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:43:31.258417 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-2a31-1b4a-a3ac-156f753f1220) - Created
[0m01:43:31.956609 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-2a57-1bc5-8bb4-8b2c5d486163
[0m01:43:31.957438 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: Close
[0m01:43:31.957912 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-2a31-1b4a-a3ac-156f753f1220) - Closing
[0m01:43:32.237049 [debug] [Thread-2 (]: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:43:32.237643 [error] [Thread-2 (]: 3 of 4 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 1.82s]
[0m01:43:32.238245 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:43:32.238632 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:43:32.239289 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql.
[0m01:43:32.238929 [info ] [Thread-2 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m01:43:32.240185 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m01:43:32.240485 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493'
[0m01:43:32.240745 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:43:32.244006 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:43:32.244508 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:43:32.246904 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:43:32.247502 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:43:32.247983 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:43:32.248326 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:43:33.231678 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-2b57-1583-bda8-5c1fafb86270) - Created
[0m01:43:33.884682 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-2b84-1a14-a760-b8c6d758e317
[0m01:43:33.885882 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m01:43:33.886315 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-2b57-1583-bda8-5c1fafb86270) - Closing
[0m01:43:34.186513 [debug] [Thread-2 (]: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:43:34.187042 [error] [Thread-2 (]: 4 of 4 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 1.95s]
[0m01:43:34.187550 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:43:34.187988 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql.
[0m01:43:34.189799 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:43:34.190201 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:43:34.190676 [info ] [MainThread]: 
[0m01:43:34.191044 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 9.61 seconds (9.61s).
[0m01:43:34.191949 [debug] [MainThread]: Command end result
[0m01:43:34.217619 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:43:34.220362 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:43:34.226018 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m01:43:34.226378 [info ] [MainThread]: 
[0m01:43:34.226850 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m01:43:34.227151 [info ] [MainThread]: 
[0m01:43:34.227496 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:43:34.227817 [error] [MainThread]:   Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:43:34.228051 [info ] [MainThread]: 
[0m01:43:34.228312 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:43:34.228558 [info ] [MainThread]: 
[0m01:43:34.228858 [error] [MainThread]: [31mFailure in test not_null_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:43:34.229125 [error] [MainThread]:   Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:43:34.229349 [info ] [MainThread]: 
[0m01:43:34.229655 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:43:34.229888 [info ] [MainThread]: 
[0m01:43:34.230204 [error] [MainThread]: [31mFailure in test unique_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:43:34.230509 [error] [MainThread]:   Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:43:34.230770 [info ] [MainThread]: 
[0m01:43:34.231024 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:43:34.231234 [info ] [MainThread]: 
[0m01:43:34.231636 [error] [MainThread]: [31mFailure in test unique_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:43:34.231986 [error] [MainThread]:   Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:43:34.232240 [info ] [MainThread]: 
[0m01:43:34.232879 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:43:34.234248 [info ] [MainThread]: 
[0m01:43:34.234870 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=4 SKIP=0 NO-OP=0 TOTAL=4
[0m01:43:34.235969 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 14.601861, "process_in_blocks": "8208", "process_kernel_time": 0.957103, "process_mem_max_rss": "262728", "process_out_blocks": "3472", "process_user_time": 6.799421}
[0m01:43:34.236753 [debug] [MainThread]: Command `dbt test` failed at 01:43:34.236409 after 14.60 seconds
[0m01:43:34.237415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3bd841c260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3bdbec98b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3bdbec9770>]}
[0m01:43:34.237939 [debug] [MainThread]: Flushing usage events
[0m01:43:35.057803 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:46:45.511207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d4e758c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d50180190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d4d8abc50>]}


============================== 01:46:45.513979 | 3b1775ca-0fd4-4899-8bd1-84829cc7e28e ==============================
[0m01:46:45.513979 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:46:45.514446 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'fail_fast': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'use_colors': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'log_cache_events': 'False', 'static_parser': 'True', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'empty': 'None', 'introspect': 'True', 'printer_width': '80', 'no_print': 'None', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'write_json': 'True', 'invocation_command': 'dbt test', 'quiet': 'False'}
[0m01:46:46.459721 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:46:46.460533 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:46:46.460978 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:46:49.214845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d4e3d4fc0>]}
[0m01:46:49.411208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d27c0a140>]}
[0m01:46:49.422584 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:46:49.650225 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:46:49.651212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d27dd6050>]}
[0m01:46:49.685301 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m01:46:50.034780 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:46:50.035305 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m01:46:50.035622 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:46:50.177372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d278b0c80>]}
[0m01:46:50.432169 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:46:50.444600 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:46:50.516969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d2768e190>]}
[0m01:46:50.517552 [info ] [MainThread]: Found 2 models, 4 data tests, 731 macros
[0m01:46:50.517939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d276186d0>]}
[0m01:46:50.530263 [info ] [MainThread]: 
[0m01:46:50.531023 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:46:50.531502 [info ] [MainThread]: 
[0m01:46:50.532279 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:46:50.532683 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:46:50.555444 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m01:46:50.556275 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m01:46:50.591739 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m01:46:50.592346 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m01:46:50.592689 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:46:51.566724 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff3-a197-1676-86d3-cccc3deb6a16) - Created
[0m01:46:52.274342 [debug] [ThreadPool]: SQL status: OK in 1.680 seconds
[0m01:46:52.299358 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eff3-a197-1676-86d3-cccc3deb6a16, command-id=01f0eff3-a1be-1f5b-8a30-425f1d4d1558) - Closing
[0m01:46:52.300215 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m01:46:52.300644 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff3-a197-1676-86d3-cccc3deb6a16) - Closing
[0m01:46:52.573250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d278ef350>]}
[0m01:46:52.576606 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:46:52.577309 [info ] [Thread-2 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m01:46:52.578145 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m01:46:52.578586 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710'
[0m01:46:52.579031 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:46:52.626761 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:46:52.627694 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:46:52.681398 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:46:52.682484 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:46:52.683135 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:46:52.683609 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:46:53.548680 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a2c4-1dfb-803e-9da2a90c239f) - Created
[0m01:46:54.194414 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-a2ed-1b56-b770-8ccf00555bf8
[0m01:46:54.196117 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m01:46:54.196797 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a2c4-1dfb-803e-9da2a90c239f) - Closing
[0m01:46:54.466362 [debug] [Thread-2 (]: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:46:54.467409 [error] [Thread-2 (]: 1 of 4 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 1.89s]
[0m01:46:54.468324 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:46:54.468798 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:46:54.469341 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql.
[0m01:46:54.469771 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m01:46:54.479501 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m01:46:54.480188 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778'
[0m01:46:54.480648 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:46:54.496272 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:46:54.497418 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:46:54.506412 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:46:54.507549 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:46:54.508246 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:46:54.517466 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:46:55.476122 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a3e3-12fc-bdc4-b8caf802b58e) - Created
[0m01:46:56.079986 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-a413-15a5-a77f-56b379e79025
[0m01:46:56.081387 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: Close
[0m01:46:56.081921 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a3e3-12fc-bdc4-b8caf802b58e) - Closing
[0m01:46:56.352281 [debug] [Thread-2 (]: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:46:56.352821 [error] [Thread-2 (]: 2 of 4 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 1.87s]
[0m01:46:56.353320 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:46:56.353602 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:46:56.353885 [info ] [Thread-2 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m01:46:56.354426 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql.
[0m01:46:56.355082 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m01:46:56.355787 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321'
[0m01:46:56.356036 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:46:56.360102 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:46:56.360687 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:46:56.363246 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:46:56.363730 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:46:56.364130 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:46:56.364445 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:46:57.196392 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a4f0-19a6-8e7a-c2e06c0821eb) - Created
[0m01:46:59.809423 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-a516-1ab1-9f0f-1448149f6dc2
[0m01:46:59.811872 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: Close
[0m01:46:59.812596 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a4f0-19a6-8e7a-c2e06c0821eb) - Closing
[0m01:47:00.090613 [debug] [Thread-2 (]: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:47:00.091289 [error] [Thread-2 (]: 3 of 4 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 3.74s]
[0m01:47:00.091889 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:47:00.092281 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:47:00.092630 [info ] [Thread-2 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m01:47:00.093174 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql.
[0m01:47:00.093833 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m01:47:00.094876 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493'
[0m01:47:00.095226 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:47:00.100767 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:47:00.101433 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:47:00.104145 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:47:00.104753 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:47:00.105280 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:47:00.105613 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:47:00.955829 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a72c-119a-800f-4a76827dc867) - Created
[0m01:47:01.611590 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-a755-1ab7-9968-3474d0165760
[0m01:47:01.612185 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m01:47:01.612504 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a72c-119a-800f-4a76827dc867) - Closing
[0m01:47:01.876556 [debug] [Thread-2 (]: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:47:01.877331 [error] [Thread-2 (]: 4 of 4 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 1.78s]
[0m01:47:01.878111 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:47:01.878588 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql.
[0m01:47:01.880570 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:47:01.880829 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:47:01.881243 [info ] [MainThread]: 
[0m01:47:01.881490 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 11.35 seconds (11.35s).
[0m01:47:01.882291 [debug] [MainThread]: Command end result
[0m01:47:01.914705 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:47:01.917389 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:47:01.925714 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m01:47:01.926327 [info ] [MainThread]: 
[0m01:47:01.927017 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m01:47:01.927493 [info ] [MainThread]: 
[0m01:47:01.928007 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:47:01.928648 [error] [MainThread]:   Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:47:01.928992 [info ] [MainThread]: 
[0m01:47:01.929471 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:47:01.929887 [info ] [MainThread]: 
[0m01:47:01.930350 [error] [MainThread]: [31mFailure in test not_null_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:47:01.930732 [error] [MainThread]:   Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:47:01.931038 [info ] [MainThread]: 
[0m01:47:01.931416 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:47:01.931826 [info ] [MainThread]: 
[0m01:47:01.932193 [error] [MainThread]: [31mFailure in test unique_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:47:01.932594 [error] [MainThread]:   Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:47:01.932972 [info ] [MainThread]: 
[0m01:47:01.933370 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:47:01.933745 [info ] [MainThread]: 
[0m01:47:01.934168 [error] [MainThread]: [31mFailure in test unique_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:47:01.934596 [error] [MainThread]:   Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:47:01.935006 [info ] [MainThread]: 
[0m01:47:01.935383 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:47:01.935778 [info ] [MainThread]: 
[0m01:47:01.936416 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=4 SKIP=0 NO-OP=0 TOTAL=4
[0m01:47:01.937466 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 16.471548, "process_in_blocks": "0", "process_kernel_time": 1.812374, "process_mem_max_rss": "263892", "process_out_blocks": "3472", "process_user_time": 5.93442}
[0m01:47:01.938470 [debug] [MainThread]: Command `dbt test` failed at 01:47:01.938306 after 16.47 seconds
[0m01:47:01.939076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d24400260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d276b18b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d276b1770>]}
[0m01:47:01.939578 [debug] [MainThread]: Flushing usage events
[0m01:47:02.777135 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:48:58.530592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa274f6cc20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa27677c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa273effc50>]}


============================== 01:48:58.533653 | d98914b3-ba5d-41f6-81e2-9ae0d0fba362 ==============================
[0m01:48:58.533653 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:48:58.534280 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'introspect': 'True', 'static_parser': 'True', 'no_print': 'None', 'target_path': 'None', 'fail_fast': 'False', 'empty': 'None', 'use_colors': 'True', 'cache_selected_only': 'False', 'invocation_command': 'dbt test', 'use_experimental_parser': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'indirect_selection': 'eager', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'quiet': 'False', 'debug': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'log_format': 'default', 'printer_width': '80', 'send_anonymous_usage_stats': 'True'}
[0m01:48:59.242484 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:48:59.242978 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:48:59.243317 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:48:59.743057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa274bf4fc0>]}
[0m01:48:59.794953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24e502140>]}
[0m01:48:59.795579 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:48:59.862156 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:48:59.862768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24e4ce050>]}
[0m01:48:59.872520 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m01:48:59.947594 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:48:59.948059 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m01:48:59.948273 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:48:59.976339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24dfbcc80>]}
[0m01:49:00.034595 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:49:00.036166 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:49:00.053056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24df62190>]}
[0m01:49:00.053710 [info ] [MainThread]: Found 2 models, 4 data tests, 731 macros
[0m01:49:00.054149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24df286d0>]}
[0m01:49:00.056191 [info ] [MainThread]: 
[0m01:49:00.056639 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:49:00.057187 [info ] [MainThread]: 
[0m01:49:00.057789 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:49:00.058059 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:49:00.063865 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m01:49:00.064379 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m01:49:00.078570 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m01:49:00.079017 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m01:49:00.079289 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:49:00.964148 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff3-eeb0-174a-b1a6-02feec5ab972) - Created
[0m01:49:01.664092 [debug] [ThreadPool]: SQL status: OK in 1.580 seconds
[0m01:49:01.670039 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eff3-eeb0-174a-b1a6-02feec5ab972, command-id=01f0eff3-eede-15ad-8643-d0829327b1e7) - Closing
[0m01:49:01.670594 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m01:49:01.670877 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff3-eeb0-174a-b1a6-02feec5ab972) - Closing
[0m01:49:01.962939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24e02b350>]}
[0m01:49:01.966769 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:49:01.967290 [info ] [Thread-2 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m01:49:01.968122 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m01:49:01.968585 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710'
[0m01:49:01.968969 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:49:01.984112 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:49:01.986189 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:49:02.001640 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:49:02.002723 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:49:02.003326 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:49:02.003713 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:49:02.850419 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-efd6-12b5-a2a9-69b24c511f7f) - Created
[0m01:49:03.447804 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-effe-1388-ba77-a641a8dec6b1
[0m01:49:03.448980 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m01:49:03.449493 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-efd6-12b5-a2a9-69b24c511f7f) - Closing
[0m01:49:03.744850 [debug] [Thread-2 (]: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:49:03.745565 [error] [Thread-2 (]: 1 of 4 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 1.78s]
[0m01:49:03.746080 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:49:03.746383 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:49:03.746655 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m01:49:03.747099 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql.
[0m01:49:03.747847 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m01:49:03.749786 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778'
[0m01:49:03.750094 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:49:03.752998 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:49:03.753690 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:49:03.758856 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:49:03.759446 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:49:03.759817 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:49:03.760088 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:49:04.585816 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-f0e0-1e6f-ba54-7b3ec7ab70a4) - Created
[0m01:49:05.230537 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-f10f-15a9-8c9a-5789635f991a
[0m01:49:05.232348 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: Close
[0m01:49:05.233238 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-f0e0-1e6f-ba54-7b3ec7ab70a4) - Closing
[0m01:49:05.495510 [debug] [Thread-2 (]: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:49:05.496144 [error] [Thread-2 (]: 2 of 4 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 1.75s]
[0m01:49:05.496685 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:49:05.497028 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:49:05.497328 [info ] [Thread-2 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m01:49:05.497903 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m01:49:05.498497 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql.
[0m01:49:05.498918 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321'
[0m01:49:05.499491 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:49:05.507218 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:49:05.508413 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:49:05.512743 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:49:05.513733 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:49:05.514534 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:49:05.515067 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:49:06.335946 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-f1ed-14c4-8f9a-a2a29e0fbcf7) - Created
[0m01:49:06.966508 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-f213-17c3-a279-5d65dcde43a1
[0m01:49:06.968502 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: Close
[0m01:49:06.969169 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-f1ed-14c4-8f9a-a2a29e0fbcf7) - Closing
[0m01:49:07.249721 [debug] [Thread-2 (]: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:49:07.250606 [error] [Thread-2 (]: 3 of 4 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 1.75s]
[0m01:49:07.251542 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:49:07.252007 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:49:07.252267 [info ] [Thread-2 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m01:49:07.252695 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql.
[0m01:49:07.253148 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m01:49:07.253656 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493'
[0m01:49:07.253895 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:49:07.256716 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:49:07.257250 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:49:07.259341 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:49:07.260034 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:49:07.260529 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:49:07.260839 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:49:08.161039 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-f2f8-19ba-a548-60a001cff83e) - Created
[0m01:49:09.066146 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-f328-1489-bba6-8d88338b2872
[0m01:49:09.068159 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m01:49:09.069115 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-f2f8-19ba-a548-60a001cff83e) - Closing
[0m01:49:09.369141 [debug] [Thread-2 (]: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:49:09.369796 [error] [Thread-2 (]: 4 of 4 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 2.12s]
[0m01:49:09.370334 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:49:09.370824 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql.
[0m01:49:09.372233 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:49:09.372563 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:49:09.372955 [info ] [MainThread]: 
[0m01:49:09.373244 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 9.32 seconds (9.32s).
[0m01:49:09.373987 [debug] [MainThread]: Command end result
[0m01:49:09.397069 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:49:09.398566 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:49:09.403194 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m01:49:09.403435 [info ] [MainThread]: 
[0m01:49:09.403762 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m01:49:09.404023 [info ] [MainThread]: 
[0m01:49:09.404315 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:49:09.404618 [error] [MainThread]:   Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:49:09.404857 [info ] [MainThread]: 
[0m01:49:09.405122 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:49:09.405399 [info ] [MainThread]: 
[0m01:49:09.405691 [error] [MainThread]: [31mFailure in test not_null_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:49:09.405960 [error] [MainThread]:   Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:49:09.406191 [info ] [MainThread]: 
[0m01:49:09.406455 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:49:09.406675 [info ] [MainThread]: 
[0m01:49:09.406920 [error] [MainThread]: [31mFailure in test unique_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:49:09.407170 [error] [MainThread]:   Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:49:09.407419 [info ] [MainThread]: 
[0m01:49:09.407681 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:49:09.407887 [info ] [MainThread]: 
[0m01:49:09.408131 [error] [MainThread]: [31mFailure in test unique_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:49:09.408428 [error] [MainThread]:   Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:49:09.408762 [info ] [MainThread]: 
[0m01:49:09.409052 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:49:09.409285 [info ] [MainThread]: 
[0m01:49:09.409535 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=4 SKIP=0 NO-OP=0 TOTAL=4
[0m01:49:09.410110 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 10.924903, "process_in_blocks": "0", "process_kernel_time": 0.274197, "process_mem_max_rss": "263824", "process_out_blocks": "3464", "process_user_time": 3.306592}
[0m01:49:09.410538 [debug] [MainThread]: Command `dbt test` failed at 01:49:09.410460 after 10.93 seconds
[0m01:49:09.410868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24c518260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24ddc5810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24ddc56d0>]}
[0m01:49:09.411153 [debug] [MainThread]: Flushing usage events
[0m01:49:13.960356 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:49:57.073393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa333d08c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa33553c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa332c9fc50>]}


============================== 01:49:57.076386 | e078afc6-beb4-43a4-9ea0-d634b7fa1f5e ==============================
[0m01:49:57.076386 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:49:57.076879 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'fail_fast': 'False', 'indirect_selection': 'eager', 'debug': 'False', 'cache_selected_only': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'write_json': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'target_path': 'None', 'static_parser': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'introspect': 'True', 'empty': 'None', 'partial_parse': 'True', 'log_format': 'default', 'invocation_command': 'dbt test', 'version_check': 'True', 'quiet': 'False', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False'}
[0m01:49:57.807945 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:49:57.808502 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:49:57.808822 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:49:58.372966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e078afc6-beb4-43a4-9ea0-d634b7fa1f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa33398cfc0>]}
[0m01:49:58.438237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e078afc6-beb4-43a4-9ea0-d634b7fa1f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa31125a140>]}
[0m01:49:58.439332 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:49:58.509100 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:49:58.509814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e078afc6-beb4-43a4-9ea0-d634b7fa1f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa311426050>]}
[0m01:49:58.518842 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m01:49:58.607597 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 0 files added, 0 files changed.
[0m01:49:58.608179 [debug] [MainThread]: Partial parsing: deleted file: customer_bi://models/example/my_second_dbt_model.sql
[0m01:49:58.608469 [debug] [MainThread]: Partial parsing: deleted file: customer_bi://models/example/my_first_dbt_model.sql
[0m01:49:58.638285 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.customer_bi.example
[0m01:49:58.642752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e078afc6-beb4-43a4-9ea0-d634b7fa1f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa310f00c80>]}
[0m01:49:58.689387 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:49:58.691080 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:49:58.705821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e078afc6-beb4-43a4-9ea0-d634b7fa1f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa310cb5d30>]}
[0m01:49:58.706272 [info ] [MainThread]: Found 731 macros
[0m01:49:58.706608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e078afc6-beb4-43a4-9ea0-d634b7fa1f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa310cc2410>]}
[0m01:49:58.707809 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m01:49:58.709589 [debug] [MainThread]: Command end result
[0m01:49:58.738981 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:49:58.740639 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:49:58.743294 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m01:49:58.743926 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 1.7191427, "process_in_blocks": "192", "process_kernel_time": 0.834417, "process_mem_max_rss": "253892", "process_out_blocks": "4824", "process_user_time": 4.370263}
[0m01:49:58.744433 [debug] [MainThread]: Command `dbt test` succeeded at 01:49:58.744342 after 1.72 seconds
[0m01:49:58.744783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa332e8b290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa310c594f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa310c59650>]}
[0m01:49:58.745083 [debug] [MainThread]: Flushing usage events
[0m01:49:59.552039 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:37:46.914210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb29c79780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb28c7a9e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb28c7a980>]}


============================== 00:37:46.918673 | f9dc47d6-d1c4-4730-9ce5-cbdba94f951b ==============================
[0m00:37:46.918673 [info ] [MainThread]: Running with dbt=1.10.0
[0m00:37:46.919506 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:37:47.330303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f9dc47d6-d1c4-4730-9ce5-cbdba94f951b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb294c7280>]}
[0m00:37:47.365966 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m00:37:47.369438 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m00:37:47.380559 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.5928156, "process_in_blocks": "288", "process_kernel_time": 0.189806, "process_mem_max_rss": "96632", "process_out_blocks": "8", "process_user_time": 2.837105}
[0m00:37:47.381770 [debug] [MainThread]: Command `cli deps` succeeded at 00:37:47.381493 after 0.59 seconds
[0m00:37:47.382599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb29c79780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb294c7280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb28b95420>]}
[0m00:37:47.383715 [debug] [MainThread]: Flushing usage events
[0m00:37:48.349136 [debug] [MainThread]: An error was encountered while trying to flush usage events
