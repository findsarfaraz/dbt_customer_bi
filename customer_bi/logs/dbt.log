[0m01:24:25.703184 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30950b4c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f30968e8190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3094043c50>]}


============================== 01:24:25.706500 | 1a1b66f4-df3f-419d-9de4-5e1c997b313a ==============================
[0m01:24:25.706500 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:24:25.707058 [debug] [MainThread]: running dbt with arguments {'static_parser': 'True', 'cache_selected_only': 'False', 'debug': 'False', 'log_format': 'default', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'fail_fast': 'False', 'target_path': 'None', 'log_cache_events': 'False', 'warn_error': 'None', 'profiles_dir': '/home/ubuntu/.dbt', 'quiet': 'False', 'empty': 'None', 'indirect_selection': 'eager', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt debug', 'write_json': 'True', 'use_colors': 'True', 'no_print': 'None', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'version_check': 'True', 'introspect': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m01:24:25.720901 [info ] [MainThread]: dbt version: 1.11.2
[0m01:24:25.721311 [info ] [MainThread]: python version: 3.13.11
[0m01:24:25.721707 [info ] [MainThread]: python path: /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/bin/python3
[0m01:24:25.722053 [info ] [MainThread]: os info: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
[0m01:24:26.497005 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:24:26.497465 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:24:26.497741 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:24:27.230543 [info ] [MainThread]: Using profiles dir at /home/ubuntu/.dbt
[0m01:24:27.230967 [info ] [MainThread]: Using profiles.yml file at /home/ubuntu/.dbt/profiles.yml
[0m01:24:27.231278 [info ] [MainThread]: Using dbt_project.yml file at /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/dbt_project.yml
[0m01:24:27.231520 [info ] [MainThread]: adapter type: databricks
[0m01:24:27.231765 [info ] [MainThread]: adapter version: 1.11.4
[0m01:24:27.310654 [info ] [MainThread]: Configuration:
[0m01:24:27.311185 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:24:27.311507 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:24:27.311777 [info ] [MainThread]: Required dependencies:
[0m01:24:27.312044 [debug] [MainThread]: Executing "git --help"
[0m01:24:27.316157 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:24:27.316501 [debug] [MainThread]: STDERR: "b''"
[0m01:24:27.316718 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:24:27.316974 [info ] [MainThread]: Connection:
[0m01:24:27.317234 [info ] [MainThread]:   host: adb-3183350029643709.9.azuredatabricks.net
[0m01:24:27.317521 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/a418758e84eae08c
[0m01:24:27.317752 [info ] [MainThread]:   catalog: dev
[0m01:24:27.318030 [info ] [MainThread]:   schema: customer_bi
[0m01:24:27.318414 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:24:27.380670 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:24:27.381279 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1a1b66f4-df3f-419d-9de4-5e1c997b313a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f306e578e90>]}
[0m01:24:27.381742 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m01:24:27.381991 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m01:24:27.382211 [debug] [MainThread]: Using databricks connection "debug"
[0m01:24:27.382426 [debug] [MainThread]: On debug: select 1 as id
[0m01:24:27.382612 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:24:29.044642 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0eff0-8141-1a55-8821-a66eea3e8cee) - Created
[0m01:24:35.889102 [debug] [MainThread]: SQL status: OK in 8.510 seconds
[0m01:24:35.891801 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0eff0-8141-1a55-8821-a66eea3e8cee, command-id=01f0eff0-8187-1295-a4b9-f0cb259ba1ac) - Closing
[0m01:24:36.217940 [debug] [MainThread]: On debug: Close
[0m01:24:36.218887 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0eff0-8141-1a55-8821-a66eea3e8cee) - Closing
[0m01:24:36.498579 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m01:24:36.499625 [info ] [MainThread]: [32mAll checks passed![0m
[0m01:24:36.501276 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 10.868009, "process_in_blocks": "197936", "process_kernel_time": 0.343193, "process_mem_max_rss": "250060", "process_out_blocks": "16", "process_user_time": 4.6794}
[0m01:24:36.501958 [debug] [MainThread]: Command `dbt debug` succeeded at 01:24:36.501784 after 10.87 seconds
[0m01:24:36.502462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f306e612580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f306e612ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f306e78b050>]}
[0m01:24:36.502938 [debug] [MainThread]: Flushing usage events
[0m01:24:37.306508 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:41:55.163040 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe64ce8cc20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe64e8b4190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe64bfdfc50>]}


============================== 01:41:55.174162 | 1a8ce117-2824-4eea-a9f4-3598ed1da52c ==============================
[0m01:41:55.174162 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:41:55.175107 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt test', 'use_colors': 'True', 'introspect': 'True', 'empty': 'None', 'cache_selected_only': 'False', 'warn_error': 'None', 'profiles_dir': '/home/ubuntu/.dbt', 'indirect_selection': 'eager', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'write_json': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'static_parser': 'True', 'version_check': 'True', 'quiet': 'False', 'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'log_format': 'default', 'target_path': 'None', 'printer_width': '80', 'debug': 'False', 'partial_parse': 'True'}
[0m01:41:56.218563 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:41:56.219069 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:41:56.219373 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:41:57.237702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe64cce0fc0>]}
[0m01:41:57.299306 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6263e2140>]}
[0m01:41:57.299962 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:41:57.390776 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:41:57.391539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6265ae050>]}
[0m01:41:57.406868 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m01:41:57.407553 [info ] [MainThread]: Unable to do partial parsing because saved manifest not found. Starting full parse.
[0m01:41:57.408038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6263da6c0>]}
[0m01:41:59.155025 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe625dea5f0>]}
[0m01:41:59.212275 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:41:59.215489 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:41:59.377170 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe6260b5300>]}
[0m01:41:59.378125 [info ] [MainThread]: Found 2 models, 4 data tests, 731 macros
[0m01:41:59.378655 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe624535a90>]}
[0m01:41:59.381309 [info ] [MainThread]: 
[0m01:41:59.381732 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:41:59.382088 [info ] [MainThread]: 
[0m01:41:59.382682 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:41:59.382998 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:41:59.391192 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m01:41:59.391869 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m01:41:59.407664 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m01:41:59.408864 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m01:41:59.409412 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:42:00.991688 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff2-f449-13d2-999d-23d3a71a469b) - Created
[0m01:42:08.272078 [debug] [ThreadPool]: SQL status: OK in 8.860 seconds
[0m01:42:08.304853 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eff2-f449-13d2-999d-23d3a71a469b, command-id=01f0eff2-f490-1c55-8589-ea2f8dfad33b) - Closing
[0m01:42:08.743168 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m01:42:08.743704 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff2-f449-13d2-999d-23d3a71a469b) - Closing
[0m01:42:09.058286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1a8ce117-2824-4eea-a9f4-3598ed1da52c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe624f399c0>]}
[0m01:42:09.067540 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:42:09.068128 [info ] [Thread-2 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m01:42:09.068848 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m01:42:09.069273 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710'
[0m01:42:09.069634 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:42:09.086430 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:42:09.090558 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:42:09.104215 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:42:09.106063 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:42:09.106441 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:42:09.106710 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:42:09.981967 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-f9b7-1596-9581-4b5ab053b439) - Created
[0m01:42:10.970664 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff2-f9e5-1bcd-b879-1d2de3a0c86c
[0m01:42:10.971476 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m01:42:10.971812 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-f9b7-1596-9581-4b5ab053b439) - Closing
[0m01:42:11.237261 [debug] [Thread-2 (]: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:42:11.237799 [error] [Thread-2 (]: 1 of 4 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 2.17s]
[0m01:42:11.238296 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:42:11.238635 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:42:11.239057 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql.
[0m01:42:11.239435 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m01:42:11.240494 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m01:42:11.240773 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778'
[0m01:42:11.241020 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:42:11.244137 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:42:11.244725 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:42:11.247021 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:42:11.247608 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:42:11.248019 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:42:11.248317 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:42:12.175921 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-fb04-179d-b751-2f4b632e0ff7) - Created
[0m01:42:12.894581 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff2-fb35-1045-a2e7-ab3f55fa1ad9
[0m01:42:12.896624 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: Close
[0m01:42:12.897443 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-fb04-179d-b751-2f4b632e0ff7) - Closing
[0m01:42:13.182336 [debug] [Thread-2 (]: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:42:13.183097 [error] [Thread-2 (]: 2 of 4 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 1.94s]
[0m01:42:13.183739 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:42:13.184150 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:42:13.184962 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql.
[0m01:42:13.184515 [info ] [Thread-2 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m01:42:13.185782 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m01:42:13.186103 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321'
[0m01:42:13.186352 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:42:13.190759 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:42:13.191406 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:42:13.193916 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:42:13.194397 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:42:13.194798 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:42:13.195099 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:42:14.068715 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-fc28-1e61-b74b-044ba6f549c0) - Created
[0m01:42:14.719815 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff2-fc55-1350-b790-0faaa83c558e
[0m01:42:14.721011 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: Close
[0m01:42:14.721622 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-fc28-1e61-b74b-044ba6f549c0) - Closing
[0m01:42:15.012613 [debug] [Thread-2 (]: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:42:15.014125 [error] [Thread-2 (]: 3 of 4 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 1.83s]
[0m01:42:15.015668 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:42:15.016641 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:42:15.017517 [info ] [Thread-2 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m01:42:15.018912 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql.
[0m01:42:15.020413 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m01:42:15.021875 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493'
[0m01:42:15.022833 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:42:15.036822 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:42:15.038776 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:42:15.043100 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:42:15.043840 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:42:15.044376 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:42:15.044786 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:42:15.948100 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-fd44-10c1-8137-f5fa44a1eb9b) - Created
[0m01:42:16.939779 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff2-fd73-1b64-9fa9-d40d2aa0b67e
[0m01:42:16.941026 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m01:42:16.941610 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff2-fd44-10c1-8137-f5fa44a1eb9b) - Closing
[0m01:42:17.221827 [debug] [Thread-2 (]: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:42:17.222730 [error] [Thread-2 (]: 4 of 4 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 2.20s]
[0m01:42:17.223602 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:42:17.224241 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql.
[0m01:42:17.226429 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:42:17.226896 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:42:17.227539 [info ] [MainThread]: 
[0m01:42:17.228030 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 17.85 seconds (17.85s).
[0m01:42:17.229535 [debug] [MainThread]: Command end result
[0m01:42:17.293181 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:42:17.295971 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:42:17.304935 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m01:42:17.305528 [info ] [MainThread]: 
[0m01:42:17.306158 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m01:42:17.306653 [info ] [MainThread]: 
[0m01:42:17.307268 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:42:17.307815 [error] [MainThread]:   Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:42:17.308273 [info ] [MainThread]: 
[0m01:42:17.308822 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:42:17.309305 [info ] [MainThread]: 
[0m01:42:17.309879 [error] [MainThread]: [31mFailure in test not_null_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:42:17.310471 [error] [MainThread]:   Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:42:17.310961 [info ] [MainThread]: 
[0m01:42:17.311494 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:42:17.311958 [info ] [MainThread]: 
[0m01:42:17.312682 [error] [MainThread]: [31mFailure in test unique_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:42:17.322009 [error] [MainThread]:   Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:42:17.322681 [info ] [MainThread]: 
[0m01:42:17.323205 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:42:17.323576 [info ] [MainThread]: 
[0m01:42:17.324092 [error] [MainThread]: [31mFailure in test unique_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:42:17.324576 [error] [MainThread]:   Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:42:17.325036 [info ] [MainThread]: 
[0m01:42:17.325542 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:42:17.326011 [info ] [MainThread]: 
[0m01:42:17.326498 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=4 SKIP=0 NO-OP=0 TOTAL=4
[0m01:42:17.327506 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 22.305647, "process_in_blocks": "301008", "process_kernel_time": 1.267319, "process_mem_max_rss": "295780", "process_out_blocks": "6616", "process_user_time": 6.970256}
[0m01:42:17.328129 [debug] [MainThread]: Command `dbt test` failed at 01:42:17.327996 after 22.31 seconds
[0m01:42:17.328665 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe625e8f570>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe624f27890>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe64e4f9e50>]}
[0m01:42:17.329182 [debug] [MainThread]: Flushing usage events
[0m01:42:18.147471 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:43:01.946289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadef360c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadf0bf0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadee2e7c50>]}


============================== 01:43:01.951786 | 8a90f654-3fce-421a-a3a3-67401a804534 ==============================
[0m01:43:01.951786 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:43:01.952354 [debug] [MainThread]: running dbt with arguments {'debug': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'introspect': 'True', 'use_colors': 'True', 'cache_selected_only': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'write_json': 'True', 'warn_error': 'None', 'no_print': 'None', 'version_check': 'True', 'quiet': 'False', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'invocation_command': 'dbt debug', 'profiles_dir': '/home/ubuntu/.dbt', 'partial_parse': 'True', 'empty': 'None', 'static_parser': 'True', 'fail_fast': 'False', 'log_cache_events': 'False', 'log_format': 'default', 'target_path': 'None'}
[0m01:43:01.968676 [info ] [MainThread]: dbt version: 1.11.2
[0m01:43:01.969274 [info ] [MainThread]: python version: 3.13.11
[0m01:43:01.969633 [info ] [MainThread]: python path: /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/bin/python3
[0m01:43:01.969910 [info ] [MainThread]: os info: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
[0m01:43:02.771463 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:43:02.771926 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:43:02.772275 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:43:03.486704 [info ] [MainThread]: Using profiles dir at /home/ubuntu/.dbt
[0m01:43:03.487191 [info ] [MainThread]: Using profiles.yml file at /home/ubuntu/.dbt/profiles.yml
[0m01:43:03.487480 [info ] [MainThread]: Using dbt_project.yml file at /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/dbt_project.yml
[0m01:43:03.487740 [info ] [MainThread]: adapter type: databricks
[0m01:43:03.487960 [info ] [MainThread]: adapter version: 1.11.4
[0m01:43:03.580437 [info ] [MainThread]: Configuration:
[0m01:43:03.581012 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m01:43:03.581341 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m01:43:03.581623 [info ] [MainThread]: Required dependencies:
[0m01:43:03.581927 [debug] [MainThread]: Executing "git --help"
[0m01:43:03.590394 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m01:43:03.591113 [debug] [MainThread]: STDERR: "b''"
[0m01:43:03.591535 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m01:43:03.592007 [info ] [MainThread]: Connection:
[0m01:43:03.592437 [info ] [MainThread]:   host: adb-3183350029643709.9.azuredatabricks.net
[0m01:43:03.592848 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/a418758e84eae08c
[0m01:43:03.593228 [info ] [MainThread]:   catalog: dev
[0m01:43:03.593599 [info ] [MainThread]:   schema: customer_bi
[0m01:43:03.594236 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:43:03.678448 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:43:03.679039 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '8a90f654-3fce-421a-a3a3-67401a804534', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadcc8e0e90>]}
[0m01:43:03.679547 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m01:43:03.679824 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m01:43:03.680069 [debug] [MainThread]: Using databricks connection "debug"
[0m01:43:03.680378 [debug] [MainThread]: On debug: select 1 as id
[0m01:43:03.680603 [debug] [MainThread]: Opening a new connection, currently in state init
[0m01:43:04.631881 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0eff3-1a44-1eee-8376-cb3134049fec) - Created
[0m01:43:05.490347 [debug] [MainThread]: SQL status: OK in 1.810 seconds
[0m01:43:05.491452 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0eff3-1a44-1eee-8376-cb3134049fec, command-id=01f0eff3-1a79-18bc-a835-62e3944964e2) - Closing
[0m01:43:05.491892 [debug] [MainThread]: On debug: Close
[0m01:43:05.492168 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0eff3-1a44-1eee-8376-cb3134049fec) - Closing
[0m01:43:05.790801 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m01:43:05.791958 [info ] [MainThread]: [32mAll checks passed![0m
[0m01:43:05.793726 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 3.905991, "process_in_blocks": "288816", "process_kernel_time": 0.361503, "process_mem_max_rss": "252020", "process_out_blocks": "24", "process_user_time": 2.911572}
[0m01:43:05.794776 [debug] [MainThread]: Command `dbt debug` succeeded at 01:43:05.794594 after 3.91 seconds
[0m01:43:05.795444 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadcc97a580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadcc97aad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fadccaf3050>]}
[0m01:43:05.796111 [debug] [MainThread]: Flushing usage events
[0m01:43:06.599701 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:43:19.710098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c02f74c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c047ac190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c01f07c50>]}


============================== 01:43:19.714192 | 2aa50040-de88-4101-807a-338281e0cc50 ==============================
[0m01:43:19.714192 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:43:19.715027 [debug] [MainThread]: running dbt with arguments {'send_anonymous_usage_stats': 'True', 'fail_fast': 'False', 'debug': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'version_check': 'True', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'log_format': 'default', 'indirect_selection': 'eager', 'static_parser': 'True', 'quiet': 'False', 'warn_error': 'None', 'empty': 'None', 'invocation_command': 'dbt test', 'no_print': 'None', 'cache_selected_only': 'False', 'use_colors': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'write_json': 'True', 'partial_parse': 'True', 'printer_width': '80', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'target_path': 'None'}
[0m01:43:21.494791 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:43:21.495626 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:43:21.496222 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:43:22.854478 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3c02bfcfc0>]}
[0m01:43:23.011140 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3be04be140>]}
[0m01:43:23.012313 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:43:23.237197 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:43:23.238240 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3be0686050>]}
[0m01:43:23.286517 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m01:43:23.850696 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:43:23.851525 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m01:43:23.852083 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:43:24.044866 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3be0174c80>]}
[0m01:43:24.451116 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:43:24.465225 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:43:24.550857 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3bdbeaa190>]}
[0m01:43:24.551993 [info ] [MainThread]: Found 2 models, 4 data tests, 731 macros
[0m01:43:24.562261 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3bdbe346d0>]}
[0m01:43:24.566406 [info ] [MainThread]: 
[0m01:43:24.577947 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:43:24.578816 [info ] [MainThread]: 
[0m01:43:24.580167 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:43:24.580901 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:43:24.610879 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m01:43:24.612414 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m01:43:24.672835 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m01:43:24.675958 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m01:43:24.684262 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:43:25.618265 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff3-26ce-18f7-8a4b-4bab226a5d1b) - Created
[0m01:43:26.362442 [debug] [ThreadPool]: SQL status: OK in 1.680 seconds
[0m01:43:26.372791 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eff3-26ce-18f7-8a4b-4bab226a5d1b, command-id=01f0eff3-26fd-1b88-953f-72291db689f1) - Closing
[0m01:43:26.373791 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m01:43:26.374337 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff3-26ce-18f7-8a4b-4bab226a5d1b) - Closing
[0m01:43:26.657094 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '2aa50040-de88-4101-807a-338281e0cc50', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3be018ca10>]}
[0m01:43:26.663388 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:43:26.664173 [info ] [Thread-2 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m01:43:26.665168 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m01:43:26.665764 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710'
[0m01:43:26.666293 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:43:26.704640 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:43:26.706360 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:43:26.742702 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:43:26.744651 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:43:26.745483 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:43:26.746031 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:43:27.571674 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-27ff-1a54-9316-4c2d0f5dc52a) - Created
[0m01:43:28.215888 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-2824-1fab-95a8-f99cb9e5ecc9
[0m01:43:28.216743 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m01:43:28.217130 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-27ff-1a54-9316-4c2d0f5dc52a) - Closing
[0m01:43:28.522718 [debug] [Thread-2 (]: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:43:28.523702 [error] [Thread-2 (]: 1 of 4 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 1.86s]
[0m01:43:28.524311 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:43:28.524691 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:43:28.525226 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql.
[0m01:43:28.525690 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m01:43:28.527708 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m01:43:28.528155 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778'
[0m01:43:28.528568 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:43:28.534768 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:43:28.535655 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:43:28.542253 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:43:28.543011 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:43:28.543449 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:43:28.543790 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:43:29.440616 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-2912-19af-95ba-655fc3f35421) - Created
[0m01:43:30.091444 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-2941-1c48-b978-87d45bd5aec3
[0m01:43:30.092518 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: Close
[0m01:43:30.093083 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-2912-19af-95ba-655fc3f35421) - Closing
[0m01:43:30.409868 [debug] [Thread-2 (]: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:43:30.410617 [error] [Thread-2 (]: 2 of 4 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 1.88s]
[0m01:43:30.411214 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:43:30.411591 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:43:30.411896 [info ] [Thread-2 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m01:43:30.412442 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql.
[0m01:43:30.413171 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m01:43:30.413898 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321'
[0m01:43:30.414380 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:43:30.421574 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:43:30.422337 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:43:30.426711 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:43:30.427556 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:43:30.428185 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:43:30.428717 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:43:31.258417 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-2a31-1b4a-a3ac-156f753f1220) - Created
[0m01:43:31.956609 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-2a57-1bc5-8bb4-8b2c5d486163
[0m01:43:31.957438 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: Close
[0m01:43:31.957912 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-2a31-1b4a-a3ac-156f753f1220) - Closing
[0m01:43:32.237049 [debug] [Thread-2 (]: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:43:32.237643 [error] [Thread-2 (]: 3 of 4 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 1.82s]
[0m01:43:32.238245 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:43:32.238632 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:43:32.239289 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql.
[0m01:43:32.238929 [info ] [Thread-2 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m01:43:32.240185 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m01:43:32.240485 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493'
[0m01:43:32.240745 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:43:32.244006 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:43:32.244508 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:43:32.246904 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:43:32.247502 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:43:32.247983 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:43:32.248326 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:43:33.231678 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-2b57-1583-bda8-5c1fafb86270) - Created
[0m01:43:33.884682 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-2b84-1a14-a760-b8c6d758e317
[0m01:43:33.885882 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m01:43:33.886315 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-2b57-1583-bda8-5c1fafb86270) - Closing
[0m01:43:34.186513 [debug] [Thread-2 (]: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:43:34.187042 [error] [Thread-2 (]: 4 of 4 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 1.95s]
[0m01:43:34.187550 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:43:34.187988 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql.
[0m01:43:34.189799 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:43:34.190201 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:43:34.190676 [info ] [MainThread]: 
[0m01:43:34.191044 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 9.61 seconds (9.61s).
[0m01:43:34.191949 [debug] [MainThread]: Command end result
[0m01:43:34.217619 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:43:34.220362 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:43:34.226018 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m01:43:34.226378 [info ] [MainThread]: 
[0m01:43:34.226850 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m01:43:34.227151 [info ] [MainThread]: 
[0m01:43:34.227496 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:43:34.227817 [error] [MainThread]:   Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:43:34.228051 [info ] [MainThread]: 
[0m01:43:34.228312 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:43:34.228558 [info ] [MainThread]: 
[0m01:43:34.228858 [error] [MainThread]: [31mFailure in test not_null_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:43:34.229125 [error] [MainThread]:   Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:43:34.229349 [info ] [MainThread]: 
[0m01:43:34.229655 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:43:34.229888 [info ] [MainThread]: 
[0m01:43:34.230204 [error] [MainThread]: [31mFailure in test unique_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:43:34.230509 [error] [MainThread]:   Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:43:34.230770 [info ] [MainThread]: 
[0m01:43:34.231024 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:43:34.231234 [info ] [MainThread]: 
[0m01:43:34.231636 [error] [MainThread]: [31mFailure in test unique_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:43:34.231986 [error] [MainThread]:   Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:43:34.232240 [info ] [MainThread]: 
[0m01:43:34.232879 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:43:34.234248 [info ] [MainThread]: 
[0m01:43:34.234870 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=4 SKIP=0 NO-OP=0 TOTAL=4
[0m01:43:34.235969 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 14.601861, "process_in_blocks": "8208", "process_kernel_time": 0.957103, "process_mem_max_rss": "262728", "process_out_blocks": "3472", "process_user_time": 6.799421}
[0m01:43:34.236753 [debug] [MainThread]: Command `dbt test` failed at 01:43:34.236409 after 14.60 seconds
[0m01:43:34.237415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3bd841c260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3bdbec98b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3bdbec9770>]}
[0m01:43:34.237939 [debug] [MainThread]: Flushing usage events
[0m01:43:35.057803 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:46:45.511207 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d4e758c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d50180190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d4d8abc50>]}


============================== 01:46:45.513979 | 3b1775ca-0fd4-4899-8bd1-84829cc7e28e ==============================
[0m01:46:45.513979 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:46:45.514446 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'target_path': 'None', 'fail_fast': 'False', 'indirect_selection': 'eager', 'version_check': 'True', 'use_colors': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'log_cache_events': 'False', 'static_parser': 'True', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'empty': 'None', 'introspect': 'True', 'printer_width': '80', 'no_print': 'None', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'write_json': 'True', 'invocation_command': 'dbt test', 'quiet': 'False'}
[0m01:46:46.459721 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:46:46.460533 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:46:46.460978 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:46:49.214845 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d4e3d4fc0>]}
[0m01:46:49.411208 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d27c0a140>]}
[0m01:46:49.422584 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:46:49.650225 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:46:49.651212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d27dd6050>]}
[0m01:46:49.685301 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m01:46:50.034780 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:46:50.035305 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m01:46:50.035622 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:46:50.177372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d278b0c80>]}
[0m01:46:50.432169 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:46:50.444600 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:46:50.516969 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d2768e190>]}
[0m01:46:50.517552 [info ] [MainThread]: Found 2 models, 4 data tests, 731 macros
[0m01:46:50.517939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d276186d0>]}
[0m01:46:50.530263 [info ] [MainThread]: 
[0m01:46:50.531023 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:46:50.531502 [info ] [MainThread]: 
[0m01:46:50.532279 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:46:50.532683 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:46:50.555444 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m01:46:50.556275 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m01:46:50.591739 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m01:46:50.592346 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m01:46:50.592689 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:46:51.566724 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff3-a197-1676-86d3-cccc3deb6a16) - Created
[0m01:46:52.274342 [debug] [ThreadPool]: SQL status: OK in 1.680 seconds
[0m01:46:52.299358 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eff3-a197-1676-86d3-cccc3deb6a16, command-id=01f0eff3-a1be-1f5b-8a30-425f1d4d1558) - Closing
[0m01:46:52.300215 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m01:46:52.300644 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff3-a197-1676-86d3-cccc3deb6a16) - Closing
[0m01:46:52.573250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '3b1775ca-0fd4-4899-8bd1-84829cc7e28e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d278ef350>]}
[0m01:46:52.576606 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:46:52.577309 [info ] [Thread-2 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m01:46:52.578145 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m01:46:52.578586 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710'
[0m01:46:52.579031 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:46:52.626761 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:46:52.627694 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:46:52.681398 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:46:52.682484 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:46:52.683135 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:46:52.683609 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:46:53.548680 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a2c4-1dfb-803e-9da2a90c239f) - Created
[0m01:46:54.194414 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-a2ed-1b56-b770-8ccf00555bf8
[0m01:46:54.196117 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m01:46:54.196797 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a2c4-1dfb-803e-9da2a90c239f) - Closing
[0m01:46:54.466362 [debug] [Thread-2 (]: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:46:54.467409 [error] [Thread-2 (]: 1 of 4 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 1.89s]
[0m01:46:54.468324 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:46:54.468798 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:46:54.469341 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql.
[0m01:46:54.469771 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m01:46:54.479501 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m01:46:54.480188 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778'
[0m01:46:54.480648 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:46:54.496272 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:46:54.497418 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:46:54.506412 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:46:54.507549 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:46:54.508246 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:46:54.517466 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:46:55.476122 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a3e3-12fc-bdc4-b8caf802b58e) - Created
[0m01:46:56.079986 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-a413-15a5-a77f-56b379e79025
[0m01:46:56.081387 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: Close
[0m01:46:56.081921 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a3e3-12fc-bdc4-b8caf802b58e) - Closing
[0m01:46:56.352281 [debug] [Thread-2 (]: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:46:56.352821 [error] [Thread-2 (]: 2 of 4 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 1.87s]
[0m01:46:56.353320 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:46:56.353602 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:46:56.353885 [info ] [Thread-2 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m01:46:56.354426 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql.
[0m01:46:56.355082 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m01:46:56.355787 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321'
[0m01:46:56.356036 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:46:56.360102 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:46:56.360687 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:46:56.363246 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:46:56.363730 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:46:56.364130 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:46:56.364445 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:46:57.196392 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a4f0-19a6-8e7a-c2e06c0821eb) - Created
[0m01:46:59.809423 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-a516-1ab1-9f0f-1448149f6dc2
[0m01:46:59.811872 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: Close
[0m01:46:59.812596 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a4f0-19a6-8e7a-c2e06c0821eb) - Closing
[0m01:47:00.090613 [debug] [Thread-2 (]: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:47:00.091289 [error] [Thread-2 (]: 3 of 4 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 3.74s]
[0m01:47:00.091889 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:47:00.092281 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:47:00.092630 [info ] [Thread-2 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m01:47:00.093174 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql.
[0m01:47:00.093833 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m01:47:00.094876 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493'
[0m01:47:00.095226 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:47:00.100767 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:47:00.101433 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:47:00.104145 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:47:00.104753 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:47:00.105280 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:47:00.105613 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:47:00.955829 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a72c-119a-800f-4a76827dc867) - Created
[0m01:47:01.611590 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-a755-1ab7-9968-3474d0165760
[0m01:47:01.612185 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m01:47:01.612504 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-a72c-119a-800f-4a76827dc867) - Closing
[0m01:47:01.876556 [debug] [Thread-2 (]: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:47:01.877331 [error] [Thread-2 (]: 4 of 4 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 1.78s]
[0m01:47:01.878111 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:47:01.878588 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql.
[0m01:47:01.880570 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:47:01.880829 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:47:01.881243 [info ] [MainThread]: 
[0m01:47:01.881490 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 11.35 seconds (11.35s).
[0m01:47:01.882291 [debug] [MainThread]: Command end result
[0m01:47:01.914705 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:47:01.917389 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:47:01.925714 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m01:47:01.926327 [info ] [MainThread]: 
[0m01:47:01.927017 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m01:47:01.927493 [info ] [MainThread]: 
[0m01:47:01.928007 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:47:01.928648 [error] [MainThread]:   Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:47:01.928992 [info ] [MainThread]: 
[0m01:47:01.929471 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:47:01.929887 [info ] [MainThread]: 
[0m01:47:01.930350 [error] [MainThread]: [31mFailure in test not_null_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:47:01.930732 [error] [MainThread]:   Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:47:01.931038 [info ] [MainThread]: 
[0m01:47:01.931416 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:47:01.931826 [info ] [MainThread]: 
[0m01:47:01.932193 [error] [MainThread]: [31mFailure in test unique_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:47:01.932594 [error] [MainThread]:   Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:47:01.932972 [info ] [MainThread]: 
[0m01:47:01.933370 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:47:01.933745 [info ] [MainThread]: 
[0m01:47:01.934168 [error] [MainThread]: [31mFailure in test unique_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:47:01.934596 [error] [MainThread]:   Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:47:01.935006 [info ] [MainThread]: 
[0m01:47:01.935383 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:47:01.935778 [info ] [MainThread]: 
[0m01:47:01.936416 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=4 SKIP=0 NO-OP=0 TOTAL=4
[0m01:47:01.937466 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 16.471548, "process_in_blocks": "0", "process_kernel_time": 1.812374, "process_mem_max_rss": "263892", "process_out_blocks": "3472", "process_user_time": 5.93442}
[0m01:47:01.938470 [debug] [MainThread]: Command `dbt test` failed at 01:47:01.938306 after 16.47 seconds
[0m01:47:01.939076 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d24400260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d276b18b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1d276b1770>]}
[0m01:47:01.939578 [debug] [MainThread]: Flushing usage events
[0m01:47:02.777135 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:48:58.530592 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa274f6cc20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa27677c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa273effc50>]}


============================== 01:48:58.533653 | d98914b3-ba5d-41f6-81e2-9ae0d0fba362 ==============================
[0m01:48:58.533653 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:48:58.534280 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'introspect': 'True', 'static_parser': 'True', 'no_print': 'None', 'target_path': 'None', 'fail_fast': 'False', 'empty': 'None', 'use_colors': 'True', 'cache_selected_only': 'False', 'invocation_command': 'dbt test', 'use_experimental_parser': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'indirect_selection': 'eager', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'quiet': 'False', 'debug': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'version_check': 'True', 'log_format': 'default', 'printer_width': '80', 'send_anonymous_usage_stats': 'True'}
[0m01:48:59.242484 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:48:59.242978 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:48:59.243317 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:48:59.743057 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa274bf4fc0>]}
[0m01:48:59.794953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24e502140>]}
[0m01:48:59.795579 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:48:59.862156 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:48:59.862768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24e4ce050>]}
[0m01:48:59.872520 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m01:48:59.947594 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:48:59.948059 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m01:48:59.948273 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:48:59.976339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24dfbcc80>]}
[0m01:49:00.034595 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:49:00.036166 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:49:00.053056 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24df62190>]}
[0m01:49:00.053710 [info ] [MainThread]: Found 2 models, 4 data tests, 731 macros
[0m01:49:00.054149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24df286d0>]}
[0m01:49:00.056191 [info ] [MainThread]: 
[0m01:49:00.056639 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:49:00.057187 [info ] [MainThread]: 
[0m01:49:00.057789 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:49:00.058059 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:49:00.063865 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m01:49:00.064379 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m01:49:00.078570 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m01:49:00.079017 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m01:49:00.079289 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:49:00.964148 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff3-eeb0-174a-b1a6-02feec5ab972) - Created
[0m01:49:01.664092 [debug] [ThreadPool]: SQL status: OK in 1.580 seconds
[0m01:49:01.670039 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0eff3-eeb0-174a-b1a6-02feec5ab972, command-id=01f0eff3-eede-15ad-8643-d0829327b1e7) - Closing
[0m01:49:01.670594 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m01:49:01.670877 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0eff3-eeb0-174a-b1a6-02feec5ab972) - Closing
[0m01:49:01.962939 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'd98914b3-ba5d-41f6-81e2-9ae0d0fba362', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24e02b350>]}
[0m01:49:01.966769 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:49:01.967290 [info ] [Thread-2 (]: 1 of 4 START test not_null_my_first_dbt_model_id ............................... [RUN]
[0m01:49:01.968122 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710) - Creating connection
[0m01:49:01.968585 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710'
[0m01:49:01.968969 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:49:01.984112 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:49:01.986189 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:49:02.001640 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:49:02.002723 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"
[0m01:49:02.003326 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:49:02.003713 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:49:02.850419 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-efd6-12b5-a2a9-69b24c511f7f) - Created
[0m01:49:03.447804 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_first_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-effe-1388-ba77-a641a8dec6b1
[0m01:49:03.448980 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710: Close
[0m01:49:03.449493 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-efd6-12b5-a2a9-69b24c511f7f) - Closing
[0m01:49:03.744850 [debug] [Thread-2 (]: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:49:03.745565 [error] [Thread-2 (]: 1 of 4 ERROR not_null_my_first_dbt_model_id .................................... [[31mERROR[0m in 1.78s]
[0m01:49:03.746080 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710
[0m01:49:03.746383 [debug] [Thread-2 (]: Began running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:49:03.746655 [info ] [Thread-2 (]: 2 of 4 START test not_null_my_second_dbt_model_id .............................. [RUN]
[0m01:49:03.747099 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_first_dbt_model_id.5fb22c2710' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql.
[0m01:49:03.747847 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.not_null_my_second_dbt_model_id.151b76d778) - Creating connection
[0m01:49:03.749786 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778'
[0m01:49:03.750094 [debug] [Thread-2 (]: Began compiling node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:49:03.752998 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:49:03.753690 [debug] [Thread-2 (]: Began executing node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:49:03.758856 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:49:03.759446 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"
[0m01:49:03.759817 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
[0m01:49:03.760088 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:49:04.585816 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-f0e0-1e6f-ba54-7b3ec7ab70a4) - Created
[0m01:49:05.230537 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.not_null_my_second_dbt_model_id.151b76d778"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    



select id
from `dev`.`customer_bi`.`my_second_dbt_model`
where id is null



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-f10f-15a9-8c9a-5789635f991a
[0m01:49:05.232348 [debug] [Thread-2 (]: On test.customer_bi.not_null_my_second_dbt_model_id.151b76d778: Close
[0m01:49:05.233238 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-f0e0-1e6f-ba54-7b3ec7ab70a4) - Closing
[0m01:49:05.495510 [debug] [Thread-2 (]: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:49:05.496144 [error] [Thread-2 (]: 2 of 4 ERROR not_null_my_second_dbt_model_id ................................... [[31mERROR[0m in 1.75s]
[0m01:49:05.496685 [debug] [Thread-2 (]: Finished running node test.customer_bi.not_null_my_second_dbt_model_id.151b76d778
[0m01:49:05.497028 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:49:05.497328 [info ] [Thread-2 (]: 3 of 4 START test unique_my_first_dbt_model_id ................................. [RUN]
[0m01:49:05.497903 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_first_dbt_model_id.16e066b321) - Creating connection
[0m01:49:05.498497 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.not_null_my_second_dbt_model_id.151b76d778' to be skipped because of status 'error'.  Reason: Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql.
[0m01:49:05.498918 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321'
[0m01:49:05.499491 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:49:05.507218 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:49:05.508413 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:49:05.512743 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:49:05.513733 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"
[0m01:49:05.514534 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:49:05.515067 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:49:06.335946 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-f1ed-14c4-8f9a-a2a29e0fbcf7) - Created
[0m01:49:06.966508 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_first_dbt_model_id.16e066b321"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_first_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-f213-17c3-a279-5d65dcde43a1
[0m01:49:06.968502 [debug] [Thread-2 (]: On test.customer_bi.unique_my_first_dbt_model_id.16e066b321: Close
[0m01:49:06.969169 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-f1ed-14c4-8f9a-a2a29e0fbcf7) - Closing
[0m01:49:07.249721 [debug] [Thread-2 (]: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:49:07.250606 [error] [Thread-2 (]: 3 of 4 ERROR unique_my_first_dbt_model_id ...................................... [[31mERROR[0m in 1.75s]
[0m01:49:07.251542 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_first_dbt_model_id.16e066b321
[0m01:49:07.252007 [debug] [Thread-2 (]: Began running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:49:07.252267 [info ] [Thread-2 (]: 4 of 4 START test unique_my_second_dbt_model_id ................................ [RUN]
[0m01:49:07.252695 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_first_dbt_model_id.16e066b321' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql.
[0m01:49:07.253148 [debug] [Thread-2 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493) - Creating connection
[0m01:49:07.253656 [debug] [Thread-2 (]: Acquiring new databricks connection 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493'
[0m01:49:07.253895 [debug] [Thread-2 (]: Began compiling node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:49:07.256716 [debug] [Thread-2 (]: Writing injected SQL for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:49:07.257250 [debug] [Thread-2 (]: Began executing node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:49:07.259341 [debug] [Thread-2 (]: Writing runtime sql for node "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:49:07.260034 [debug] [Thread-2 (]: Using databricks connection "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"
[0m01:49:07.260529 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
[0m01:49:07.260839 [debug] [Thread-2 (]: Opening a new connection, currently in state init
[0m01:49:08.161039 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-f2f8-19ba-a548-60a001cff83e) - Created
[0m01:49:09.066146 [debug] [Thread-2 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

select
    id as unique_field,
    count(*) as n_records

from `dev`.`customer_bi`.`my_second_dbt_model`
where id is not null
group by id
having count(*) > 1



  
  
      
    ) dbt_internal_test
: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [TABLE_OR_VIEW_NOT_FOUND] org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.apache.spark.sql.catalyst.ExtendedAnalysisException: [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
	at org.apache.spark.sql.catalyst.ExtendedAnalysisException.copyPlan(ExtendedAnalysisException.scala:96)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1011)
	... 53 more
, operation-id=01f0eff3-f328-1489-bba6-8d88338b2872
[0m01:49:09.068159 [debug] [Thread-2 (]: On test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493: Close
[0m01:49:09.069115 [debug] [Thread-2 (]: Databricks adapter: Connection(session-id=01f0eff3-f2f8-19ba-a548-60a001cff83e) - Closing
[0m01:49:09.369141 [debug] [Thread-2 (]: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:49:09.369796 [error] [Thread-2 (]: 4 of 4 ERROR unique_my_second_dbt_model_id ..................................... [[31mERROR[0m in 2.12s]
[0m01:49:09.370334 [debug] [Thread-2 (]: Finished running node test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493
[0m01:49:09.370824 [debug] [Thread-5 (]: Marking all children of 'test.customer_bi.unique_my_second_dbt_model_id.57a0f8c493' to be skipped because of status 'error'.  Reason: Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql.
[0m01:49:09.372233 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:49:09.372563 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:49:09.372955 [info ] [MainThread]: 
[0m01:49:09.373244 [info ] [MainThread]: Finished running 4 data tests in 0 hours 0 minutes and 9.32 seconds (9.32s).
[0m01:49:09.373987 [debug] [MainThread]: Command end result
[0m01:49:09.397069 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:49:09.398566 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:49:09.403194 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m01:49:09.403435 [info ] [MainThread]: 
[0m01:49:09.403762 [info ] [MainThread]: [31mCompleted with 4 errors, 0 partial successes, and 0 warnings:[0m
[0m01:49:09.404023 [info ] [MainThread]: 
[0m01:49:09.404315 [error] [MainThread]: [31mFailure in test not_null_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:49:09.404618 [error] [MainThread]:   Database Error in test not_null_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:49:09.404857 [info ] [MainThread]: 
[0m01:49:09.405122 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_first_dbt_model_id.sql
[0m01:49:09.405399 [info ] [MainThread]: 
[0m01:49:09.405691 [error] [MainThread]: [31mFailure in test not_null_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:49:09.405960 [error] [MainThread]:   Database Error in test not_null_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 17 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:49:09.406191 [info ] [MainThread]: 
[0m01:49:09.406455 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/not_null_my_second_dbt_model_id.sql
[0m01:49:09.406675 [info ] [MainThread]: 
[0m01:49:09.406920 [error] [MainThread]: [31mFailure in test unique_my_first_dbt_model_id (models/example/schema.yml)[0m
[0m01:49:09.407170 [error] [MainThread]:   Database Error in test unique_my_first_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_first_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:49:09.407419 [info ] [MainThread]: 
[0m01:49:09.407681 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_first_dbt_model_id.sql
[0m01:49:09.407887 [info ] [MainThread]: 
[0m01:49:09.408131 [error] [MainThread]: [31mFailure in test unique_my_second_dbt_model_id (models/example/schema.yml)[0m
[0m01:49:09.408428 [error] [MainThread]:   Database Error in test unique_my_second_dbt_model_id (models/example/schema.yml)
  [TABLE_OR_VIEW_NOT_FOUND] The table or view `dev`.`customer_bi`.`my_second_dbt_model` cannot be found. Verify the spelling and correctness of the schema and catalog.
  If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.
  To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01; line 18 pos 5
  compiled code at target/run/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:49:09.408762 [info ] [MainThread]: 
[0m01:49:09.409052 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/example/schema.yml/unique_my_second_dbt_model_id.sql
[0m01:49:09.409285 [info ] [MainThread]: 
[0m01:49:09.409535 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=4 SKIP=0 NO-OP=0 TOTAL=4
[0m01:49:09.410110 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 10.924903, "process_in_blocks": "0", "process_kernel_time": 0.274197, "process_mem_max_rss": "263824", "process_out_blocks": "3464", "process_user_time": 3.306592}
[0m01:49:09.410538 [debug] [MainThread]: Command `dbt test` failed at 01:49:09.410460 after 10.93 seconds
[0m01:49:09.410868 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24c518260>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24ddc5810>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa24ddc56d0>]}
[0m01:49:09.411153 [debug] [MainThread]: Flushing usage events
[0m01:49:13.960356 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:49:57.073393 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa333d08c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa33553c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa332c9fc50>]}


============================== 01:49:57.076386 | e078afc6-beb4-43a4-9ea0-d634b7fa1f5e ==============================
[0m01:49:57.076386 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:49:57.076879 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'fail_fast': 'False', 'indirect_selection': 'eager', 'debug': 'False', 'cache_selected_only': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'write_json': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'target_path': 'None', 'static_parser': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'introspect': 'True', 'empty': 'None', 'partial_parse': 'True', 'log_format': 'default', 'invocation_command': 'dbt test', 'version_check': 'True', 'quiet': 'False', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_experimental_parser': 'False'}
[0m01:49:57.807945 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:49:57.808502 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:49:57.808822 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:49:58.372966 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e078afc6-beb4-43a4-9ea0-d634b7fa1f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa33398cfc0>]}
[0m01:49:58.438237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e078afc6-beb4-43a4-9ea0-d634b7fa1f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa31125a140>]}
[0m01:49:58.439332 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:49:58.509100 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:49:58.509814 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e078afc6-beb4-43a4-9ea0-d634b7fa1f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa311426050>]}
[0m01:49:58.518842 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m01:49:58.607597 [debug] [MainThread]: Partial parsing enabled: 3 files deleted, 0 files added, 0 files changed.
[0m01:49:58.608179 [debug] [MainThread]: Partial parsing: deleted file: customer_bi://models/example/my_second_dbt_model.sql
[0m01:49:58.608469 [debug] [MainThread]: Partial parsing: deleted file: customer_bi://models/example/my_first_dbt_model.sql
[0m01:49:58.638285 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.customer_bi.example
[0m01:49:58.642752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e078afc6-beb4-43a4-9ea0-d634b7fa1f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa310f00c80>]}
[0m01:49:58.689387 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:49:58.691080 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:49:58.705821 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e078afc6-beb4-43a4-9ea0-d634b7fa1f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa310cb5d30>]}
[0m01:49:58.706272 [info ] [MainThread]: Found 731 macros
[0m01:49:58.706608 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e078afc6-beb4-43a4-9ea0-d634b7fa1f5e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa310cc2410>]}
[0m01:49:58.707809 [warn ] [MainThread]: Nothing to do. Try checking your model configs and model specification args
[0m01:49:58.709589 [debug] [MainThread]: Command end result
[0m01:49:58.738981 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:49:58.740639 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:49:58.743294 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m01:49:58.743926 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": true, "command_wall_clock_time": 1.7191427, "process_in_blocks": "192", "process_kernel_time": 0.834417, "process_mem_max_rss": "253892", "process_out_blocks": "4824", "process_user_time": 4.370263}
[0m01:49:58.744433 [debug] [MainThread]: Command `dbt test` succeeded at 01:49:58.744342 after 1.72 seconds
[0m01:49:58.744783 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa332e8b290>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa310c594f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa310c59650>]}
[0m01:49:58.745083 [debug] [MainThread]: Flushing usage events
[0m01:49:59.552039 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:37:46.914210 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb29c79780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb28c7a9e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb28c7a980>]}


============================== 00:37:46.918673 | f9dc47d6-d1c4-4730-9ce5-cbdba94f951b ==============================
[0m00:37:46.918673 [info ] [MainThread]: Running with dbt=1.10.0
[0m00:37:46.919506 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'write_json': 'True', 'log_cache_events': 'False', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'version_check': 'True', 'fail_fast': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'debug': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'no_print': 'None', 'quiet': 'False', 'empty': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'log_format': 'default', 'invocation_command': 'dbt ', 'target_path': 'None', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True'}
[0m00:37:47.330303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f9dc47d6-d1c4-4730-9ce5-cbdba94f951b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb294c7280>]}
[0m00:37:47.365966 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m00:37:47.369438 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m00:37:47.380559 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.5928156, "process_in_blocks": "288", "process_kernel_time": 0.189806, "process_mem_max_rss": "96632", "process_out_blocks": "8", "process_user_time": 2.837105}
[0m00:37:47.381770 [debug] [MainThread]: Command `cli deps` succeeded at 00:37:47.381493 after 0.59 seconds
[0m00:37:47.382599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb29c79780>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb294c7280>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ffb28b95420>]}
[0m00:37:47.383715 [debug] [MainThread]: Flushing usage events
[0m00:37:48.349136 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m00:46:35.176546 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd61ae156f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd619e1c7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd619e1c760>]}


============================== 00:46:35.180251 | 7db5ddd3-f502-4bcd-b838-781a5de7ba9c ==============================
[0m00:46:35.180251 [info ] [MainThread]: Running with dbt=1.10.0
[0m00:46:35.180964 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'version_check': 'True', 'debug': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'fail_fast': 'False', 'warn_error': 'None', 'use_colors': 'True', 'use_experimental_parser': 'False', 'empty': 'None', 'quiet': 'False', 'no_print': 'None', 'log_format': 'default', 'static_parser': 'True', 'invocation_command': 'dbt ', 'introspect': 'True', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True'}
[0m00:46:35.194584 [info ] [MainThread]: dbt version: 1.10.0
[0m00:46:35.195458 [info ] [MainThread]: python version: 3.10.12
[0m00:46:35.196222 [info ] [MainThread]: python path: /bin/python3
[0m00:46:35.197657 [info ] [MainThread]: os info: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
[0m00:46:35.986527 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m00:46:35.987902 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m00:46:35.988404 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m00:46:36.565233 [info ] [MainThread]: Using profiles dir at /home/ubuntu/.dbt
[0m00:46:36.565832 [info ] [MainThread]: Using profiles.yml file at /home/ubuntu/.dbt/profiles.yml
[0m00:46:36.566385 [info ] [MainThread]: Using dbt_project.yml file at /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/dbt_project.yml
[0m00:46:36.566719 [info ] [MainThread]: adapter type: databricks
[0m00:46:36.567185 [info ] [MainThread]: adapter version: 1.10.3
[0m00:46:36.686387 [info ] [MainThread]: Configuration:
[0m00:46:36.687037 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m00:46:36.687386 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m00:46:36.687740 [info ] [MainThread]: Required dependencies:
[0m00:46:36.688069 [debug] [MainThread]: Executing "git --help"
[0m00:46:36.690489 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m00:46:36.691281 [debug] [MainThread]: STDERR: "b''"
[0m00:46:36.691799 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m00:46:36.692316 [info ] [MainThread]: Connection:
[0m00:46:36.692740 [info ] [MainThread]:   host: adb-3183350029643709.9.azuredatabricks.net
[0m00:46:36.693061 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/a418758e84eae08c
[0m00:46:36.693385 [info ] [MainThread]:   catalog: dev
[0m00:46:36.693845 [info ] [MainThread]:   schema: customer_bi
[0m00:46:36.694709 [info ] [MainThread]: Registered adapter: databricks=1.10.3
[0m00:46:36.819956 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug, idle-time=0s, language=None, compute-name=) - Creating connection
[0m00:46:36.820482 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m00:46:36.820927 [debug] [MainThread]: Using databricks connection "debug"
[0m00:46:36.821312 [debug] [MainThread]: On debug: select 1 as id
[0m00:46:36.821664 [debug] [MainThread]: Opening a new connection, currently in state init
[0m00:46:38.913201 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0f0b4-6217-1841-b77d-75125278cd9e) - Created
[0m00:46:44.334490 [debug] [MainThread]: SQL status: OK in 7.510 seconds
[0m00:46:44.336613 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0f0b4-6217-1841-b77d-75125278cd9e, command-id=01f0f0b4-62d6-132a-b68e-c2e38d1e4c6b) - Closing
[0m00:46:44.644878 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m00:46:44.645813 [info ] [MainThread]: [32mAll checks passed![0m
[0m00:46:44.651638 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 9.548514, "process_in_blocks": "4064", "process_kernel_time": 1.056622, "process_mem_max_rss": "234276", "process_out_blocks": "16", "process_user_time": 6.042242}
[0m00:46:44.653010 [debug] [MainThread]: Command `cli debug` succeeded at 00:46:44.652736 after 9.55 seconds
[0m00:46:44.653859 [debug] [MainThread]: Connection 'debug' was properly closed.
[0m00:46:44.654609 [debug] [MainThread]: On debug: Close
[0m00:46:44.659792 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0f0b4-6217-1841-b77d-75125278cd9e) - Closing
[0m00:46:45.004580 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd61ae156f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd61c21e920>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd61ac9add0>]}
[0m00:46:45.005928 [debug] [MainThread]: Flushing usage events
[0m00:46:46.030699 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:53:59.269429 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe62f458ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe630c5c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe62e3dbc50>]}


============================== 10:53:59.273913 | f7821246-bfb9-4541-87da-a1254a9ffcdf ==============================
[0m10:53:59.273913 [info ] [MainThread]: Running with dbt=1.11.2
[0m10:53:59.274578 [debug] [MainThread]: running dbt with arguments {'fail_fast': 'False', 'write_json': 'True', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'printer_width': '80', 'profiles_dir': '/home/ubuntu/.dbt', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt ', 'warn_error': 'None', 'cache_selected_only': 'False', 'quiet': 'False', 'no_print': 'None', 'use_colors': 'True', 'static_parser': 'True', 'target_path': 'None', 'debug': 'False', 'log_cache_events': 'False', 'log_format': 'default', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'indirect_selection': 'eager', 'partial_parse': 'True', 'empty': 'None', 'version_check': 'True'}
[0m10:53:59.712190 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f7821246-bfb9-4541-87da-a1254a9ffcdf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe62f0a8fc0>]}
[0m10:53:59.769023 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:53:59.790493 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:53:59.791704 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.6926386, "process_in_blocks": "672", "process_kernel_time": 0.877406, "process_mem_max_rss": "103068", "process_out_blocks": "192", "process_user_time": 2.94558}
[0m10:53:59.792268 [debug] [MainThread]: Command `cli deps` succeeded at 10:53:59.792131 after 0.69 seconds
[0m10:53:59.792687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe62e1bcc00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe62e1bd040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe62e627e50>]}
[0m10:53:59.793044 [debug] [MainThread]: Flushing usage events
[0m10:54:00.732951 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m01:53:54.408424 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf0e408c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf0fc8c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf0d35fc50>]}


============================== 01:53:54.413099 | 84ac3def-ebfc-45e6-822e-9d912f93eba7 ==============================
[0m01:53:54.413099 [info ] [MainThread]: Running with dbt=1.11.2
[0m01:53:54.413665 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'printer_width': '80', 'debug': 'False', 'use_colors': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'send_anonymous_usage_stats': 'True', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'write_json': 'True', 'cache_selected_only': 'False', 'fail_fast': 'False', 'target_path': 'None', 'introspect': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'quiet': 'False', 'warn_error': 'None', 'log_cache_events': 'False', 'static_parser': 'True', 'no_print': 'None'}
[0m01:53:56.872978 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m01:53:56.873694 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m01:53:56.874080 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m01:54:02.031106 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf0e084fc0>]}
[0m01:54:02.253480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcee79c2140>]}
[0m01:54:02.257650 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m01:54:02.626260 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:54:02.627002 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcee798de50>]}
[0m01:54:02.651470 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m01:54:03.124062 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m01:54:03.124918 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m01:54:03.125417 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m01:54:03.301259 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcee7679310>]}
[0m01:54:03.620381 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:54:03.624091 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:54:03.656709 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcee7a204b0>]}
[0m01:54:03.657302 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m01:54:03.657668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcee7581f30>]}
[0m01:54:03.659842 [info ] [MainThread]: 
[0m01:54:03.660297 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m01:54:03.660623 [info ] [MainThread]: 
[0m01:54:03.661270 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:54:03.661635 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:54:03.689877 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m01:54:03.690695 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m01:54:03.757037 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m01:54:03.757703 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m01:54:03.758074 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:54:04.750696 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f186-f881-1b6e-becf-aada64f23019) - Created
[0m01:54:05.475103 [debug] [ThreadPool]: SQL status: OK in 1.720 seconds
[0m01:54:05.492523 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f186-f881-1b6e-becf-aada64f23019, command-id=01f0f186-f8b1-1514-8570-1e0e5c7854ed) - Closing
[0m01:54:05.493389 [debug] [ThreadPool]: On list_dev: Close
[0m01:54:05.493829 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f186-f881-1b6e-becf-aada64f23019) - Closing
[0m01:54:05.811975 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m01:54:05.812682 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m01:54:05.825597 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m01:54:05.826111 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m01:54:05.826522 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m01:54:06.721877 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f186-f9c5-1baf-b142-1fc454620e4d) - Created
[0m01:54:07.879017 [debug] [ThreadPool]: SQL status: OK in 2.050 seconds
[0m01:54:07.890854 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f186-f9c5-1baf-b142-1fc454620e4d, command-id=01f0f186-fa02-1dff-85d3-400d3662298b) - Closing
[0m01:54:07.891706 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m01:54:07.892155 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f186-f9c5-1baf-b142-1fc454620e4d) - Closing
[0m01:54:08.168650 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcee762a8d0>]}
[0m01:54:08.200639 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_customer
[0m01:54:08.201493 [info ] [Thread-3 (]: 1 of 5 START sql table model customer_bi.bronze_customer ....................... [RUN]
[0m01:54:08.210987 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m01:54:08.211451 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m01:54:08.220852 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_customer
[0m01:54:08.238682 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m01:54:08.240452 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_customer
[0m01:54:08.313966 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m01:54:08.314875 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m01:54:08.315553 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcee7645a70>]}
[0m01:54:08.450901 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m01:54:08.479867 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m01:54:08.480604 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_customer`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  
[0m01:54:08.481098 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m01:54:09.362198 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f186-fb80-1c4f-88bc-0bbc05cf1d29) - Created
[0m01:54:12.750468 [debug] [Thread-3 (]: SQL status: OK in 4.270 seconds
[0m01:54:12.751550 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f186-fb80-1c4f-88bc-0bbc05cf1d29, command-id=01f0f186-fba7-1f61-9d79-911f8e56911b) - Closing
[0m01:54:12.759904 [debug] [Thread-3 (]: Applying tags to relation None
[0m01:54:12.773824 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: Close
[0m01:54:12.774364 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f186-fb80-1c4f-88bc-0bbc05cf1d29) - Closing
[0m01:54:13.037286 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf0e20e530>]}
[0m01:54:13.037937 [info ] [Thread-3 (]: 1 of 5 OK created sql table model customer_bi.bronze_customer .................. [[32mOK[0m in 4.82s]
[0m01:54:13.038417 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_customer
[0m01:54:13.038774 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_date
[0m01:54:13.039135 [info ] [Thread-3 (]: 2 of 5 START sql table model customer_bi.bronze_date ........................... [RUN]
[0m01:54:13.039742 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m01:54:13.039985 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m01:54:13.040235 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_date
[0m01:54:13.042158 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m01:54:13.042692 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_date
[0m01:54:13.044934 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m01:54:13.046547 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m01:54:13.047115 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m01:54:13.047545 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_date`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_date`
  
[0m01:54:13.047803 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m01:54:13.885102 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f186-fe6c-11f2-a81e-85dad9dc4aa5) - Created
[0m01:54:16.429960 [debug] [Thread-3 (]: SQL status: OK in 3.380 seconds
[0m01:54:16.431945 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f186-fe6c-11f2-a81e-85dad9dc4aa5, command-id=01f0f186-fe9e-1a0f-b531-279eef2e4aba) - Closing
[0m01:54:16.433380 [debug] [Thread-3 (]: Applying tags to relation None
[0m01:54:16.435757 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: Close
[0m01:54:16.436205 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f186-fe6c-11f2-a81e-85dad9dc4aa5) - Closing
[0m01:54:16.697666 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcee75f7150>]}
[0m01:54:16.698778 [info ] [Thread-3 (]: 2 of 5 OK created sql table model customer_bi.bronze_date ...................... [[32mOK[0m in 3.66s]
[0m01:54:16.699690 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_date
[0m01:54:16.700316 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_returns
[0m01:54:16.701204 [info ] [Thread-3 (]: 3 of 5 START sql table model customer_bi.bronze_returns ........................ [RUN]
[0m01:54:16.702268 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m01:54:16.702783 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m01:54:16.703338 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_returns
[0m01:54:16.730481 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m01:54:16.731608 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_returns
[0m01:54:16.737010 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m01:54:16.741495 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m01:54:16.742512 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m01:54:16.743075 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_returns`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  
[0m01:54:16.743453 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m01:54:17.538086 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f187-00d1-17d1-9aeb-b283c5aaaec2) - Created
[0m01:54:19.488270 [debug] [Thread-3 (]: SQL status: OK in 2.740 seconds
[0m01:54:19.489839 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f187-00d1-17d1-9aeb-b283c5aaaec2, command-id=01f0f187-00f9-17a4-941b-95176e4ae0f6) - Closing
[0m01:54:19.491056 [debug] [Thread-3 (]: Applying tags to relation None
[0m01:54:19.493106 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: Close
[0m01:54:19.493662 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f187-00d1-17d1-9aeb-b283c5aaaec2) - Closing
[0m01:54:19.776977 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcee7440520>]}
[0m01:54:19.777974 [info ] [Thread-3 (]: 3 of 5 OK created sql table model customer_bi.bronze_returns ................... [[32mOK[0m in 3.07s]
[0m01:54:19.778784 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_returns
[0m01:54:19.779335 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_sales
[0m01:54:19.780018 [info ] [Thread-3 (]: 4 of 5 START sql table model customer_bi.bronze_sales .......................... [RUN]
[0m01:54:19.780831 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m01:54:19.781337 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m01:54:19.781823 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_sales
[0m01:54:19.795271 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m01:54:19.934696 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_sales
[0m01:54:19.937994 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m01:54:19.941668 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m01:54:20.196666 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m01:54:20.197385 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_sales`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  
[0m01:54:20.197780 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m01:54:21.108982 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f187-0327-10e0-a621-53a13f43a1bc) - Created
[0m01:54:23.168360 [debug] [Thread-3 (]: SQL status: OK in 2.970 seconds
[0m01:54:23.170223 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f187-0327-10e0-a621-53a13f43a1bc, command-id=01f0f187-034c-1fc3-8319-5ebdb6b68555) - Closing
[0m01:54:23.171674 [debug] [Thread-3 (]: Applying tags to relation None
[0m01:54:23.173983 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: Close
[0m01:54:23.174633 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f187-0327-10e0-a621-53a13f43a1bc) - Closing
[0m01:54:23.425102 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf0ac9b7d0>]}
[0m01:54:23.426284 [info ] [Thread-3 (]: 4 of 5 OK created sql table model customer_bi.bronze_sales ..................... [[32mOK[0m in 3.64s]
[0m01:54:23.427128 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_sales
[0m01:54:23.427655 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_store
[0m01:54:23.428251 [info ] [Thread-3 (]: 5 of 5 START sql table model customer_bi.bronze_store .......................... [RUN]
[0m01:54:23.428949 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m01:54:23.429346 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m01:54:23.429713 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_store
[0m01:54:23.441463 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m01:54:23.453002 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_store
[0m01:54:23.457429 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m01:54:23.460333 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m01:54:23.461388 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m01:54:23.462122 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_store`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_store`
  
[0m01:54:23.462686 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m01:54:24.413006 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f187-0544-12f5-b635-f4e82fe43962) - Created
[0m01:54:26.367060 [debug] [Thread-3 (]: SQL status: OK in 2.900 seconds
[0m01:54:26.368608 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f187-0544-12f5-b635-f4e82fe43962, command-id=01f0f187-0571-1d57-b9a1-a1226e0e7b4e) - Closing
[0m01:54:26.369632 [debug] [Thread-3 (]: Applying tags to relation None
[0m01:54:26.371551 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: Close
[0m01:54:26.372320 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f187-0544-12f5-b635-f4e82fe43962) - Closing
[0m01:54:26.635844 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '84ac3def-ebfc-45e6-822e-9d912f93eba7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcee743fef0>]}
[0m01:54:26.636672 [info ] [Thread-3 (]: 5 of 5 OK created sql table model customer_bi.bronze_store ..................... [[32mOK[0m in 3.21s]
[0m01:54:26.637279 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_store
[0m01:54:26.639398 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m01:54:26.639804 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m01:54:26.640300 [info ] [MainThread]: 
[0m01:54:26.640713 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 22.98 seconds (22.98s).
[0m01:54:26.641935 [debug] [MainThread]: Command end result
[0m01:54:26.730038 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m01:54:26.732919 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m01:54:26.743706 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m01:54:26.744530 [info ] [MainThread]: 
[0m01:54:26.745830 [info ] [MainThread]: [32mCompleted successfully[0m
[0m01:54:26.749135 [info ] [MainThread]: 
[0m01:54:26.749717 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m01:54:26.750784 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 30.023684, "process_in_blocks": "1072", "process_kernel_time": 3.113982, "process_mem_max_rss": "263848", "process_out_blocks": "3448", "process_user_time": 10.578211}
[0m01:54:26.751409 [debug] [MainThread]: Command `dbt run` succeeded at 01:54:26.751274 after 30.02 seconds
[0m01:54:26.751934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcee776b0b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcf0ef4d250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fcee72fc410>]}
[0m01:54:26.752302 [debug] [MainThread]: Flushing usage events
[0m01:54:28.475360 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:00:09.192599 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7dd454ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7deccc190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7dc39bc50>]}


============================== 02:00:09.198179 | 1f10e744-8e33-467b-92f6-7f073077604d ==============================
[0m02:00:09.198179 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:00:09.198778 [debug] [MainThread]: running dbt with arguments {'use_experimental_parser': 'False', 'version_check': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'no_print': 'None', 'cache_selected_only': 'False', 'log_format': 'default', 'use_colors': 'True', 'write_json': 'True', 'partial_parse': 'True', 'static_parser': 'True', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'None', 'warn_error': 'None', 'log_cache_events': 'False', 'target_path': 'None', 'fail_fast': 'False', 'quiet': 'False', 'introspect': 'True', 'debug': 'False', 'printer_width': '80', 'send_anonymous_usage_stats': 'True', 'invocation_command': 'dbt ', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs'}
[0m02:00:10.634936 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1f10e744-8e33-467b-92f6-7f073077604d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7dd098fc0>]}
[0m02:00:10.664411 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m02:00:10.666991 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m02:00:10.669407 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 1.7568682, "process_in_blocks": "0", "process_kernel_time": 1.093373, "process_mem_max_rss": "103236", "process_out_blocks": "16", "process_user_time": 4.185387}
[0m02:00:10.669925 [debug] [MainThread]: Command `cli deps` succeeded at 02:00:10.669814 after 1.76 seconds
[0m02:00:10.670360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7dc31c8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7dc31cf30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa7dc5efd50>]}
[0m02:00:10.670701 [debug] [MainThread]: Flushing usage events
[0m02:00:13.586647 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:02:03.333612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5c3410c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5c4c40190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5c23a3c50>]}


============================== 02:02:03.337637 | fc7e0b59-871d-42c7-b935-f3ff66f09d2c ==============================
[0m02:02:03.337637 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:02:03.338382 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'log_cache_events': 'False', 'version_check': 'True', 'quiet': 'False', 'printer_width': '80', 'use_colors': 'True', 'fail_fast': 'False', 'empty': 'False', 'invocation_command': 'dbt run', 'partial_parse': 'True', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'introspect': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'write_json': 'True', 'use_experimental_parser': 'False', 'target_path': 'None', 'cache_selected_only': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'log_format': 'default', 'static_parser': 'True'}
[0m02:02:06.680630 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:02:06.681321 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:02:06.681860 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:02:08.785372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5c3098fc0>]}
[0m02:02:08.955898 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a095e140>]}
[0m02:02:08.963228 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m02:02:09.187656 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:02:09.188496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a0b25e50>]}
[0m02:02:09.219287 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m02:02:09.598791 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:02:09.599254 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m02:02:09.599570 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:02:09.610685 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dev.bronze
[0m02:02:09.711699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a0601310>]}
[0m02:02:10.057603 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:02:10.060329 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:02:10.077777 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a09a04b0>]}
[0m02:02:10.078505 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m02:02:10.079051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a058df30>]}
[0m02:02:10.089475 [info ] [MainThread]: 
[0m02:02:10.090213 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:02:10.090662 [info ] [MainThread]: 
[0m02:02:10.091466 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:02:10.091870 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:02:10.168379 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m02:02:10.169333 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m02:02:10.233564 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m02:02:10.234220 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m02:02:10.234617 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:02:18.688688 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f188-1f95-1404-8754-35975f6f39cf) - Created
[0m02:02:19.530945 [debug] [ThreadPool]: SQL status: OK in 9.300 seconds
[0m02:02:19.878848 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f188-1f95-1404-8754-35975f6f39cf, command-id=01f0f188-1fbf-17d1-a41b-6ad1be652f64) - Closing
[0m02:02:19.880030 [debug] [ThreadPool]: On list_dev: Close
[0m02:02:19.880639 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f188-1f95-1404-8754-35975f6f39cf) - Closing
[0m02:02:20.188493 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m02:02:20.189345 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m02:02:20.207299 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m02:02:20.208268 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m02:02:20.208894 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:02:22.107102 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f188-21b6-11b4-a75a-44d2e55f1a5c) - Created
[0m02:02:22.876596 [debug] [ThreadPool]: SQL status: OK in 2.670 seconds
[0m02:02:22.940965 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f188-21b6-11b4-a75a-44d2e55f1a5c, command-id=01f0f188-21e9-1337-a1d7-303bd84b41d2) - Closing
[0m02:02:22.942363 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m02:02:22.942917 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f188-21b6-11b4-a75a-44d2e55f1a5c) - Closing
[0m02:02:23.232635 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a05ee390>]}
[0m02:02:23.255329 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_customer
[0m02:02:23.256338 [info ] [Thread-3 (]: 1 of 5 START sql view model customer_bi.bronze_customer ........................ [RUN]
[0m02:02:23.257176 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m02:02:23.257589 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m02:02:23.258010 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_customer
[0m02:02:23.274458 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m02:02:23.275834 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_customer
[0m02:02:23.334687 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:02:23.338265 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:02:23.339085 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a071d2e0>]}
[0m02:02:23.367651 [debug] [Thread-3 (]: Dropping relation `dev`.`customer_bi`.`bronze_customer` because it is of type table
[0m02:02:23.396307 [debug] [Thread-3 (]: Applying DROP to: `dev`.`customer_bi`.`bronze_customer`
[0m02:02:23.412327 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m02:02:23.413056 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */
drop table if exists `dev`.`customer_bi`.`bronze_customer`
[0m02:02:23.413582 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:02:24.310484 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f188-232b-1319-8e6f-47f89a07c473) - Created
[0m02:02:25.128755 [debug] [Thread-3 (]: SQL status: OK in 1.720 seconds
[0m02:02:25.130160 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f188-232b-1319-8e6f-47f89a07c473, command-id=01f0f188-2352-1523-8c3f-fef459bc07aa) - Closing
[0m02:02:25.159807 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_customer`
[0m02:02:25.190252 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m02:02:25.191166 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m02:02:25.199934 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_customer`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  )

[0m02:02:26.019592 [debug] [Thread-3 (]: SQL status: OK in 0.820 seconds
[0m02:02:26.020703 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f188-232b-1319-8e6f-47f89a07c473, command-id=01f0f188-23e6-1897-b2b2-89553088987d) - Closing
[0m02:02:26.040135 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:02:26.055860 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: Close
[0m02:02:26.056411 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f188-232b-1319-8e6f-47f89a07c473) - Closing
[0m02:02:26.325833 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5c3222530>]}
[0m02:02:26.326699 [info ] [Thread-3 (]: 1 of 5 OK created sql view model customer_bi.bronze_customer ................... [[32mOK[0m in 3.07s]
[0m02:02:26.327351 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_customer
[0m02:02:26.327726 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_date
[0m02:02:26.328256 [info ] [Thread-3 (]: 2 of 5 START sql view model customer_bi.bronze_date ............................ [RUN]
[0m02:02:26.328889 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m02:02:26.329268 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m02:02:26.329597 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_date
[0m02:02:26.347036 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m02:02:26.348360 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_date
[0m02:02:26.360641 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:02:26.361865 [debug] [Thread-3 (]: Dropping relation `dev`.`customer_bi`.`bronze_date` because it is of type table
[0m02:02:26.367759 [debug] [Thread-3 (]: Applying DROP to: `dev`.`customer_bi`.`bronze_date`
[0m02:02:26.368680 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m02:02:26.369183 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */
drop table if exists `dev`.`customer_bi`.`bronze_date`
[0m02:02:26.369559 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:02:27.187412 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f188-250b-17e1-b2de-7f4fee22760c) - Created
[0m02:02:27.809975 [debug] [Thread-3 (]: SQL status: OK in 1.440 seconds
[0m02:02:27.811306 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f188-250b-17e1-b2de-7f4fee22760c, command-id=01f0f188-2530-1e62-931c-71e4b312e697) - Closing
[0m02:02:27.812232 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_date`
[0m02:02:27.813052 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m02:02:27.813644 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m02:02:27.814106 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_date`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_date`
  )

[0m02:02:28.514561 [debug] [Thread-3 (]: SQL status: OK in 0.700 seconds
[0m02:02:28.515995 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f188-250b-17e1-b2de-7f4fee22760c, command-id=01f0f188-259b-1259-8d94-926a33232ec1) - Closing
[0m02:02:28.516999 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:02:28.518078 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: Close
[0m02:02:28.518558 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f188-250b-17e1-b2de-7f4fee22760c) - Closing
[0m02:02:28.835308 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a05833d0>]}
[0m02:02:28.836205 [info ] [Thread-3 (]: 2 of 5 OK created sql view model customer_bi.bronze_date ....................... [[32mOK[0m in 2.51s]
[0m02:02:28.837038 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_date
[0m02:02:28.837515 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_returns
[0m02:02:28.838115 [info ] [Thread-3 (]: 3 of 5 START sql view model customer_bi.bronze_returns ......................... [RUN]
[0m02:02:28.838708 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m02:02:28.839083 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m02:02:28.839491 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_returns
[0m02:02:28.842608 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m02:02:28.843280 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_returns
[0m02:02:28.855341 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:02:28.856760 [debug] [Thread-3 (]: Dropping relation `dev`.`customer_bi`.`bronze_returns` because it is of type table
[0m02:02:28.869637 [debug] [Thread-3 (]: Applying DROP to: `dev`.`customer_bi`.`bronze_returns`
[0m02:02:28.870889 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m02:02:28.871374 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */
drop table if exists `dev`.`customer_bi`.`bronze_returns`
[0m02:02:28.871753 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:02:29.797661 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f188-26a3-1e71-b9ec-1caf2682cd88) - Created
[0m02:02:30.443482 [debug] [Thread-3 (]: SQL status: OK in 1.570 seconds
[0m02:02:30.444563 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f188-26a3-1e71-b9ec-1caf2682cd88, command-id=01f0f188-26d0-10da-b5f5-795e210287d2) - Closing
[0m02:02:30.445551 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_returns`
[0m02:02:30.446970 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m02:02:30.447599 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m02:02:30.447940 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_returns`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  )

[0m02:02:31.181473 [debug] [Thread-3 (]: SQL status: OK in 0.730 seconds
[0m02:02:31.182835 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f188-26a3-1e71-b9ec-1caf2682cd88, command-id=01f0f188-2739-18ae-a2a1-ffa7b90c159a) - Closing
[0m02:02:31.183670 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:02:31.184538 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: Close
[0m02:02:31.184970 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f188-26a3-1e71-b9ec-1caf2682cd88) - Closing
[0m02:02:31.428970 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a025f5b0>]}
[0m02:02:31.429872 [info ] [Thread-3 (]: 3 of 5 OK created sql view model customer_bi.bronze_returns .................... [[32mOK[0m in 2.59s]
[0m02:02:31.430718 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_returns
[0m02:02:31.431236 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_sales
[0m02:02:31.431867 [info ] [Thread-3 (]: 4 of 5 START sql view model customer_bi.bronze_sales ........................... [RUN]
[0m02:02:31.432666 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m02:02:31.433130 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m02:02:31.433543 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_sales
[0m02:02:31.444876 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m02:02:31.445881 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_sales
[0m02:02:31.448542 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:02:31.449809 [debug] [Thread-3 (]: Dropping relation `dev`.`customer_bi`.`bronze_sales` because it is of type table
[0m02:02:31.462587 [debug] [Thread-3 (]: Applying DROP to: `dev`.`customer_bi`.`bronze_sales`
[0m02:02:31.463801 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m02:02:31.464343 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */
drop table if exists `dev`.`customer_bi`.`bronze_sales`
[0m02:02:31.464735 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:02:32.277438 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f188-2849-13a4-a90c-901160d2b738) - Created
[0m02:02:32.920698 [debug] [Thread-3 (]: SQL status: OK in 1.460 seconds
[0m02:02:32.922397 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f188-2849-13a4-a90c-901160d2b738, command-id=01f0f188-2871-1705-92f8-6b65072a3713) - Closing
[0m02:02:32.923824 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_sales`
[0m02:02:32.925056 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m02:02:32.934476 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m02:02:32.935129 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_sales`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  )

[0m02:02:33.617784 [debug] [Thread-3 (]: SQL status: OK in 0.680 seconds
[0m02:02:33.619225 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f188-2849-13a4-a90c-901160d2b738, command-id=01f0f188-28df-15c1-a8d4-eca70d2cf61a) - Closing
[0m02:02:33.620359 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:02:33.621357 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: Close
[0m02:02:33.621874 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f188-2849-13a4-a90c-901160d2b738) - Closing
[0m02:02:33.873458 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a03b7950>]}
[0m02:02:33.874534 [info ] [Thread-3 (]: 4 of 5 OK created sql view model customer_bi.bronze_sales ...................... [[32mOK[0m in 2.44s]
[0m02:02:33.875404 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_sales
[0m02:02:33.875963 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_store
[0m02:02:33.876697 [info ] [Thread-3 (]: 5 of 5 START sql view model customer_bi.bronze_store ........................... [RUN]
[0m02:02:33.877519 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m02:02:33.877999 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m02:02:33.878423 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_store
[0m02:02:33.977387 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m02:02:33.978620 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_store
[0m02:02:33.981458 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:02:33.982672 [debug] [Thread-3 (]: Dropping relation `dev`.`customer_bi`.`bronze_store` because it is of type table
[0m02:02:33.985806 [debug] [Thread-3 (]: Applying DROP to: `dev`.`customer_bi`.`bronze_store`
[0m02:02:33.986746 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m02:02:33.987252 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */
drop table if exists `dev`.`customer_bi`.`bronze_store`
[0m02:02:33.987723 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:02:34.962309 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f188-2a0e-1a01-bd55-4ecc809d0f38) - Created
[0m02:02:38.011270 [debug] [Thread-3 (]: SQL status: OK in 4.020 seconds
[0m02:02:38.012876 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f188-2a0e-1a01-bd55-4ecc809d0f38, command-id=01f0f188-2a37-1ff5-adc6-5e09fd9f198a) - Closing
[0m02:02:38.014121 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_store`
[0m02:02:38.015117 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m02:02:38.015874 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m02:02:38.016373 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_store`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_store`
  )

[0m02:02:38.714006 [debug] [Thread-3 (]: SQL status: OK in 0.700 seconds
[0m02:02:38.715412 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f188-2a0e-1a01-bd55-4ecc809d0f38, command-id=01f0f188-2aa3-1da9-9c7f-2681dc82e679) - Closing
[0m02:02:38.716686 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:02:38.717891 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: Close
[0m02:02:38.718420 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f188-2a0e-1a01-bd55-4ecc809d0f38) - Closing
[0m02:02:39.034652 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fc7e0b59-871d-42c7-b935-f3ff66f09d2c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a03b7fb0>]}
[0m02:02:39.035688 [info ] [Thread-3 (]: 5 of 5 OK created sql view model customer_bi.bronze_store ...................... [[32mOK[0m in 5.16s]
[0m02:02:39.036602 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_store
[0m02:02:39.039016 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:02:39.039446 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:02:39.040143 [info ] [MainThread]: 
[0m02:02:39.040637 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 28.95 seconds (28.95s).
[0m02:02:39.064557 [debug] [MainThread]: Command end result
[0m02:02:39.153612 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:02:39.166289 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:02:39.183688 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m02:02:39.184241 [info ] [MainThread]: 
[0m02:02:39.184871 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:02:39.185423 [info ] [MainThread]: 
[0m02:02:39.185878 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m02:02:39.209438 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 33.649494, "process_in_blocks": "0", "process_kernel_time": 3.227288, "process_mem_max_rss": "261440", "process_out_blocks": "3456", "process_user_time": 11.325502}
[0m02:02:39.210379 [debug] [MainThread]: Command `dbt run` succeeded at 02:02:39.210173 after 33.65 seconds
[0m02:02:39.211326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5c2aa2390>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a029fa10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc5a029ff50>]}
[0m02:02:39.212273 [debug] [MainThread]: Flushing usage events
[0m02:02:40.272407 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:07:02.998555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1de030ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1df8a8190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1dcf77c50>]}


============================== 02:07:03.009298 | 8d8afc65-2d1c-4120-9c4e-c1ac249af589 ==============================
[0m02:07:03.009298 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:07:03.009967 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'introspect': 'True', 'static_parser': 'True', 'debug': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'log_format': 'default', 'cache_selected_only': 'False', 'fail_fast': 'False', 'invocation_command': 'dbt ', 'no_print': 'None', 'empty': 'None', 'use_experimental_parser': 'False', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'write_json': 'True', 'use_colors': 'True', 'printer_width': '80', 'log_cache_events': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'target_path': 'None', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None'}
[0m02:07:04.129237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8d8afc65-2d1c-4120-9c4e-c1ac249af589', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1ddc98fc0>]}
[0m02:07:04.160006 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m02:07:04.170516 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m02:07:04.171842 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 1.3493245, "process_in_blocks": "0", "process_kernel_time": 0.68575, "process_mem_max_rss": "103276", "process_out_blocks": "16", "process_user_time": 3.347391}
[0m02:07:04.172543 [debug] [MainThread]: Command `cli deps` succeeded at 02:07:04.172369 after 1.35 seconds
[0m02:07:04.173073 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1dceec8d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1dceecf30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fc1dd1cbd50>]}
[0m02:07:04.173600 [debug] [MainThread]: Flushing usage events
[0m02:07:05.093871 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:08:15.767699 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff09fa74c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff0a12ac190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff09ea07c50>]}


============================== 02:08:15.779205 | 6da74edc-c5d3-407a-bfc1-79aa0a7dc444 ==============================
[0m02:08:15.779205 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:08:15.780215 [debug] [MainThread]: running dbt with arguments {'version_check': 'True', 'introspect': 'True', 'empty': 'False', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'partial_parse': 'True', 'warn_error': 'None', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'no_print': 'None', 'indirect_selection': 'eager', 'static_parser': 'True', 'fail_fast': 'False', 'cache_selected_only': 'False', 'quiet': 'False', 'use_colors': 'True', 'debug': 'False', 'log_cache_events': 'False', 'invocation_command': 'dbt run', 'use_experimental_parser': 'False', 'log_format': 'default', 'write_json': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'send_anonymous_usage_stats': 'True'}
[0m02:08:16.686564 [error] [MainThread]: Encountered an error:

[0m02:08:16.702544 [error] [MainThread]: Traceback (most recent call last):
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 182, in wrapper
    result, success = func(*args, **kwargs)
                      ~~~~^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 128, in wrapper
    return func(*args, **kwargs)
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/dbt/cli/requires.py", line 272, in wrapper
    profile = load_profile(flags.PROJECT_DIR, flags.VARS, flags.PROFILE, flags.TARGET, threads)
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/dbt/config/runtime.py", line 79, in load_profile
    profile = Profile.render(
        profile_renderer, profile_name, profile_name_override, target_override, threads_override
    )
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/dbt/config/profile.py", line 404, in render
    return cls.from_raw_profiles(
           ~~~~~~~~~~~~~~~~~~~~~^
        raw_profiles=raw_profiles,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
        threads_override=threads_override,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/dbt/config/profile.py", line 370, in from_raw_profiles
    return cls.from_raw_profile_info(
           ~~~~~~~~~~~~~~~~~~~~~~~~~^
        raw_profile=raw_profile,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
        threads_override=threads_override,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/dbt/config/profile.py", line 326, in from_raw_profile_info
    credentials: Credentials = cls._credentials_from_profile(
                               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        profile_data, profile_name, target_name
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/dbt/config/profile.py", line 150, in _credentials_from_profile
    cls = load_plugin(typename)
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/dbt/adapters/factory.py", line 239, in load_plugin
    return FACTORY.load_plugin(name)
           ~~~~~~~~~~~~~~~~~~~^^^^^^
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/dbt/adapters/factory.py", line 68, in load_plugin
    mod: Any = import_module("." + name, "dbt.adapters")
               ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ubuntu/.local/share/uv/python/cpython-3.13.11-linux-x86_64-gnu/lib/python3.13/importlib/__init__.py", line 88, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1387, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1360, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1331, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 1023, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/__init__.py", line 3, in <module>
    from dbt.adapters.databricks.credentials import DatabricksCredentials
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/dbt/adapters/databricks/credentials.py", line 14, in <module>
    from databricks.sdk import WorkspaceClient
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/databricks/sdk/__init__.py", line 32, in <module>
    from databricks.sdk.service import ml as pkg_ml
  File "/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/lib/python3.13/site-packages/databricks/sdk/service/ml.py", line 25, in <module>
    @dataclass
     ^^^^^^^^^
  File "/home/ubuntu/.local/share/uv/python/cpython-3.13.11-linux-x86_64-gnu/lib/python3.13/dataclasses.py", line 1305, in dataclass
    return wrap(cls)
  File "/home/ubuntu/.local/share/uv/python/cpython-3.13.11-linux-x86_64-gnu/lib/python3.13/dataclasses.py", line 1295, in wrap
    return _process_class(cls, init, repr, eq, order, unsafe_hash,
                          frozen, match_args, kw_only, slots,
                          weakref_slot)
  File "/home/ubuntu/.local/share/uv/python/cpython-3.13.11-linux-x86_64-gnu/lib/python3.13/dataclasses.py", line 1157, in _process_class
    func_builder.add_fns_to_class(cls)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^
  File "/home/ubuntu/.local/share/uv/python/cpython-3.13.11-linux-x86_64-gnu/lib/python3.13/dataclasses.py", line 498, in add_fns_to_class
    exec(txt, self.globals, ns)
    ~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 0, in <module>
KeyboardInterrupt

[0m02:08:16.704592 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 1.0799813, "process_in_blocks": "1192", "process_kernel_time": 0.211545, "process_mem_max_rss": "133860", "process_out_blocks": "24", "process_user_time": 3.618536}
[0m02:08:16.705685 [debug] [MainThread]: Command `dbt run` failed at 02:08:16.705487 after 1.08 seconds
[0m02:08:16.706228 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff09cd93490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff09cc7f2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7ff09cc7a030>]}
[0m02:08:16.706836 [debug] [MainThread]: Flushing usage events
[0m02:08:17.433263 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:08:45.462630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a2d6cc20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a45f0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60a1cb3c50>]}


============================== 02:08:45.466145 | 4c68f4fd-1de3-4fb8-8051-b140bb0f1d67 ==============================
[0m02:08:45.466145 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:08:45.466705 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'introspect': 'True', 'cache_selected_only': 'False', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'write_json': 'True', 'debug': 'False', 'warn_error': 'None', 'fail_fast': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'use_experimental_parser': 'False', 'no_print': 'None', 'target_path': 'None', 'invocation_command': 'dbt debug', 'quiet': 'False', 'use_colors': 'True', 'indirect_selection': 'eager', 'empty': 'None', 'printer_width': '80', 'log_cache_events': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'partial_parse': 'True', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m02:08:45.475647 [info ] [MainThread]: dbt version: 1.11.2
[0m02:08:45.476154 [info ] [MainThread]: python version: 3.13.11
[0m02:08:45.476487 [info ] [MainThread]: python path: /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/.venv/bin/python3
[0m02:08:45.476803 [info ] [MainThread]: os info: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35
[0m02:08:46.264542 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:08:46.265011 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:08:46.265340 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:08:46.837663 [info ] [MainThread]: Using profiles dir at /home/ubuntu/.dbt
[0m02:08:46.838144 [info ] [MainThread]: Using profiles.yml file at /home/ubuntu/.dbt/profiles.yml
[0m02:08:46.838571 [info ] [MainThread]: Using dbt_project.yml file at /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/dbt_project.yml
[0m02:08:46.838867 [info ] [MainThread]: adapter type: databricks
[0m02:08:46.839113 [info ] [MainThread]: adapter version: 1.11.4
[0m02:08:46.943401 [info ] [MainThread]: Configuration:
[0m02:08:46.943832 [info ] [MainThread]:   profiles.yml file [[32mOK found and valid[0m]
[0m02:08:46.944136 [info ] [MainThread]:   dbt_project.yml file [[32mOK found and valid[0m]
[0m02:08:46.944382 [info ] [MainThread]: Required dependencies:
[0m02:08:46.944723 [debug] [MainThread]: Executing "git --help"
[0m02:08:46.948422 [debug] [MainThread]: STDOUT: "b"usage: git [--version] [--help] [-C <path>] [-c <name>=<value>]\n           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]\n           [-p | --paginate | -P | --no-pager] [--no-replace-objects] [--bare]\n           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]\n           [--super-prefix=<path>] [--config-env=<name>=<envvar>]\n           <command> [<args>]\n\nThese are common Git commands used in various situations:\n\nstart a working area (see also: git help tutorial)\n   clone     Clone a repository into a new directory\n   init      Create an empty Git repository or reinitialize an existing one\n\nwork on the current change (see also: git help everyday)\n   add       Add file contents to the index\n   mv        Move or rename a file, a directory, or a symlink\n   restore   Restore working tree files\n   rm        Remove files from the working tree and from the index\n\nexamine the history and state (see also: git help revisions)\n   bisect    Use binary search to find the commit that introduced a bug\n   diff      Show changes between commits, commit and working tree, etc\n   grep      Print lines matching a pattern\n   log       Show commit logs\n   show      Show various types of objects\n   status    Show the working tree status\n\ngrow, mark and tweak your common history\n   branch    List, create, or delete branches\n   commit    Record changes to the repository\n   merge     Join two or more development histories together\n   rebase    Reapply commits on top of another base tip\n   reset     Reset current HEAD to the specified state\n   switch    Switch branches\n   tag       Create, list, delete or verify a tag object signed with GPG\n\ncollaborate (see also: git help workflows)\n   fetch     Download objects and refs from another repository\n   pull      Fetch from and integrate with another repository or a local branch\n   push      Update remote refs along with associated objects\n\n'git help -a' and 'git help -g' list available subcommands and some\nconcept guides. See 'git help <command>' or 'git help <concept>'\nto read about a specific subcommand or concept.\nSee 'git help git' for an overview of the system.\n""
[0m02:08:46.948997 [debug] [MainThread]: STDERR: "b''"
[0m02:08:46.949371 [info ] [MainThread]:  - git [[32mOK found[0m]

[0m02:08:46.949652 [info ] [MainThread]: Connection:
[0m02:08:46.949948 [info ] [MainThread]:   host: adb-3183350029643709.9.azuredatabricks.net
[0m02:08:46.950197 [info ] [MainThread]:   http_path: /sql/1.0/warehouses/a418758e84eae08c
[0m02:08:46.950459 [info ] [MainThread]:   catalog: dev
[0m02:08:46.950804 [info ] [MainThread]:   schema: customer_bi
[0m02:08:46.951317 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m02:08:47.030751 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:08:47.031441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '4c68f4fd-1de3-4fb8-8051-b140bb0f1d67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60802ace90>]}
[0m02:08:47.031978 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=debug) - Creating connection
[0m02:08:47.032241 [debug] [MainThread]: Acquiring new databricks connection 'debug'
[0m02:08:47.032481 [debug] [MainThread]: Using databricks connection "debug"
[0m02:08:47.032776 [debug] [MainThread]: On debug: select 1 as id
[0m02:08:47.033004 [debug] [MainThread]: Opening a new connection, currently in state init
[0m02:08:47.855057 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0f189-077e-171c-ac7e-bf6e9052f87c) - Created
[0m02:08:48.234961 [debug] [MainThread]: SQL status: OK in 1.200 seconds
[0m02:08:48.235767 [debug] [MainThread]: Databricks adapter: Cursor(session-id=01f0f189-077e-171c-ac7e-bf6e9052f87c, command-id=01f0f189-07ad-1ea9-abc0-c5b00343abc5) - Closing
[0m02:08:48.236125 [debug] [MainThread]: On debug: Close
[0m02:08:48.236358 [debug] [MainThread]: Databricks adapter: Connection(session-id=01f0f189-077e-171c-ac7e-bf6e9052f87c) - Closing
[0m02:08:48.563316 [info ] [MainThread]:   Connection test: [[32mOK connection ok[0m]

[0m02:08:48.563700 [info ] [MainThread]: [32mAll checks passed![0m
[0m02:08:48.564251 [debug] [MainThread]: Resource report: {"command_name": "debug", "command_success": true, "command_wall_clock_time": 3.1737041, "process_in_blocks": "352", "process_kernel_time": 0.570154, "process_mem_max_rss": "250224", "process_out_blocks": "24", "process_user_time": 3.775928}
[0m02:08:48.564664 [debug] [MainThread]: Command `dbt debug` succeeded at 02:08:48.564589 after 3.17 seconds
[0m02:08:48.565014 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6080346580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6080346ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f60804bee50>]}
[0m02:08:48.565293 [debug] [MainThread]: Flushing usage events
[0m02:08:49.312577 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:08:57.507011 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93e5ac8c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93e7358190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93e4a5bc50>]}


============================== 02:08:57.518935 | 703f01d3-d21a-4930-8537-ebad3b4414cb ==============================
[0m02:08:57.518935 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:08:57.519578 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'target_path': 'None', 'static_parser': 'True', 'empty': 'False', 'fail_fast': 'False', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'quiet': 'False', 'log_cache_events': 'False', 'write_json': 'True', 'invocation_command': 'dbt run', 'no_print': 'None', 'profiles_dir': '/home/ubuntu/.dbt', 'version_check': 'True', 'use_experimental_parser': 'False', 'use_colors': 'True', 'printer_width': '80', 'debug': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'warn_error': 'None'}
[0m02:08:59.638720 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:08:59.639230 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:08:59.639535 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:09:03.866133 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93e5750fc0>]}
[0m02:09:04.039353 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93bf09a140>]}
[0m02:09:04.045879 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m02:09:04.250015 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:09:04.254568 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93bf261e50>]}
[0m02:09:04.290144 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m02:09:04.710765 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:09:04.711254 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m02:09:04.711608 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:09:04.746443 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dev.bronze
[0m02:09:04.865312 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93bed51310>]}
[0m02:09:05.227709 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:09:05.237992 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:09:05.254453 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93bf0f84b0>]}
[0m02:09:05.254990 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m02:09:05.255377 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93bec75f30>]}
[0m02:09:05.257340 [info ] [MainThread]: 
[0m02:09:05.265612 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:09:05.266138 [info ] [MainThread]: 
[0m02:09:05.266794 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:09:05.267078 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:09:05.273426 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m02:09:05.273997 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m02:09:05.310205 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m02:09:05.310783 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m02:09:05.311172 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:09:06.195461 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f189-11fb-127a-b427-6c16b491f0d9) - Created
[0m02:09:06.813542 [debug] [ThreadPool]: SQL status: OK in 1.500 seconds
[0m02:09:06.829302 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f189-11fb-127a-b427-6c16b491f0d9, command-id=01f0f189-122c-1b7d-8fdb-7dbe1225cc09) - Closing
[0m02:09:06.830017 [debug] [ThreadPool]: On list_dev: Close
[0m02:09:06.830377 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f189-11fb-127a-b427-6c16b491f0d9) - Closing
[0m02:09:07.074918 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m02:09:07.075364 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m02:09:07.080759 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m02:09:07.081113 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m02:09:07.081397 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:09:07.997278 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f189-1327-143a-bfda-3665a937ad98) - Created
[0m02:09:08.730338 [debug] [ThreadPool]: SQL status: OK in 1.650 seconds
[0m02:09:08.736882 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f189-1327-143a-bfda-3665a937ad98, command-id=01f0f189-1351-1b5e-bf86-4c1679b47242) - Closing
[0m02:09:08.743379 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m02:09:08.743985 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f189-1327-143a-bfda-3665a937ad98) - Closing
[0m02:09:08.988981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93bee4e990>]}
[0m02:09:08.995120 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_customer
[0m02:09:08.995846 [info ] [Thread-3 (]: 1 of 5 START sql view model customer_bi.bronze_customer ........................ [RUN]
[0m02:09:08.996605 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m02:09:08.997013 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m02:09:08.997409 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_customer
[0m02:09:09.005351 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m02:09:09.014121 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_customer
[0m02:09:09.051839 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:09:09.054786 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:09:09.062331 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93bed3d2e0>]}
[0m02:09:09.107930 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_customer`
[0m02:09:09.137947 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m02:09:09.138737 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m02:09:09.139115 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_customer`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  )

[0m02:09:09.139382 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:09:09.902538 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f189-1468-1de8-b0e5-cce4a9b428e5) - Created
[0m02:09:10.867231 [debug] [Thread-3 (]: SQL status: OK in 1.730 seconds
[0m02:09:10.868521 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f189-1468-1de8-b0e5-cce4a9b428e5, command-id=01f0f189-148d-1a20-8c4c-67ef95c2bf1c) - Closing
[0m02:09:10.887017 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:09:10.890853 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: Close
[0m02:09:10.891341 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f189-1468-1de8-b0e5-cce4a9b428e5) - Closing
[0m02:09:11.167206 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93e58d6530>]}
[0m02:09:11.167907 [info ] [Thread-3 (]: 1 of 5 OK created sql view model customer_bi.bronze_customer ................... [[32mOK[0m in 2.17s]
[0m02:09:11.168458 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_customer
[0m02:09:11.168806 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_date
[0m02:09:11.169305 [info ] [Thread-3 (]: 2 of 5 START sql view model customer_bi.bronze_date ............................ [RUN]
[0m02:09:11.169828 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m02:09:11.170190 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m02:09:11.170515 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_date
[0m02:09:11.173001 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m02:09:11.181360 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_date
[0m02:09:11.184027 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:09:11.185510 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_date`
[0m02:09:11.186252 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m02:09:11.186904 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m02:09:11.187239 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_date`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_date`
  )

[0m02:09:11.187538 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:09:12.024129 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f189-15c5-1d3e-86cb-ddfbdb33d11a) - Created
[0m02:09:12.906272 [debug] [Thread-3 (]: SQL status: OK in 1.720 seconds
[0m02:09:12.907565 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f189-15c5-1d3e-86cb-ddfbdb33d11a, command-id=01f0f189-15ed-1487-ba13-d3707f903de3) - Closing
[0m02:09:12.908485 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:09:12.909372 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: Close
[0m02:09:12.909987 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f189-15c5-1d3e-86cb-ddfbdb33d11a) - Closing
[0m02:09:13.144632 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93becb7c50>]}
[0m02:09:13.145574 [info ] [Thread-3 (]: 2 of 5 OK created sql view model customer_bi.bronze_date ....................... [[32mOK[0m in 1.97s]
[0m02:09:13.146396 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_date
[0m02:09:13.146880 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_returns
[0m02:09:13.147705 [info ] [Thread-3 (]: 3 of 5 START sql view model customer_bi.bronze_returns ......................... [RUN]
[0m02:09:13.148428 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m02:09:13.148845 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m02:09:13.149247 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_returns
[0m02:09:13.152982 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m02:09:13.154288 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_returns
[0m02:09:13.166738 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:09:13.168520 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_returns`
[0m02:09:13.169712 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m02:09:13.172392 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m02:09:13.173482 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_returns`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  )

[0m02:09:13.176668 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:09:13.936809 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f189-1706-18a0-9929-5d696d632f53) - Created
[0m02:09:14.795392 [debug] [Thread-3 (]: SQL status: OK in 1.620 seconds
[0m02:09:14.796436 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f189-1706-18a0-9929-5d696d632f53, command-id=01f0f189-172f-122b-8880-cd729609c1cd) - Closing
[0m02:09:14.797094 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:09:14.797818 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: Close
[0m02:09:14.798150 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f189-1706-18a0-9929-5d696d632f53) - Closing
[0m02:09:15.058538 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93beb203d0>]}
[0m02:09:15.059166 [info ] [Thread-3 (]: 3 of 5 OK created sql view model customer_bi.bronze_returns .................... [[32mOK[0m in 1.91s]
[0m02:09:15.059691 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_returns
[0m02:09:15.060002 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_sales
[0m02:09:15.060372 [info ] [Thread-3 (]: 4 of 5 START sql view model customer_bi.bronze_sales ........................... [RUN]
[0m02:09:15.061064 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m02:09:15.061408 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m02:09:15.061688 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_sales
[0m02:09:15.064212 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m02:09:15.064837 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_sales
[0m02:09:15.067230 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:09:15.068824 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_sales`
[0m02:09:15.069671 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m02:09:15.070187 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m02:09:15.070480 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_sales`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  )

[0m02:09:15.070744 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:09:15.817540 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f189-1846-1f19-98b4-6b1e0956481c) - Created
[0m02:09:16.654045 [debug] [Thread-3 (]: SQL status: OK in 1.580 seconds
[0m02:09:16.654987 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f189-1846-1f19-98b4-6b1e0956481c, command-id=01f0f189-186d-15cf-90e4-7c1032e4cfde) - Closing
[0m02:09:16.655662 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:09:16.656243 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: Close
[0m02:09:16.656488 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f189-1846-1f19-98b4-6b1e0956481c) - Closing
[0m02:09:16.901643 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93beaf48f0>]}
[0m02:09:16.902751 [info ] [Thread-3 (]: 4 of 5 OK created sql view model customer_bi.bronze_sales ...................... [[32mOK[0m in 1.84s]
[0m02:09:16.903498 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_sales
[0m02:09:16.903976 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_store
[0m02:09:16.904519 [info ] [Thread-3 (]: 5 of 5 START sql view model customer_bi.bronze_store ........................... [RUN]
[0m02:09:16.905350 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m02:09:16.905762 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m02:09:16.906128 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_store
[0m02:09:16.909371 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m02:09:16.910347 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_store
[0m02:09:16.916663 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:09:16.918613 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_store`
[0m02:09:16.920008 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m02:09:16.920983 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m02:09:16.921649 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_store`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_store`
  )

[0m02:09:16.922173 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:09:17.695088 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f189-1986-1ef2-84c2-1437f0cbaed1) - Created
[0m02:09:18.537299 [debug] [Thread-3 (]: SQL status: OK in 1.610 seconds
[0m02:09:18.538704 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f189-1986-1ef2-84c2-1437f0cbaed1, command-id=01f0f189-19ab-1e6d-8e40-58461034c5f5) - Closing
[0m02:09:18.539717 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:09:18.541490 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: Close
[0m02:09:18.542215 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f189-1986-1ef2-84c2-1437f0cbaed1) - Closing
[0m02:09:18.790027 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '703f01d3-d21a-4930-8537-ebad3b4414cb', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93beaf5f70>]}
[0m02:09:18.790688 [info ] [Thread-3 (]: 5 of 5 OK created sql view model customer_bi.bronze_store ...................... [[32mOK[0m in 1.88s]
[0m02:09:18.791357 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_store
[0m02:09:18.793227 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:09:18.793542 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:09:18.793991 [info ] [MainThread]: 
[0m02:09:18.794288 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 13.53 seconds (13.53s).
[0m02:09:18.795304 [debug] [MainThread]: Command end result
[0m02:09:18.836367 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:09:18.839087 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:09:18.846304 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m02:09:18.848056 [info ] [MainThread]: 
[0m02:09:18.850070 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:09:18.851006 [info ] [MainThread]: 
[0m02:09:18.852194 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m02:09:18.853709 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 19.051981, "process_in_blocks": "256", "process_kernel_time": 2.246483, "process_mem_max_rss": "261888", "process_out_blocks": "3432", "process_user_time": 8.935611}
[0m02:09:18.854440 [debug] [MainThread]: Command `dbt run` succeeded at 02:09:18.854281 after 19.05 seconds
[0m02:09:18.855016 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93bf01cad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93e74c2990>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f93beb0a0f0>]}
[0m02:09:18.855579 [debug] [MainThread]: Flushing usage events
[0m02:09:19.731448 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:13:57.243668 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2a8fa8c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2aa838190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2a7f3fc50>]}


============================== 02:13:57.261862 | 07ec37c2-35e2-4974-a942-4bff52abe517 ==============================
[0m02:13:57.261862 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:13:57.262564 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'empty': 'False', 'cache_selected_only': 'False', 'fail_fast': 'False', 'log_cache_events': 'False', 'debug': 'False', 'version_check': 'True', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'static_parser': 'True', 'use_colors': 'True', 'quiet': 'False', 'introspect': 'True', 'warn_error': 'None', 'printer_width': '80', 'target_path': 'None', 'log_format': 'default', 'partial_parse': 'True', 'no_print': 'None', 'indirect_selection': 'eager', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'invocation_command': 'dbt run'}
[0m02:13:59.307126 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:13:59.307731 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:13:59.308213 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:14:00.784849 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2a8c30fc0>]}
[0m02:14:00.957674 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa282562140>]}
[0m02:14:00.958420 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m02:14:01.126294 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:14:01.127128 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa28272de50>]}
[0m02:14:01.160188 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m02:14:01.393160 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:14:01.393643 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m02:14:01.393911 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:14:01.413804 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dev.bronze
[0m02:14:01.495832 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa282209310>]}
[0m02:14:01.710471 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:14:01.745542 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:14:01.756362 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2825c04b0>]}
[0m02:14:01.756863 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m02:14:01.757215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa282151f30>]}
[0m02:14:01.767311 [info ] [MainThread]: 
[0m02:14:01.767879 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:14:01.768241 [info ] [MainThread]: 
[0m02:14:01.768857 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:14:01.769184 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:14:01.786225 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m02:14:01.786878 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m02:14:01.820380 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m02:14:01.821099 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m02:14:01.821569 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:14:02.662043 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f189-c391-15b2-b6d7-21394ca542d5) - Created
[0m02:14:03.097464 [debug] [ThreadPool]: SQL status: OK in 1.280 seconds
[0m02:14:03.105394 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f189-c391-15b2-b6d7-21394ca542d5, command-id=01f0f189-c3b8-16dc-9195-74f299020d51) - Closing
[0m02:14:03.105972 [debug] [ThreadPool]: On list_dev: Close
[0m02:14:03.106318 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f189-c391-15b2-b6d7-21394ca542d5) - Closing
[0m02:14:03.339019 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_dev_customer_bi) - Creating connection
[0m02:14:03.339445 [debug] [ThreadPool]: Acquiring new databricks connection 'create_dev_customer_bi'
[0m02:14:03.341325 [debug] [ThreadPool]: Creating schema "database: "dev"
schema: "customer_bi"
"
[0m02:14:03.349287 [debug] [ThreadPool]: Using databricks connection "create_dev_customer_bi"
[0m02:14:03.349802 [debug] [ThreadPool]: On create_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "create_dev_customer_bi"} */
create schema if not exists `dev`.`customer_bi`
  
[0m02:14:03.350181 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:14:04.171171 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f189-c48d-1af6-8c13-4d2b2fd13e6f) - Created
[0m02:14:04.846497 [debug] [ThreadPool]: SQL status: OK in 1.500 seconds
[0m02:14:04.847444 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f189-c48d-1af6-8c13-4d2b2fd13e6f, command-id=01f0f189-c4b8-1bef-8473-e7fa23b2130b) - Closing
[0m02:14:04.847858 [debug] [ThreadPool]: On create_dev_customer_bi: Close
[0m02:14:04.848132 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f189-c48d-1af6-8c13-4d2b2fd13e6f) - Closing
[0m02:14:05.096011 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m02:14:05.096484 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m02:14:05.100962 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m02:14:05.101378 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m02:14:05.101675 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:14:05.854664 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f189-c5af-1b0b-b6aa-d1cfb4b0d2c6) - Created
[0m02:14:06.465017 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m02:14:06.468761 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f189-c5af-1b0b-b6aa-d1cfb4b0d2c6, command-id=01f0f189-c5d5-1056-926f-942b574afee4) - Closing
[0m02:14:06.469241 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m02:14:06.469517 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f189-c5af-1b0b-b6aa-d1cfb4b0d2c6) - Closing
[0m02:14:06.712409 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa282206150>]}
[0m02:14:06.715539 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_customer
[0m02:14:06.716256 [info ] [Thread-4 (]: 1 of 5 START sql view model customer_bi.bronze_customer ........................ [RUN]
[0m02:14:06.716924 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m02:14:06.717283 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m02:14:06.717608 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_customer
[0m02:14:06.731651 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m02:14:06.739993 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_customer
[0m02:14:06.774098 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:14:06.776667 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:14:06.777382 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2821d52e0>]}
[0m02:14:06.813486 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_customer`
[0m02:14:06.841587 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m02:14:06.842419 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m02:14:06.842906 [debug] [Thread-4 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_customer`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  )

[0m02:14:06.843357 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:14:07.618405 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f189-c6d8-1ff6-b594-7f3c5fd064cc) - Created
[0m02:14:08.404262 [debug] [Thread-4 (]: SQL status: OK in 1.560 seconds
[0m02:14:08.405595 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f189-c6d8-1ff6-b594-7f3c5fd064cc, command-id=01f0f189-c701-1688-a4f6-6b2ad23a9353) - Closing
[0m02:14:08.425508 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:14:08.435066 [debug] [Thread-4 (]: On model.customer_bi.bronze_customer: Close
[0m02:14:08.435591 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f189-c6d8-1ff6-b594-7f3c5fd064cc) - Closing
[0m02:14:08.732320 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2a8db6530>]}
[0m02:14:08.733354 [info ] [Thread-4 (]: 1 of 5 OK created sql view model customer_bi.bronze_customer ................... [[32mOK[0m in 2.01s]
[0m02:14:08.734216 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_customer
[0m02:14:08.734734 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_date
[0m02:14:08.735414 [info ] [Thread-4 (]: 2 of 5 START sql view model customer_bi.bronze_date ............................ [RUN]
[0m02:14:08.736039 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m02:14:08.736487 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m02:14:08.736912 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_date
[0m02:14:08.748848 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m02:14:08.751916 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_date
[0m02:14:08.754533 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:14:08.762178 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_date`
[0m02:14:08.763688 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m02:14:08.764547 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m02:14:08.764949 [debug] [Thread-4 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_date`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_date`
  )

[0m02:14:08.765290 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:14:09.533974 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f189-c81e-1841-9f97-69d61b35bb8f) - Created
[0m02:14:10.276401 [debug] [Thread-4 (]: SQL status: OK in 1.510 seconds
[0m02:14:10.277476 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f189-c81e-1841-9f97-69d61b35bb8f, command-id=01f0f189-c846-1928-ba4c-88f191256e8d) - Closing
[0m02:14:10.278284 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:14:10.279290 [debug] [Thread-4 (]: On model.customer_bi.bronze_date: Close
[0m02:14:10.279737 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f189-c81e-1841-9f97-69d61b35bb8f) - Closing
[0m02:14:10.526323 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2823062d0>]}
[0m02:14:10.527372 [info ] [Thread-4 (]: 2 of 5 OK created sql view model customer_bi.bronze_date ....................... [[32mOK[0m in 1.79s]
[0m02:14:10.528154 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_date
[0m02:14:10.528712 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_returns
[0m02:14:10.529389 [info ] [Thread-4 (]: 3 of 5 START sql view model customer_bi.bronze_returns ......................... [RUN]
[0m02:14:10.530094 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m02:14:10.530410 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m02:14:10.530794 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_returns
[0m02:14:10.538527 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m02:14:10.539744 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_returns
[0m02:14:10.543868 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:14:10.545093 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_returns`
[0m02:14:10.545954 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m02:14:10.546693 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m02:14:10.547167 [debug] [Thread-4 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_returns`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  )

[0m02:14:10.547541 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:14:11.343818 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f189-c949-123d-af50-b3cf82d3df57) - Created
[0m02:14:12.178934 [debug] [Thread-4 (]: SQL status: OK in 1.630 seconds
[0m02:14:12.179760 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f189-c949-123d-af50-b3cf82d3df57, command-id=01f0f189-c977-1b2f-9128-c3dc24672a3e) - Closing
[0m02:14:12.180350 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:14:12.180929 [debug] [Thread-4 (]: On model.customer_bi.bronze_returns: Close
[0m02:14:12.181200 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f189-c949-123d-af50-b3cf82d3df57) - Closing
[0m02:14:12.430287 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa281fcd9b0>]}
[0m02:14:12.431861 [info ] [Thread-4 (]: 3 of 5 OK created sql view model customer_bi.bronze_returns .................... [[32mOK[0m in 1.90s]
[0m02:14:12.432732 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_returns
[0m02:14:12.433247 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_sales
[0m02:14:12.433971 [info ] [Thread-4 (]: 4 of 5 START sql view model customer_bi.bronze_sales ........................... [RUN]
[0m02:14:12.434882 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m02:14:12.435307 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m02:14:12.435532 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_sales
[0m02:14:12.438040 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m02:14:12.438426 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_sales
[0m02:14:12.439672 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:14:12.440400 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_sales`
[0m02:14:12.441356 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m02:14:12.441781 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m02:14:12.442095 [debug] [Thread-4 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_sales`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  )

[0m02:14:12.442335 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:14:13.190131 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f189-ca8a-1b09-87bd-9da66a39fe48) - Created
[0m02:14:13.874672 [debug] [Thread-4 (]: SQL status: OK in 1.430 seconds
[0m02:14:13.876624 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f189-ca8a-1b09-87bd-9da66a39fe48, command-id=01f0f189-cab1-1661-8855-8bcf775dd46a) - Closing
[0m02:14:13.877340 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:14:13.878602 [debug] [Thread-4 (]: On model.customer_bi.bronze_sales: Close
[0m02:14:13.878885 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f189-ca8a-1b09-87bd-9da66a39fe48) - Closing
[0m02:14:14.120441 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa282481550>]}
[0m02:14:14.121932 [info ] [Thread-4 (]: 4 of 5 OK created sql view model customer_bi.bronze_sales ...................... [[32mOK[0m in 1.69s]
[0m02:14:14.122869 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_sales
[0m02:14:14.123384 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_store
[0m02:14:14.124084 [info ] [Thread-4 (]: 5 of 5 START sql view model customer_bi.bronze_store ........................... [RUN]
[0m02:14:14.124782 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m02:14:14.125237 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m02:14:14.125608 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_store
[0m02:14:14.128248 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m02:14:14.129041 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_store
[0m02:14:14.131918 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:14:14.134217 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_store`
[0m02:14:14.135507 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m02:14:14.136400 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m02:14:14.136922 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_store`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_store`
  )

[0m02:14:14.137267 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:14:14.903578 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f189-cbaa-1635-856f-12c75acfa638) - Created
[0m02:14:15.640784 [debug] [Thread-4 (]: SQL status: OK in 1.500 seconds
[0m02:14:15.641616 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f189-cbaa-1635-856f-12c75acfa638, command-id=01f0f189-cbd4-1401-8f8c-fe7e63ef769c) - Closing
[0m02:14:15.642348 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:14:15.643264 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: Close
[0m02:14:15.643587 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f189-cbaa-1635-856f-12c75acfa638) - Closing
[0m02:14:15.881378 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '07ec37c2-35e2-4974-a942-4bff52abe517', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa282483050>]}
[0m02:14:15.882070 [info ] [Thread-4 (]: 5 of 5 OK created sql view model customer_bi.bronze_store ...................... [[32mOK[0m in 1.76s]
[0m02:14:15.882656 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_store
[0m02:14:15.884277 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:14:15.884610 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:14:15.885104 [info ] [MainThread]: 
[0m02:14:15.885448 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 14.12 seconds (14.12s).
[0m02:14:15.886449 [debug] [MainThread]: Command end result
[0m02:14:15.912777 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:14:15.915462 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:14:15.921652 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m02:14:15.922111 [info ] [MainThread]: 
[0m02:14:15.922548 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:14:15.922997 [info ] [MainThread]: 
[0m02:14:15.923400 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m02:14:15.924215 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 18.798172, "process_in_blocks": "0", "process_kernel_time": 2.230239, "process_mem_max_rss": "261948", "process_out_blocks": "3448", "process_user_time": 8.236573}
[0m02:14:15.924673 [debug] [MainThread]: Command `dbt run` succeeded at 02:14:15.924568 after 18.80 seconds
[0m02:14:15.925024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa2a81b9e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa281fdb830>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa281fdbd70>]}
[0m02:14:15.925367 [debug] [MainThread]: Flushing usage events
[0m02:14:20.002742 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:15:40.885441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa89d264c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa89eae8190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa89c1b7c50>]}


============================== 02:15:40.888182 | fea84ca2-42e9-4ccf-a08e-e0dde0b549b7 ==============================
[0m02:15:40.888182 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:15:40.888634 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'introspect': 'True', 'write_json': 'True', 'fail_fast': 'False', 'use_colors': 'True', 'version_check': 'True', 'quiet': 'False', 'warn_error': 'None', 'target_path': 'None', 'no_print': 'None', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'send_anonymous_usage_stats': 'True', 'indirect_selection': 'eager', 'printer_width': '80', 'log_cache_events': 'False', 'static_parser': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/ubuntu/.dbt', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'log_format': 'default', 'debug': 'False'}
[0m02:15:41.561656 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:15:41.562158 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:15:41.562557 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:15:42.060295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa89ceb8fc0>]}
[0m02:15:42.122260 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa876822140>]}
[0m02:15:42.122948 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m02:15:42.179440 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:15:42.180033 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8767ede50>]}
[0m02:15:42.187030 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m02:15:42.267514 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:15:42.267874 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m02:15:42.268111 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:15:42.276276 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dev.bronze
[0m02:15:42.302559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8764c9310>]}
[0m02:15:42.394704 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:15:42.397054 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:15:42.404358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8768804b0>]}
[0m02:15:42.404903 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m02:15:42.405378 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8763fdf30>]}
[0m02:15:42.407276 [info ] [MainThread]: 
[0m02:15:42.407662 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:15:42.408005 [info ] [MainThread]: 
[0m02:15:42.408543 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:15:42.408849 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:15:42.415549 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m02:15:42.416175 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m02:15:42.430699 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m02:15:42.431239 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m02:15:42.431615 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:15:43.196374 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f189-ff0e-12cc-9244-8f12b8754671) - Created
[0m02:15:43.637616 [debug] [ThreadPool]: SQL status: OK in 1.210 seconds
[0m02:15:43.642908 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f189-ff0e-12cc-9244-8f12b8754671, command-id=01f0f189-ff33-1bda-9995-96fb1e5d4d8e) - Closing
[0m02:15:43.643398 [debug] [ThreadPool]: On list_dev: Close
[0m02:15:43.643702 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f189-ff0e-12cc-9244-8f12b8754671) - Closing
[0m02:15:43.890458 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m02:15:43.890978 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m02:15:43.895504 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m02:15:43.895837 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m02:15:43.896159 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:15:44.653883 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18a-0001-120f-acb8-e07b644bc61b) - Created
[0m02:15:45.204148 [debug] [ThreadPool]: SQL status: OK in 1.310 seconds
[0m02:15:45.207669 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18a-0001-120f-acb8-e07b644bc61b, command-id=01f0f18a-0029-1f96-9b20-413b8dc524b9) - Closing
[0m02:15:45.208291 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m02:15:45.208566 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18a-0001-120f-acb8-e07b644bc61b) - Closing
[0m02:15:45.450615 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8764b6990>]}
[0m02:15:45.453820 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_customer
[0m02:15:45.454398 [info ] [Thread-3 (]: 1 of 5 START sql view model customer_bi.bronze_customer ........................ [RUN]
[0m02:15:45.455081 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m02:15:45.455511 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m02:15:45.455879 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_customer
[0m02:15:45.463666 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m02:15:45.464480 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_customer
[0m02:15:45.482397 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:15:45.484091 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:15:45.484577 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8764c52e0>]}
[0m02:15:45.495110 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_customer`
[0m02:15:45.506926 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m02:15:45.507776 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m02:15:45.508288 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_customer`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  )

[0m02:15:45.508656 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:15:46.276067 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18a-011a-1277-802f-e5ef0980f8e9) - Created
[0m02:15:47.172461 [debug] [Thread-3 (]: SQL status: OK in 1.660 seconds
[0m02:15:47.174701 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18a-011a-1277-802f-e5ef0980f8e9, command-id=01f0f18a-013d-1723-b0cf-8931ce3d368e) - Closing
[0m02:15:47.183206 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:15:47.184384 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: Close
[0m02:15:47.184628 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18a-011a-1277-802f-e5ef0980f8e9) - Closing
[0m02:15:47.422637 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa89d06e530>]}
[0m02:15:47.423482 [info ] [Thread-3 (]: 1 of 5 OK created sql view model customer_bi.bronze_customer ................... [[32mOK[0m in 1.97s]
[0m02:15:47.424097 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_customer
[0m02:15:47.424484 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_date
[0m02:15:47.424914 [info ] [Thread-3 (]: 2 of 5 START sql view model customer_bi.bronze_date ............................ [RUN]
[0m02:15:47.425643 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m02:15:47.426044 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m02:15:47.426309 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_date
[0m02:15:47.427965 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m02:15:47.428325 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_date
[0m02:15:47.429437 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:15:47.430392 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_date`
[0m02:15:47.431023 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m02:15:47.431385 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m02:15:47.431705 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_date`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_date`
  )

[0m02:15:47.431935 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:15:48.169881 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18a-025b-149a-8d50-a4dc1a6361b5) - Created
[0m02:15:49.260744 [debug] [Thread-3 (]: SQL status: OK in 1.830 seconds
[0m02:15:49.261943 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18a-025b-149a-8d50-a4dc1a6361b5, command-id=01f0f18a-027f-149d-8edd-12e02aa3e404) - Closing
[0m02:15:49.262805 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:15:49.263685 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: Close
[0m02:15:49.263979 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18a-025b-149a-8d50-a4dc1a6361b5) - Closing
[0m02:15:49.501793 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa87642fb50>]}
[0m02:15:49.502548 [info ] [Thread-3 (]: 2 of 5 OK created sql view model customer_bi.bronze_date ....................... [[32mOK[0m in 2.08s]
[0m02:15:49.503094 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_date
[0m02:15:49.503474 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_returns
[0m02:15:49.504083 [info ] [Thread-3 (]: 3 of 5 START sql view model customer_bi.bronze_returns ......................... [RUN]
[0m02:15:49.504660 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m02:15:49.505004 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m02:15:49.505327 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_returns
[0m02:15:49.508410 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m02:15:49.516441 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_returns
[0m02:15:49.531957 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:15:49.533763 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_returns`
[0m02:15:49.535039 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m02:15:49.535905 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m02:15:49.536541 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_returns`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  )

[0m02:15:49.537041 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:15:50.315665 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18a-03c4-14b6-a8f0-0044f6828b1a) - Created
[0m02:15:51.181416 [debug] [Thread-3 (]: SQL status: OK in 1.640 seconds
[0m02:15:51.182444 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18a-03c4-14b6-a8f0-0044f6828b1a, command-id=01f0f18a-03eb-12af-8868-2a3ef3dba212) - Closing
[0m02:15:51.183120 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:15:51.183822 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: Close
[0m02:15:51.184124 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18a-03c4-14b6-a8f0-0044f6828b1a) - Closing
[0m02:15:51.440853 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa874124ec0>]}
[0m02:15:51.441715 [info ] [Thread-3 (]: 3 of 5 OK created sql view model customer_bi.bronze_returns .................... [[32mOK[0m in 1.94s]
[0m02:15:51.442341 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_returns
[0m02:15:51.442740 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_sales
[0m02:15:51.443289 [info ] [Thread-3 (]: 4 of 5 START sql view model customer_bi.bronze_sales ........................... [RUN]
[0m02:15:51.443885 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m02:15:51.444244 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m02:15:51.444544 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_sales
[0m02:15:51.447217 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m02:15:51.448346 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_sales
[0m02:15:51.450624 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:15:51.459173 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_sales`
[0m02:15:51.460044 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m02:15:51.460558 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m02:15:51.460925 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_sales`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  )

[0m02:15:51.461265 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:15:52.237280 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18a-050a-11e2-9251-f11d1fa84c06) - Created
[0m02:15:53.112832 [debug] [Thread-3 (]: SQL status: OK in 1.650 seconds
[0m02:15:53.113987 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18a-050a-11e2-9251-f11d1fa84c06, command-id=01f0f18a-052f-17ec-aa22-60c7d69763c7) - Closing
[0m02:15:53.114810 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:15:53.115721 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: Close
[0m02:15:53.116125 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18a-050a-11e2-9251-f11d1fa84c06) - Closing
[0m02:15:53.357070 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa876228350>]}
[0m02:15:53.358083 [info ] [Thread-3 (]: 4 of 5 OK created sql view model customer_bi.bronze_sales ...................... [[32mOK[0m in 1.91s]
[0m02:15:53.358918 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_sales
[0m02:15:53.359478 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_store
[0m02:15:53.360077 [info ] [Thread-3 (]: 5 of 5 START sql view model customer_bi.bronze_store ........................... [RUN]
[0m02:15:53.360725 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m02:15:53.361186 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m02:15:53.361591 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_store
[0m02:15:53.372463 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m02:15:53.373422 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_store
[0m02:15:53.376872 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:15:53.386273 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_store`
[0m02:15:53.387616 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m02:15:53.388424 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m02:15:53.388924 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_store`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_store`
  )

[0m02:15:53.389331 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:15:54.163064 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18a-0650-16b8-a6e0-c8108fbb2455) - Created
[0m02:15:55.020854 [debug] [Thread-3 (]: SQL status: OK in 1.630 seconds
[0m02:15:55.022192 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18a-0650-16b8-a6e0-c8108fbb2455, command-id=01f0f18a-0677-10d2-b3d2-9ac0fd789c28) - Closing
[0m02:15:55.023047 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:15:55.023904 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: Close
[0m02:15:55.024380 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18a-0650-16b8-a6e0-c8108fbb2455) - Closing
[0m02:15:55.266841 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fea84ca2-42e9-4ccf-a08e-e0dde0b549b7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa899af8290>]}
[0m02:15:55.267627 [info ] [Thread-3 (]: 5 of 5 OK created sql view model customer_bi.bronze_store ...................... [[32mOK[0m in 1.91s]
[0m02:15:55.268110 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_store
[0m02:15:55.269685 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:15:55.270060 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:15:55.270599 [info ] [MainThread]: 
[0m02:15:55.270954 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 12.86 seconds (12.86s).
[0m02:15:55.271971 [debug] [MainThread]: Command end result
[0m02:15:55.337937 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:15:55.354858 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:15:55.371796 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m02:15:55.372366 [info ] [MainThread]: 
[0m02:15:55.372862 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:15:55.373224 [info ] [MainThread]: 
[0m02:15:55.373622 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m02:15:55.374545 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 14.534926, "process_in_blocks": "0", "process_kernel_time": 0.566899, "process_mem_max_rss": "261788", "process_out_blocks": "3440", "process_user_time": 3.990525}
[0m02:15:55.381613 [debug] [MainThread]: Command `dbt run` succeeded at 02:15:55.381357 after 14.54 seconds
[0m02:15:55.382198 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa89c1a77d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa89dd9c2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa876285df0>]}
[0m02:15:55.382644 [debug] [MainThread]: Flushing usage events
[0m02:15:56.328860 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:17:06.589995 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03d93bcc20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03dadc0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03d834fc50>]}


============================== 02:17:06.603765 | 1dec9a1f-455a-4421-857f-0a5c87ae0802 ==============================
[0m02:17:06.603765 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:17:06.604602 [debug] [MainThread]: running dbt with arguments {'indirect_selection': 'eager', 'log_format': 'default', 'version_check': 'True', 'write_json': 'True', 'introspect': 'True', 'fail_fast': 'False', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/ubuntu/.dbt', 'send_anonymous_usage_stats': 'True', 'use_experimental_parser': 'False', 'debug': 'False', 'empty': 'False', 'target_path': 'None', 'printer_width': '80', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'log_cache_events': 'False', 'use_colors': 'True', 'no_print': 'None', 'partial_parse': 'True', 'warn_error': 'None', 'static_parser': 'True', 'quiet': 'False', 'cache_selected_only': 'False'}
[0m02:17:09.203224 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:17:09.203907 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:17:09.204383 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:17:10.669570 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03d9044fc0>]}
[0m02:17:10.795058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b293e140>]}
[0m02:17:10.796014 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m02:17:10.894334 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:17:10.895012 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b2909e50>]}
[0m02:17:10.906513 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m02:17:10.994834 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:17:10.995212 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m02:17:10.995443 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:17:11.003794 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dev.bronze
[0m02:17:11.031446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b25e5310>]}
[0m02:17:11.146914 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:17:11.149185 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:17:11.159462 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b299c4b0>]}
[0m02:17:11.160170 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m02:17:11.160746 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b24edf30>]}
[0m02:17:11.163732 [info ] [MainThread]: 
[0m02:17:11.164203 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:17:11.164550 [info ] [MainThread]: 
[0m02:17:11.165162 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:17:11.165493 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:17:11.173988 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m02:17:11.174584 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m02:17:11.191283 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m02:17:11.191811 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m02:17:11.194852 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:17:11.999093 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18a-34c2-131f-ac49-d014a13d1049) - Created
[0m02:17:12.423627 [debug] [ThreadPool]: SQL status: OK in 1.230 seconds
[0m02:17:12.427946 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18a-34c2-131f-ac49-d014a13d1049, command-id=01f0f18a-34ec-1aaf-a72d-8382aaa7f124) - Closing
[0m02:17:12.428599 [debug] [ThreadPool]: On list_dev: Close
[0m02:17:12.428925 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18a-34c2-131f-ac49-d014a13d1049) - Closing
[0m02:17:15.030832 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_dev_customer_bi) - Creating connection
[0m02:17:15.031259 [debug] [ThreadPool]: Acquiring new databricks connection 'create_dev_customer_bi'
[0m02:17:15.031799 [debug] [ThreadPool]: Creating schema "database: "dev"
schema: "customer_bi"
"
[0m02:17:15.037323 [debug] [ThreadPool]: Using databricks connection "create_dev_customer_bi"
[0m02:17:15.037739 [debug] [ThreadPool]: On create_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "create_dev_customer_bi"} */
create schema if not exists `dev`.`customer_bi`
  
[0m02:17:15.038053 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:17:15.796795 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18a-35b4-1afa-bf8f-278234a01820) - Created
[0m02:17:16.429677 [debug] [ThreadPool]: SQL status: OK in 1.390 seconds
[0m02:17:16.431246 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18a-35b4-1afa-bf8f-278234a01820, command-id=01f0f18a-35db-1d75-9b2d-dae549499685) - Closing
[0m02:17:16.431836 [debug] [ThreadPool]: On create_dev_customer_bi: Close
[0m02:17:16.432283 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18a-35b4-1afa-bf8f-278234a01820) - Closing
[0m02:17:16.702770 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m02:17:16.704039 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m02:17:16.709844 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m02:17:16.710160 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m02:17:16.710403 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:17:17.475745 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18a-36cb-1d2f-9076-eaf6fe9ca8a9) - Created
[0m02:17:17.995042 [debug] [ThreadPool]: SQL status: OK in 1.280 seconds
[0m02:17:17.998579 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18a-36cb-1d2f-9076-eaf6fe9ca8a9, command-id=01f0f18a-36f1-1806-b412-21d404d62ebb) - Closing
[0m02:17:17.998964 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m02:17:17.999283 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18a-36cb-1d2f-9076-eaf6fe9ca8a9) - Closing
[0m02:17:18.244590 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b253e510>]}
[0m02:17:18.250261 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_customer
[0m02:17:18.251085 [info ] [Thread-4 (]: 1 of 5 START sql view model customer_bi.bronze_customer ........................ [RUN]
[0m02:17:18.251898 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m02:17:18.252348 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m02:17:18.252818 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_customer
[0m02:17:18.258608 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m02:17:18.259195 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_customer
[0m02:17:18.269674 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:17:18.271209 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:17:18.271799 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b25d12e0>]}
[0m02:17:18.283560 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_customer`
[0m02:17:18.291659 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m02:17:18.292237 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m02:17:18.292677 [debug] [Thread-4 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_customer`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  )

[0m02:17:18.292977 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:17:19.053744 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18a-37cf-1a58-9574-eb74ec52a3c8) - Created
[0m02:17:19.858595 [debug] [Thread-4 (]: SQL status: OK in 1.570 seconds
[0m02:17:19.859449 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18a-37cf-1a58-9574-eb74ec52a3c8, command-id=01f0f18a-37f4-1d2a-abe3-92c0d926ffd4) - Closing
[0m02:17:19.867081 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:17:19.868482 [debug] [Thread-4 (]: On model.customer_bi.bronze_customer: Close
[0m02:17:19.868772 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18a-37cf-1a58-9574-eb74ec52a3c8) - Closing
[0m02:17:20.129541 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03d91ca530>]}
[0m02:17:20.131113 [info ] [Thread-4 (]: 1 of 5 OK created sql view model customer_bi.bronze_customer ................... [[32mOK[0m in 1.88s]
[0m02:17:20.132109 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_customer
[0m02:17:20.132698 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_date
[0m02:17:20.133379 [info ] [Thread-4 (]: 2 of 5 START sql view model customer_bi.bronze_date ............................ [RUN]
[0m02:17:20.134515 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m02:17:20.134873 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m02:17:20.135178 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_date
[0m02:17:20.138057 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m02:17:20.138549 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_date
[0m02:17:20.141089 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:17:20.142646 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_date`
[0m02:17:20.143881 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m02:17:20.144557 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m02:17:20.145047 [debug] [Thread-4 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_date`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_date`
  )

[0m02:17:20.145526 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:17:20.950364 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18a-3908-1450-b41f-1f973ce1d5f4) - Created
[0m02:17:21.656326 [debug] [Thread-4 (]: SQL status: OK in 1.510 seconds
[0m02:17:21.657662 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18a-3908-1450-b41f-1f973ce1d5f4, command-id=01f0f18a-392d-1be7-b8a0-0b4c9e177408) - Closing
[0m02:17:21.658610 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:17:21.659613 [debug] [Thread-4 (]: On model.customer_bi.bronze_date: Close
[0m02:17:21.660117 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18a-3908-1450-b41f-1f973ce1d5f4) - Closing
[0m02:17:21.933264 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b24fe2d0>]}
[0m02:17:21.934328 [info ] [Thread-4 (]: 2 of 5 OK created sql view model customer_bi.bronze_date ....................... [[32mOK[0m in 1.80s]
[0m02:17:21.935086 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_date
[0m02:17:21.935549 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_returns
[0m02:17:21.936225 [info ] [Thread-4 (]: 3 of 5 START sql view model customer_bi.bronze_returns ......................... [RUN]
[0m02:17:21.936893 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m02:17:21.937326 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m02:17:21.937734 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_returns
[0m02:17:21.998087 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m02:17:21.998917 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_returns
[0m02:17:22.008472 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:17:22.009699 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_returns`
[0m02:17:22.010499 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m02:17:22.011031 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m02:17:22.011460 [debug] [Thread-4 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_returns`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  )

[0m02:17:22.011803 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:17:22.805642 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18a-3a39-1b20-87c2-e7f2ff6c3582) - Created
[0m02:17:23.615760 [debug] [Thread-4 (]: SQL status: OK in 1.600 seconds
[0m02:17:23.616882 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18a-3a39-1b20-87c2-e7f2ff6c3582, command-id=01f0f18a-3a5f-181e-bbb0-b1985bb239a4) - Closing
[0m02:17:23.617677 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:17:23.618567 [debug] [Thread-4 (]: On model.customer_bi.bronze_returns: Close
[0m02:17:23.618949 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18a-3a39-1b20-87c2-e7f2ff6c3582) - Closing
[0m02:17:23.888999 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b21efa10>]}
[0m02:17:23.889773 [info ] [Thread-4 (]: 3 of 5 OK created sql view model customer_bi.bronze_returns .................... [[32mOK[0m in 1.95s]
[0m02:17:23.890358 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_returns
[0m02:17:23.890772 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_sales
[0m02:17:23.891244 [info ] [Thread-4 (]: 4 of 5 START sql view model customer_bi.bronze_sales ........................... [RUN]
[0m02:17:23.891752 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m02:17:23.892066 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m02:17:23.892360 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_sales
[0m02:17:23.901983 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m02:17:23.902750 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_sales
[0m02:17:23.904842 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:17:23.905973 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_sales`
[0m02:17:23.914893 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m02:17:23.915790 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m02:17:23.916313 [debug] [Thread-4 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_sales`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  )

[0m02:17:23.916719 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:17:24.768707 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18a-3b7c-134a-baa2-e4365286d5d0) - Created
[0m02:17:25.489349 [debug] [Thread-4 (]: SQL status: OK in 1.570 seconds
[0m02:17:25.490466 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18a-3b7c-134a-baa2-e4365286d5d0, command-id=01f0f18a-3ba1-18cb-984f-b91e50f5fd73) - Closing
[0m02:17:25.491181 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:17:25.492014 [debug] [Thread-4 (]: On model.customer_bi.bronze_sales: Close
[0m02:17:25.492371 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18a-3b7c-134a-baa2-e4365286d5d0) - Closing
[0m02:17:25.746789 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b2431190>]}
[0m02:17:25.747530 [info ] [Thread-4 (]: 4 of 5 OK created sql view model customer_bi.bronze_sales ...................... [[32mOK[0m in 1.85s]
[0m02:17:25.748203 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_sales
[0m02:17:25.748621 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_store
[0m02:17:25.749112 [info ] [Thread-4 (]: 5 of 5 START sql view model customer_bi.bronze_store ........................... [RUN]
[0m02:17:25.749619 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m02:17:25.749919 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m02:17:25.750214 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_store
[0m02:17:25.752600 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m02:17:25.753229 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_store
[0m02:17:25.763035 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:17:25.765102 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_store`
[0m02:17:25.765878 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m02:17:25.766408 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m02:17:25.766742 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_store`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_store`
  )

[0m02:17:25.767034 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:17:26.545003 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18a-3ca1-1d08-a760-88f2ada125f6) - Created
[0m02:17:27.250581 [debug] [Thread-4 (]: SQL status: OK in 1.480 seconds
[0m02:17:27.251622 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18a-3ca1-1d08-a760-88f2ada125f6, command-id=01f0f18a-3cc8-1378-a134-7225328e4fe7) - Closing
[0m02:17:27.252392 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:17:27.253162 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: Close
[0m02:17:27.253502 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18a-3ca1-1d08-a760-88f2ada125f6) - Closing
[0m02:17:27.524521 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1dec9a1f-455a-4421-857f-0a5c87ae0802', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b2431190>]}
[0m02:17:27.525360 [info ] [Thread-4 (]: 5 of 5 OK created sql view model customer_bi.bronze_store ...................... [[32mOK[0m in 1.77s]
[0m02:17:27.525905 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_store
[0m02:17:27.534223 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:17:27.534671 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:17:27.535202 [info ] [MainThread]: 
[0m02:17:27.535561 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 16.37 seconds (16.37s).
[0m02:17:27.536661 [debug] [MainThread]: Command end result
[0m02:17:27.640135 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:17:27.642473 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:17:27.656700 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m02:17:27.657218 [info ] [MainThread]: 
[0m02:17:27.657726 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:17:27.658058 [info ] [MainThread]: 
[0m02:17:27.658436 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m02:17:27.708591 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 18.922176, "process_in_blocks": "0", "process_kernel_time": 1.948413, "process_mem_max_rss": "261672", "process_out_blocks": "3448", "process_user_time": 8.409529}
[0m02:17:27.709507 [debug] [MainThread]: Command `dbt run` succeeded at 02:17:27.709341 after 18.95 seconds
[0m02:17:27.709994 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b28bc6b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03d97ab6b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f03b23cfe90>]}
[0m02:17:27.710442 [debug] [MainThread]: Flushing usage events
[0m02:17:28.673144 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:26:38.035239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb11e824c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb120058190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb11d7bbc50>]}


============================== 02:26:38.050405 | 5cabf6bb-c243-4d86-a40e-ea651ca96d56 ==============================
[0m02:26:38.050405 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:26:38.051002 [debug] [MainThread]: running dbt with arguments {'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'send_anonymous_usage_stats': 'True', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'fail_fast': 'False', 'cache_selected_only': 'False', 'debug': 'False', 'partial_parse': 'True', 'static_parser': 'True', 'version_check': 'True', 'invocation_command': 'dbt run', 'log_format': 'default', 'introspect': 'True', 'no_print': 'None', 'quiet': 'False', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'target_path': 'None', 'use_experimental_parser': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'empty': 'False', 'printer_width': '80', 'write_json': 'True'}
[0m02:26:39.866724 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:26:39.867272 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:26:39.867590 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:26:40.637492 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb11e4acfc0>]}
[0m02:26:40.692432 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f7d02140>]}
[0m02:26:40.693015 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m02:26:40.765996 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:26:40.766698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f7ecde50>]}
[0m02:26:40.775162 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m02:26:40.869264 [warn ] [MainThread]: [[33mWARNING[0m][CustomTopLevelKeyDeprecation]: Deprecated functionality
Unexpected top-level key model in file `models/bronze/properties.yml`
[0m02:26:40.869861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f79a8c80>]}
[0m02:26:40.878803 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:26:40.879343 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m02:26:40.879674 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:26:40.887087 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dev.bronze
[0m02:26:40.924718 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f7c14910>]}
[0m02:26:40.995009 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:26:40.996951 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:26:41.004418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f780f5f0>]}
[0m02:26:41.004804 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m02:26:41.005096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f7744e90>]}
[0m02:26:41.006537 [info ] [MainThread]: 
[0m02:26:41.006824 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:26:41.007052 [info ] [MainThread]: 
[0m02:26:41.007446 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:26:41.007672 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:26:41.013949 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m02:26:41.014407 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m02:26:41.028138 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m02:26:41.028636 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m02:26:41.028931 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:26:42.095247 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18b-8788-1b4f-97c4-22412df231b2) - Created
[0m02:26:42.900566 [debug] [ThreadPool]: SQL status: OK in 1.870 seconds
[0m02:26:42.906182 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18b-8788-1b4f-97c4-22412df231b2, command-id=01f0f18b-87b5-1df8-862f-ade70cf8db52) - Closing
[0m02:26:42.906698 [debug] [ThreadPool]: On list_dev: Close
[0m02:26:42.906984 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18b-8788-1b4f-97c4-22412df231b2) - Closing
[0m02:26:43.190599 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m02:26:43.190985 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m02:26:43.195332 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m02:26:43.195683 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m02:26:43.195954 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:26:43.993837 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18b-88c1-1225-87ca-2f5bc0bdbda2) - Created
[0m02:26:44.726360 [debug] [ThreadPool]: SQL status: OK in 1.530 seconds
[0m02:26:44.730815 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18b-88c1-1225-87ca-2f5bc0bdbda2, command-id=01f0f18b-88ea-121f-840d-e18b1baff3f7) - Closing
[0m02:26:44.731339 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m02:26:44.731602 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18b-88c1-1225-87ca-2f5bc0bdbda2) - Closing
[0m02:26:45.031565 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f77852e0>]}
[0m02:26:45.036144 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_customer
[0m02:26:45.036723 [info ] [Thread-3 (]: 1 of 5 START sql view model customer_bi.bronze_customer ........................ [RUN]
[0m02:26:45.037351 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m02:26:45.037761 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m02:26:45.038233 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_customer
[0m02:26:45.043448 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m02:26:45.044033 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_customer
[0m02:26:45.057849 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:26:45.059810 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:26:45.060298 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f77c4190>]}
[0m02:26:45.072241 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_customer`
[0m02:26:45.081598 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m02:26:45.082352 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m02:26:45.082807 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_customer`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  )

[0m02:26:45.083107 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:26:45.886416 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18b-89f8-1e3a-9806-e40625ad04b3) - Created
[0m02:26:48.768588 [debug] [Thread-3 (]: SQL status: OK in 3.690 seconds
[0m02:26:48.770029 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18b-89f8-1e3a-9806-e40625ad04b3, command-id=01f0f18b-8a29-16d0-85ab-7d212bd503a8) - Closing
[0m02:26:48.777573 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:26:48.778990 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: Close
[0m02:26:48.779258 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18b-89f8-1e3a-9806-e40625ad04b3) - Closing
[0m02:26:49.039562 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb11e6339d0>]}
[0m02:26:49.040722 [info ] [Thread-3 (]: 1 of 5 OK created sql view model customer_bi.bronze_customer ................... [[32mOK[0m in 4.00s]
[0m02:26:49.041732 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_customer
[0m02:26:49.042389 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_date
[0m02:26:49.042849 [info ] [Thread-3 (]: 2 of 5 START sql view model customer_bi.bronze_date ............................ [RUN]
[0m02:26:49.043350 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m02:26:49.043674 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m02:26:49.043956 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_date
[0m02:26:49.046071 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m02:26:49.046613 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_date
[0m02:26:49.047867 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:26:49.051049 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_date`
[0m02:26:49.051777 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m02:26:49.052214 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m02:26:49.052506 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_date`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_date`
  )

[0m02:26:49.052728 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:26:49.812811 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18b-8c89-15f7-8903-e98220213e29) - Created
[0m02:26:50.791644 [debug] [Thread-3 (]: SQL status: OK in 1.740 seconds
[0m02:26:50.792841 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18b-8c89-15f7-8903-e98220213e29, command-id=01f0f18b-8cb0-1766-a67c-df06a778d6a3) - Closing
[0m02:26:50.793674 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:26:50.794609 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: Close
[0m02:26:50.795168 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18b-8c89-15f7-8903-e98220213e29) - Closing
[0m02:26:51.043864 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f766af90>]}
[0m02:26:51.044765 [info ] [Thread-3 (]: 2 of 5 OK created sql view model customer_bi.bronze_date ....................... [[32mOK[0m in 2.00s]
[0m02:26:51.045460 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_date
[0m02:26:51.045908 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_returns
[0m02:26:51.046577 [info ] [Thread-3 (]: 3 of 5 START sql view model customer_bi.bronze_returns ......................... [RUN]
[0m02:26:51.047342 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m02:26:51.047744 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m02:26:51.048061 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_returns
[0m02:26:51.051206 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m02:26:51.079537 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_returns
[0m02:26:51.083011 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:26:51.084174 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_returns`
[0m02:26:51.085090 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m02:26:51.085872 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m02:26:51.086233 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_returns`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  )

[0m02:26:51.086645 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:26:51.938258 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18b-8de7-13d8-a72b-233ec4dfddc3) - Created
[0m02:26:52.823767 [debug] [Thread-3 (]: SQL status: OK in 1.740 seconds
[0m02:26:52.824924 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18b-8de7-13d8-a72b-233ec4dfddc3, command-id=01f0f18b-8e12-1714-90d4-5793eab56057) - Closing
[0m02:26:52.825778 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:26:52.826530 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: Close
[0m02:26:52.826874 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18b-8de7-13d8-a72b-233ec4dfddc3) - Closing
[0m02:26:53.087239 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f7975250>]}
[0m02:26:53.088159 [info ] [Thread-3 (]: 3 of 5 OK created sql view model customer_bi.bronze_returns .................... [[32mOK[0m in 2.04s]
[0m02:26:53.088855 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_returns
[0m02:26:53.089319 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_sales
[0m02:26:53.089883 [info ] [Thread-3 (]: 4 of 5 START sql view model customer_bi.bronze_sales ........................... [RUN]
[0m02:26:53.090423 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m02:26:53.090740 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m02:26:53.091040 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_sales
[0m02:26:53.094011 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m02:26:53.095249 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_sales
[0m02:26:53.098450 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:26:53.107208 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_sales`
[0m02:26:53.108250 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m02:26:53.109230 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m02:26:53.109836 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_sales`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  )

[0m02:26:53.110238 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:26:53.907170 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18b-8f33-13f4-8490-af9e386e13e6) - Created
[0m02:26:54.803951 [debug] [Thread-3 (]: SQL status: OK in 1.690 seconds
[0m02:26:54.805136 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18b-8f33-13f4-8490-af9e386e13e6, command-id=01f0f18b-8f58-1971-b93c-d592e8d0b3d1) - Closing
[0m02:26:54.805940 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:26:54.806836 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: Close
[0m02:26:54.807267 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18b-8f33-13f4-8490-af9e386e13e6) - Closing
[0m02:26:55.065350 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f7975970>]}
[0m02:26:55.066307 [info ] [Thread-3 (]: 4 of 5 OK created sql view model customer_bi.bronze_sales ...................... [[32mOK[0m in 1.97s]
[0m02:26:55.066982 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_sales
[0m02:26:55.067432 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_store
[0m02:26:55.067972 [info ] [Thread-3 (]: 5 of 5 START sql view model customer_bi.bronze_store ........................... [RUN]
[0m02:26:55.068542 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m02:26:55.068961 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m02:26:55.069376 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_store
[0m02:26:55.079947 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m02:26:55.102832 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_store
[0m02:26:55.105940 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:26:55.107265 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_store`
[0m02:26:55.108235 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m02:26:55.131166 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m02:26:55.131764 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_store`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_store`
  )

[0m02:26:55.132146 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:26:55.929525 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18b-9082-15b4-89ae-a99082992456) - Created
[0m02:26:56.850854 [debug] [Thread-3 (]: SQL status: OK in 1.720 seconds
[0m02:26:56.852075 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18b-9082-15b4-89ae-a99082992456, command-id=01f0f18b-90a8-178a-b9f4-e66dd3c96dac) - Closing
[0m02:26:56.852843 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:26:56.853526 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: Close
[0m02:26:56.853837 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18b-9082-15b4-89ae-a99082992456) - Closing
[0m02:26:57.090940 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5cabf6bb-c243-4d86-a40e-ea651ca96d56', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f7975fd0>]}
[0m02:26:57.091889 [info ] [Thread-3 (]: 5 of 5 OK created sql view model customer_bi.bronze_store ...................... [[32mOK[0m in 2.02s]
[0m02:26:57.092543 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_store
[0m02:26:57.094418 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:26:57.094821 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:26:57.095385 [info ] [MainThread]: 
[0m02:26:57.095721 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 16.09 seconds (16.09s).
[0m02:26:57.096659 [debug] [MainThread]: Command end result
[0m02:26:57.182529 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:26:57.185189 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:26:57.202424 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m02:26:57.202908 [info ] [MainThread]: 
[0m02:26:57.211663 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:26:57.212135 [info ] [MainThread]: 
[0m02:26:57.212648 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m02:26:57.213365 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- CustomTopLevelKeyDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m02:26:57.214300 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 19.375332, "process_in_blocks": "0", "process_kernel_time": 4.063336, "process_mem_max_rss": "262492", "process_out_blocks": "3440", "process_user_time": 6.281639}
[0m02:26:57.214816 [debug] [MainThread]: Command `dbt run` succeeded at 02:26:57.214714 after 19.38 seconds
[0m02:26:57.215270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb11ec48a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f7d37410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0f7d37bf0>]}
[0m02:26:57.215752 [debug] [MainThread]: Flushing usage events
[0m02:26:58.311111 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:28:31.160265 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5270fd8c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f527280c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f526ff5fc50>]}


============================== 02:28:31.171515 | f617e5ab-6da1-4817-b4dc-d9e7eb03d618 ==============================
[0m02:28:31.171515 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:28:31.172162 [debug] [MainThread]: running dbt with arguments {'write_json': 'True', 'static_parser': 'True', 'empty': 'None', 'profiles_dir': '/home/ubuntu/.dbt', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'version_check': 'True', 'use_colors': 'True', 'use_experimental_parser': 'False', 'partial_parse': 'True', 'printer_width': '80', 'log_format': 'default', 'invocation_command': 'dbt clean', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'warn_error': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'introspect': 'True', 'quiet': 'False', 'fail_fast': 'False', 'debug': 'False', 'target_path': 'None', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'cache_selected_only': 'False'}
[0m02:28:31.491467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f617e5ab-6da1-4817-b4dc-d9e7eb03d618', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5270c5cfc0>]}
[0m02:28:31.618176 [debug] [MainThread]: Resource report: {"command_name": "clean", "command_success": true, "command_wall_clock_time": 0.61995083, "process_in_blocks": "8", "process_kernel_time": 1.370664, "process_mem_max_rss": "102660", "process_out_blocks": "32", "process_user_time": 3.949371}
[0m02:28:31.618741 [debug] [MainThread]: Command `dbt clean` succeeded at 02:28:31.618600 after 0.62 seconds
[0m02:28:31.619239 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f526febc050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f526febc490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f526feed950>]}
[0m02:28:31.619700 [debug] [MainThread]: Flushing usage events
[0m02:28:32.468774 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:29:41.411092 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f583a228c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f583bab8190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58391bbc50>]}


============================== 02:29:41.415106 | b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf ==============================
[0m02:29:41.415106 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:29:41.415716 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'use_experimental_parser': 'False', 'no_print': 'None', 'warn_error': 'None', 'use_colors': 'True', 'version_check': 'True', 'static_parser': 'True', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'debug': 'False', 'send_anonymous_usage_stats': 'True', 'printer_width': '80', 'target_path': 'None', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'introspect': 'True', 'log_cache_events': 'False', 'log_format': 'default', 'fail_fast': 'False', 'quiet': 'False', 'partial_parse': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'cache_selected_only': 'False'}
[0m02:29:44.205486 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:29:44.206372 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:29:44.206995 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:29:44.867992 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5839eacfc0>]}
[0m02:29:44.920229 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58137ae360>]}
[0m02:29:44.920907 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m02:29:44.988849 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:29:44.989421 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5813976250>]}
[0m02:29:44.998098 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m02:29:45.083064 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:29:45.083459 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m02:29:45.083729 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:29:45.092753 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dev.bronze
[0m02:29:45.123257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5813465310>]}
[0m02:29:45.221884 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:29:45.223938 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:29:45.230987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f581380c4b0>]}
[0m02:29:45.231417 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m02:29:45.231812 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5813399f30>]}
[0m02:29:45.233658 [info ] [MainThread]: 
[0m02:29:45.234027 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:29:45.234368 [info ] [MainThread]: 
[0m02:29:45.234850 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:29:45.235100 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:29:45.240874 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m02:29:45.241504 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m02:29:45.261176 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m02:29:45.261839 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m02:29:45.262280 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:29:46.022072 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18b-f59e-1beb-bf46-7f89f5f5dd56) - Created
[0m02:29:46.461533 [debug] [ThreadPool]: SQL status: OK in 1.200 seconds
[0m02:29:46.464549 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18b-f59e-1beb-bf46-7f89f5f5dd56, command-id=01f0f18b-f5c5-19e9-b651-758ec6afe0bc) - Closing
[0m02:29:46.465090 [debug] [ThreadPool]: On list_dev: Close
[0m02:29:46.465395 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18b-f59e-1beb-bf46-7f89f5f5dd56) - Closing
[0m02:29:46.703377 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_dev_customer_bi) - Creating connection
[0m02:29:46.703790 [debug] [ThreadPool]: Acquiring new databricks connection 'create_dev_customer_bi'
[0m02:29:46.704349 [debug] [ThreadPool]: Creating schema "database: "dev"
schema: "customer_bi"
"
[0m02:29:46.708509 [debug] [ThreadPool]: Using databricks connection "create_dev_customer_bi"
[0m02:29:46.709441 [debug] [ThreadPool]: On create_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "create_dev_customer_bi"} */
create schema if not exists `dev`.`customer_bi`
  
[0m02:29:46.709894 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:29:47.486348 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18b-f695-1267-9926-ef597f3fb406) - Created
[0m02:29:48.070155 [debug] [ThreadPool]: SQL status: OK in 1.360 seconds
[0m02:29:48.070933 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18b-f695-1267-9926-ef597f3fb406, command-id=01f0f18b-f6bb-14e3-bf3e-fcd08997409f) - Closing
[0m02:29:48.071304 [debug] [ThreadPool]: On create_dev_customer_bi: Close
[0m02:29:48.071556 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18b-f695-1267-9926-ef597f3fb406) - Closing
[0m02:29:48.325865 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m02:29:48.326711 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m02:29:48.332580 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m02:29:48.332910 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m02:29:48.333145 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:29:49.072897 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18b-f7a1-182b-aaca-cd82257f573e) - Created
[0m02:29:49.585789 [debug] [ThreadPool]: SQL status: OK in 1.250 seconds
[0m02:29:49.636525 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18b-f7a1-182b-aaca-cd82257f573e, command-id=01f0f18b-f7c6-1d7a-86a6-dc041cab3f42) - Closing
[0m02:29:49.637388 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m02:29:49.637939 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18b-f7a1-182b-aaca-cd82257f573e) - Closing
[0m02:29:49.880252 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5813286510>]}
[0m02:29:49.884022 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_customer
[0m02:29:49.884762 [info ] [Thread-4 (]: 1 of 5 START sql view model customer_bi.bronze_customer ........................ [RUN]
[0m02:29:49.885473 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m02:29:49.885882 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m02:29:49.886302 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_customer
[0m02:29:49.903200 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m02:29:49.928162 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_customer
[0m02:29:49.974577 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:29:49.985831 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:29:49.986613 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f581344d2e0>]}
[0m02:29:50.019614 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_customer`
[0m02:29:50.059675 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m02:29:50.213487 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m02:29:50.214087 [debug] [Thread-4 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_customer`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  )

[0m02:29:50.214490 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:29:51.024631 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18b-f8e8-1bbf-8264-6dbce3ad7c22) - Created
[0m02:29:51.954559 [debug] [Thread-4 (]: SQL status: OK in 1.740 seconds
[0m02:29:51.955614 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18b-f8e8-1bbf-8264-6dbce3ad7c22, command-id=01f0f18b-f915-103c-9793-c09ac9502a80) - Closing
[0m02:29:51.976259 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:29:51.978661 [debug] [Thread-4 (]: On model.customer_bi.bronze_customer: Close
[0m02:29:51.979269 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18b-f8e8-1bbf-8264-6dbce3ad7c22) - Closing
[0m02:29:52.261313 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f583a03a530>]}
[0m02:29:52.262299 [info ] [Thread-4 (]: 1 of 5 OK created sql view model customer_bi.bronze_customer ................... [[32mOK[0m in 2.37s]
[0m02:29:52.263088 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_customer
[0m02:29:52.263654 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_date
[0m02:29:52.264490 [info ] [Thread-4 (]: 2 of 5 START sql view model customer_bi.bronze_date ............................ [RUN]
[0m02:29:52.265312 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m02:29:52.265790 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m02:29:52.273670 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_date
[0m02:29:52.278715 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m02:29:52.280581 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_date
[0m02:29:52.290742 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:29:52.292498 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_date`
[0m02:29:52.293685 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m02:29:52.337006 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m02:29:52.337750 [debug] [Thread-4 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_date`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_date`
  )

[0m02:29:52.338390 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:29:53.237002 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18b-fa58-1e62-ab58-fc8f19b78daf) - Created
[0m02:29:53.907538 [debug] [Thread-4 (]: SQL status: OK in 1.570 seconds
[0m02:29:53.908767 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18b-fa58-1e62-ab58-fc8f19b78daf, command-id=01f0f18b-fa82-1377-ad44-4db979ee1677) - Closing
[0m02:29:53.909731 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:29:53.910795 [debug] [Thread-4 (]: On model.customer_bi.bronze_date: Close
[0m02:29:53.911262 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18b-fa58-1e62-ab58-fc8f19b78daf) - Closing
[0m02:29:54.147855 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5813401a50>]}
[0m02:29:54.148712 [info ] [Thread-4 (]: 2 of 5 OK created sql view model customer_bi.bronze_date ....................... [[32mOK[0m in 1.88s]
[0m02:29:54.149379 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_date
[0m02:29:54.149787 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_returns
[0m02:29:54.150310 [info ] [Thread-4 (]: 3 of 5 START sql view model customer_bi.bronze_returns ......................... [RUN]
[0m02:29:54.150969 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m02:29:54.151395 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m02:29:54.151797 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_returns
[0m02:29:54.166898 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m02:29:54.168161 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_returns
[0m02:29:54.178667 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:29:54.180552 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_returns`
[0m02:29:54.181929 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m02:29:54.191000 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m02:29:54.191653 [debug] [Thread-4 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_returns`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  )

[0m02:29:54.192029 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:29:54.976883 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18b-fb81-1d80-b3ac-02a9f391bd2c) - Created
[0m02:29:55.666899 [debug] [Thread-4 (]: SQL status: OK in 1.470 seconds
[0m02:29:55.668277 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18b-fb81-1d80-b3ac-02a9f391bd2c, command-id=01f0f18b-fba7-1a97-b22a-329a931e3d10) - Closing
[0m02:29:55.669219 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:29:55.670256 [debug] [Thread-4 (]: On model.customer_bi.bronze_returns: Close
[0m02:29:55.670620 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18b-fb81-1d80-b3ac-02a9f391bd2c) - Closing
[0m02:29:55.910767 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5813220f30>]}
[0m02:29:55.911677 [info ] [Thread-4 (]: 3 of 5 OK created sql view model customer_bi.bronze_returns .................... [[32mOK[0m in 1.76s]
[0m02:29:55.912395 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_returns
[0m02:29:55.912989 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_sales
[0m02:29:55.913778 [info ] [Thread-4 (]: 4 of 5 START sql view model customer_bi.bronze_sales ........................... [RUN]
[0m02:29:55.914505 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m02:29:55.914995 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m02:29:55.915474 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_sales
[0m02:29:55.920362 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m02:29:55.921329 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_sales
[0m02:29:55.932568 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:29:55.934208 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_sales`
[0m02:29:55.945148 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m02:29:56.053145 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m02:29:56.053757 [debug] [Thread-4 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_sales`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  )

[0m02:29:56.054118 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:29:56.917448 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18b-fcc1-13a5-b9bc-ff610332b43d) - Created
[0m02:29:57.596170 [debug] [Thread-4 (]: SQL status: OK in 1.540 seconds
[0m02:29:57.597804 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18b-fcc1-13a5-b9bc-ff610332b43d, command-id=01f0f18b-fcec-1e59-9120-4e9623fc11a3) - Closing
[0m02:29:57.598798 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:29:57.599692 [debug] [Thread-4 (]: On model.customer_bi.bronze_sales: Close
[0m02:29:57.600061 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18b-fcc1-13a5-b9bc-ff610332b43d) - Closing
[0m02:29:57.939084 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f581382df70>]}
[0m02:29:57.939932 [info ] [Thread-4 (]: 4 of 5 OK created sql view model customer_bi.bronze_sales ...................... [[32mOK[0m in 2.02s]
[0m02:29:57.940543 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_sales
[0m02:29:57.941006 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_store
[0m02:29:57.941534 [info ] [Thread-4 (]: 5 of 5 START sql view model customer_bi.bronze_store ........................... [RUN]
[0m02:29:57.942079 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m02:29:57.942449 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m02:29:57.942784 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_store
[0m02:29:57.945626 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m02:29:57.950539 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_store
[0m02:29:57.957166 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:29:57.959542 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_store`
[0m02:29:57.960667 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m02:29:57.961768 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m02:29:57.962294 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_store`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_store`
  )

[0m02:29:57.962781 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:29:58.744682 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18b-fdfa-1aee-beef-c78ba6b12c11) - Created
[0m02:29:59.551075 [debug] [Thread-4 (]: SQL status: OK in 1.590 seconds
[0m02:29:59.552620 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18b-fdfa-1aee-beef-c78ba6b12c11, command-id=01f0f18b-fe1f-1622-be85-37634dcf0642) - Closing
[0m02:29:59.553713 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:29:59.554695 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: Close
[0m02:29:59.555096 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18b-fdfa-1aee-beef-c78ba6b12c11) - Closing
[0m02:29:59.805258 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b4d495ac-a6c1-4c85-bb5b-b181dd40c8bf', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f581382e3f0>]}
[0m02:29:59.806115 [info ] [Thread-4 (]: 5 of 5 OK created sql view model customer_bi.bronze_store ...................... [[32mOK[0m in 1.86s]
[0m02:29:59.806891 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_store
[0m02:29:59.809006 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:29:59.809402 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:29:59.809964 [info ] [MainThread]: 
[0m02:29:59.810336 [info ] [MainThread]: Finished running 5 view models in 0 hours 0 minutes and 14.58 seconds (14.58s).
[0m02:29:59.811479 [debug] [MainThread]: Command end result
[0m02:29:59.884859 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:29:59.887558 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:29:59.921860 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m02:29:59.922496 [info ] [MainThread]: 
[0m02:29:59.923089 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:29:59.923566 [info ] [MainThread]: 
[0m02:29:59.924168 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m02:29:59.925172 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 18.684404, "process_in_blocks": "0", "process_kernel_time": 2.108819, "process_mem_max_rss": "262088", "process_out_blocks": "3440", "process_user_time": 8.296236}
[0m02:29:59.925704 [debug] [MainThread]: Command `dbt run` succeeded at 02:29:59.925571 after 18.69 seconds
[0m02:29:59.927402 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f58132d9c10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5839882e70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5813243e90>]}
[0m02:29:59.927800 [debug] [MainThread]: Flushing usage events
[0m02:30:00.901412 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:34:57.180816 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f92a1fb0c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f92a39d8190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f92a1107c50>]}


============================== 02:34:57.186836 | 0b45bf41-531f-4325-aa36-4b97558d2d31 ==============================
[0m02:34:57.186836 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:34:57.187446 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run', 'log_cache_events': 'False', 'version_check': 'True', 'introspect': 'True', 'fail_fast': 'False', 'warn_error': 'None', 'target_path': 'None', 'write_json': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'printer_width': '80', 'indirect_selection': 'eager', 'quiet': 'False', 'use_colors': 'True', 'debug': 'False', 'empty': 'False', 'cache_selected_only': 'False', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'no_print': 'None', 'static_parser': 'True'}
[0m02:34:59.803606 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:34:59.804521 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:34:59.805023 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:35:01.727472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f92a1c2cfc0>]}
[0m02:35:01.885587 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927b4fa140>]}
[0m02:35:01.886474 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m02:35:02.139050 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:35:02.139687 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927b6c1e50>]}
[0m02:35:02.167275 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m02:35:02.458838 [warn ] [MainThread]: [[33mWARNING[0m][CustomTopLevelKeyDeprecation]: Deprecated functionality
Unexpected top-level key model in file `models/bronze/properties.yml`
[0m02:35:02.459528 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927b1b0c80>]}
[0m02:35:02.479383 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:35:02.479883 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m02:35:02.480157 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:35:02.507010 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dev.bronze
[0m02:35:02.611551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927b410910>]}
[0m02:35:02.830497 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:35:02.836902 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:35:02.864098 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927b01b5f0>]}
[0m02:35:02.864779 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m02:35:02.865272 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927af54e90>]}
[0m02:35:02.867685 [info ] [MainThread]: 
[0m02:35:02.868162 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:35:02.868577 [info ] [MainThread]: 
[0m02:35:02.869308 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:35:02.869697 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:35:02.886224 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m02:35:02.886890 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m02:35:02.923466 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m02:35:02.923994 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m02:35:02.924269 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:35:03.745950 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18c-b335-14cb-bb27-6a7bc499fad3) - Created
[0m02:35:04.188871 [debug] [ThreadPool]: SQL status: OK in 1.260 seconds
[0m02:35:04.197127 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18c-b335-14cb-bb27-6a7bc499fad3, command-id=01f0f18c-b35c-181a-9b68-bacc65a121a7) - Closing
[0m02:35:04.197761 [debug] [ThreadPool]: On list_dev: Close
[0m02:35:04.198183 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18c-b335-14cb-bb27-6a7bc499fad3) - Closing
[0m02:35:04.443117 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_dev_customer_bi) - Creating connection
[0m02:35:04.443686 [debug] [ThreadPool]: Acquiring new databricks connection 'create_dev_customer_bi'
[0m02:35:04.444257 [debug] [ThreadPool]: Creating schema "database: "dev"
schema: "customer_bi"
"
[0m02:35:04.449825 [debug] [ThreadPool]: Using databricks connection "create_dev_customer_bi"
[0m02:35:04.450221 [debug] [ThreadPool]: On create_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "create_dev_customer_bi"} */
create schema if not exists `dev`.`customer_bi`
  
[0m02:35:04.450551 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:35:05.225417 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18c-b430-1234-ba66-4d983a544b6e) - Created
[0m02:35:05.831324 [debug] [ThreadPool]: SQL status: OK in 1.380 seconds
[0m02:35:05.832758 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18c-b430-1234-ba66-4d983a544b6e, command-id=01f0f18c-b458-198e-b334-a39b66393965) - Closing
[0m02:35:05.833441 [debug] [ThreadPool]: On create_dev_customer_bi: Close
[0m02:35:05.833924 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18c-b430-1234-ba66-4d983a544b6e) - Closing
[0m02:35:06.076617 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m02:35:06.077297 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m02:35:06.082549 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m02:35:06.083039 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m02:35:06.083444 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:35:06.961670 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18c-b544-1282-aa14-4971c3d64ee3) - Created
[0m02:35:07.522861 [debug] [ThreadPool]: SQL status: OK in 1.440 seconds
[0m02:35:07.526448 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18c-b544-1282-aa14-4971c3d64ee3, command-id=01f0f18c-b57d-1e82-97da-e2e798fb6006) - Closing
[0m02:35:07.526865 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m02:35:07.527167 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18c-b544-1282-aa14-4971c3d64ee3) - Closing
[0m02:35:07.776241 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927af95bd0>]}
[0m02:35:07.779094 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_customer
[0m02:35:07.779596 [info ] [Thread-4 (]: 1 of 5 START sql view model customer_bi.bronze_customer ........................ [RUN]
[0m02:35:07.780083 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m02:35:07.780329 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m02:35:07.780574 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_customer
[0m02:35:07.787713 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m02:35:07.789067 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_customer
[0m02:35:07.806197 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:35:07.809179 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:35:07.810052 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927ae32df0>]}
[0m02:35:07.855744 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_customer`
[0m02:35:07.873676 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m02:35:07.874636 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m02:35:07.875266 [debug] [Thread-4 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_customer`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  )

[0m02:35:07.875887 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:35:08.645577 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18c-b670-1dd2-a236-10cfacc701fa) - Created
[0m02:35:09.415277 [debug] [Thread-4 (]: SQL status: OK in 1.540 seconds
[0m02:35:09.417249 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18c-b670-1dd2-a236-10cfacc701fa, command-id=01f0f18c-b69a-1d5f-b418-ab3c15d9ab07) - Closing
[0m02:35:09.425468 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:35:09.426637 [debug] [Thread-4 (]: On model.customer_bi.bronze_customer: Close
[0m02:35:09.426887 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18c-b670-1dd2-a236-10cfacc701fa) - Closing
[0m02:35:09.662318 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f92a1db79d0>]}
[0m02:35:09.662918 [info ] [Thread-4 (]: 1 of 5 OK created sql view model customer_bi.bronze_customer ................... [[32mOK[0m in 1.88s]
[0m02:35:09.663482 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_customer
[0m02:35:09.663873 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_date
[0m02:35:09.664348 [info ] [Thread-4 (]: 2 of 5 START sql view model customer_bi.bronze_date ............................ [RUN]
[0m02:35:09.664883 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m02:35:09.665149 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m02:35:09.665429 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_date
[0m02:35:09.670131 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m02:35:09.671184 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_date
[0m02:35:09.672899 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:35:09.673979 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_date`
[0m02:35:09.674671 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m02:35:09.675132 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m02:35:09.675514 [debug] [Thread-4 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_date`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_date`
  )

[0m02:35:09.675786 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:35:10.428615 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18c-b7a2-168e-bc9e-eb31e93bae90) - Created
[0m02:35:11.126422 [debug] [Thread-4 (]: SQL status: OK in 1.450 seconds
[0m02:35:11.128287 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18c-b7a2-168e-bc9e-eb31e93bae90, command-id=01f0f18c-b7c9-19b6-841a-2a20f4d915a8) - Closing
[0m02:35:11.129366 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:35:11.130362 [debug] [Thread-4 (]: On model.customer_bi.bronze_date: Close
[0m02:35:11.130804 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18c-b7a2-168e-bc9e-eb31e93bae90) - Closing
[0m02:35:11.412755 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927ae39710>]}
[0m02:35:11.414226 [info ] [Thread-4 (]: 2 of 5 OK created sql view model customer_bi.bronze_date ....................... [[32mOK[0m in 1.75s]
[0m02:35:11.415642 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_date
[0m02:35:11.416665 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_returns
[0m02:35:11.417370 [info ] [Thread-4 (]: 3 of 5 START sql view model customer_bi.bronze_returns ......................... [RUN]
[0m02:35:11.418427 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m02:35:11.418975 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m02:35:11.419467 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_returns
[0m02:35:11.422473 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m02:35:11.422974 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_returns
[0m02:35:11.424518 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:35:11.425516 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_returns`
[0m02:35:11.426466 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m02:35:11.426938 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m02:35:11.427320 [debug] [Thread-4 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_returns`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  )

[0m02:35:11.427625 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:35:12.201026 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18c-b8ca-15c7-823f-6227d767697c) - Created
[0m02:35:12.986984 [debug] [Thread-4 (]: SQL status: OK in 1.560 seconds
[0m02:35:12.988912 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18c-b8ca-15c7-823f-6227d767697c, command-id=01f0f18c-b8f5-1b5b-b767-e1d099f8fb04) - Closing
[0m02:35:12.990003 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:35:12.990798 [debug] [Thread-4 (]: On model.customer_bi.bronze_returns: Close
[0m02:35:12.991323 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18c-b8ca-15c7-823f-6227d767697c) - Closing
[0m02:35:13.338172 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927b41e7b0>]}
[0m02:35:13.339498 [info ] [Thread-4 (]: 3 of 5 OK created sql view model customer_bi.bronze_returns .................... [[32mOK[0m in 1.92s]
[0m02:35:13.340755 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_returns
[0m02:35:13.341642 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_sales
[0m02:35:13.342214 [info ] [Thread-4 (]: 4 of 5 START sql view model customer_bi.bronze_sales ........................... [RUN]
[0m02:35:13.343036 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m02:35:13.343458 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m02:35:13.343836 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_sales
[0m02:35:13.347271 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m02:35:13.347977 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_sales
[0m02:35:13.349826 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m02:35:13.350692 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi`.`bronze_sales`
[0m02:35:13.351500 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m02:35:13.351843 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m02:35:13.352131 [debug] [Thread-4 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_sales`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  )

[0m02:35:13.352388 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:35:14.104742 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18c-ba0d-1f33-851e-3339211e264d) - Created
[0m02:35:14.798758 [debug] [Thread-4 (]: SQL status: OK in 1.450 seconds
[0m02:35:14.800537 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18c-ba0d-1f33-851e-3339211e264d, command-id=01f0f18c-ba33-16d7-b332-a8afb07b367b) - Closing
[0m02:35:14.801855 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:35:14.802957 [debug] [Thread-4 (]: On model.customer_bi.bronze_sales: Close
[0m02:35:14.803276 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18c-ba0d-1f33-851e-3339211e264d) - Closing
[0m02:35:15.064094 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927b41c410>]}
[0m02:35:15.065634 [info ] [Thread-4 (]: 4 of 5 OK created sql view model customer_bi.bronze_sales ...................... [[32mOK[0m in 1.72s]
[0m02:35:15.066979 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_sales
[0m02:35:15.067908 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_store
[0m02:35:15.068654 [info ] [Thread-4 (]: 5 of 5 START sql table model customer_bi.bronze_store .......................... [RUN]
[0m02:35:15.069625 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m02:35:15.070145 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m02:35:15.070656 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_store
[0m02:35:15.073099 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m02:35:15.073585 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_store
[0m02:35:15.086487 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m02:35:15.108015 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m02:35:15.108571 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m02:35:15.108909 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_store`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_store`
  
[0m02:35:15.109140 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m02:35:15.964585 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18c-bb32-142c-a6ab-e01ceda98828) - Created
[0m02:35:18.068928 [debug] [Thread-4 (]: SQL status: OK in 2.960 seconds
[0m02:35:18.070015 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f18c-bb32-142c-a6ab-e01ceda98828, command-id=01f0f18c-bb68-1652-adf7-d7de9334e2fa) - Closing
[0m02:35:18.070769 [debug] [Thread-4 (]: Applying tags to relation None
[0m02:35:18.103359 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: Close
[0m02:35:18.103961 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f18c-bb32-142c-a6ab-e01ceda98828) - Closing
[0m02:35:18.346533 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '0b45bf41-531f-4325-aa36-4b97558d2d31', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927b1644d0>]}
[0m02:35:18.347308 [info ] [Thread-4 (]: 5 of 5 OK created sql table model customer_bi.bronze_store ..................... [[32mOK[0m in 3.28s]
[0m02:35:18.347896 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_store
[0m02:35:18.349538 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:35:18.349899 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:35:18.350407 [info ] [MainThread]: 
[0m02:35:18.350774 [info ] [MainThread]: Finished running 1 table model, 4 view models in 0 hours 0 minutes and 15.48 seconds (15.48s).
[0m02:35:18.351934 [debug] [MainThread]: Command end result
[0m02:35:18.566446 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:35:18.568723 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:35:18.575401 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m02:35:18.583345 [info ] [MainThread]: 
[0m02:35:18.584068 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:35:18.584487 [info ] [MainThread]: 
[0m02:35:18.584906 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m02:35:18.585559 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- CustomTopLevelKeyDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m02:35:18.586473 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 21.565277, "process_in_blocks": "2816", "process_kernel_time": 3.375177, "process_mem_max_rss": "263104", "process_out_blocks": "3448", "process_user_time": 9.161196}
[0m02:35:18.587021 [debug] [MainThread]: Command `dbt run` succeeded at 02:35:18.586902 after 21.57 seconds
[0m02:35:18.587496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f92a1687bf0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927aff8a10>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f927b1aec90>]}
[0m02:35:18.587923 [debug] [MainThread]: Flushing usage events
[0m02:35:19.572721 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:42:09.545065 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bbe470c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bbfe98190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bbd5c3c50>]}


============================== 02:42:09.547841 | e0fd3d8b-79a4-4c79-a512-68e2d534f782 ==============================
[0m02:42:09.547841 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:42:09.548463 [debug] [MainThread]: running dbt with arguments {'empty': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'invocation_command': 'dbt run', 'warn_error': 'None', 'debug': 'False', 'use_colors': 'True', 'fail_fast': 'False', 'write_json': 'True', 'log_format': 'default', 'quiet': 'False', 'version_check': 'True', 'target_path': 'None', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'cache_selected_only': 'False', 'introspect': 'True', 'indirect_selection': 'eager', 'use_experimental_parser': 'False', 'printer_width': '80', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'static_parser': 'True', 'partial_parse': 'True', 'no_print': 'None'}
[0m02:42:10.184053 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:42:10.184492 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:42:10.184758 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:42:10.575749 [warn ] [MainThread]: [[33mWARNING[0m][MissingPlusPrefixDeprecation]: Deprecated functionality
Missing '+' prefix on `config` found at `dev.bronze.config` in file `/home/ubunt
u/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/dbt_project.yml`.
Hierarchical config values without a '+' prefix are deprecated in
dbt_project.yml.
[0m02:42:10.576248 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b97adfce0>]}
[0m02:42:10.652902 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b979b6360>]}
[0m02:42:10.699244 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b97b7d950>]}
[0m02:42:10.699872 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m02:42:10.758958 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:42:10.759559 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b979a98b0>]}
[0m02:42:10.770223 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m02:42:10.854211 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:42:10.854636 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m02:42:10.854890 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:42:10.864282 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.dev.bronze
[0m02:42:10.894283 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b978cc830>]}
[0m02:42:10.973798 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:42:10.975270 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:42:10.981787 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b975b75f0>]}
[0m02:42:10.982174 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m02:42:10.982479 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b97634d10>]}
[0m02:42:10.983837 [info ] [MainThread]: 
[0m02:42:10.984134 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:42:10.984379 [info ] [MainThread]: 
[0m02:42:10.984789 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:42:10.985003 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:42:10.989515 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m02:42:10.989927 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m02:42:10.999946 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m02:42:11.000326 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m02:42:11.000588 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:42:11.825611 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18d-b270-1a7c-a8c6-f396a5301b03) - Created
[0m02:42:12.413417 [debug] [ThreadPool]: SQL status: OK in 1.410 seconds
[0m02:42:12.418448 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18d-b270-1a7c-a8c6-f396a5301b03, command-id=01f0f18d-b2a0-1b48-b1e7-aca4732c004a) - Closing
[0m02:42:12.419319 [debug] [ThreadPool]: On list_dev: Close
[0m02:42:12.419837 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18d-b270-1a7c-a8c6-f396a5301b03) - Closing
[0m02:42:12.698256 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m02:42:12.698920 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m02:42:12.706315 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m02:42:12.706953 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m02:42:12.707385 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:42:13.476826 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18d-b38b-1494-aaba-d467ce661b59) - Created
[0m02:42:14.126473 [debug] [ThreadPool]: SQL status: OK in 1.420 seconds
[0m02:42:14.134837 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18d-b38b-1494-aaba-d467ce661b59, command-id=01f0f18d-b3b6-1d35-8c4a-2869bca9bad8) - Closing
[0m02:42:14.135453 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m02:42:14.135729 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18d-b38b-1494-aaba-d467ce661b59) - Closing
[0m02:42:14.430659 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b9768d2e0>]}
[0m02:42:14.434430 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_customer
[0m02:42:14.435002 [info ] [Thread-3 (]: 1 of 5 START sql view model customer_bi.bronze_customer ........................ [RUN]
[0m02:42:14.435629 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m02:42:14.435914 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m02:42:14.436176 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_customer
[0m02:42:14.442318 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m02:42:14.443284 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_customer
[0m02:42:14.461603 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:42:14.464349 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:42:14.465120 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b97448190>]}
[0m02:42:14.484866 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_customer`
[0m02:42:14.499929 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m02:42:14.500827 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m02:42:14.501400 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_customer`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  )

[0m02:42:14.501802 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:42:15.267406 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18d-b4b5-1bc4-b75d-10058ef016f7) - Created
[0m02:42:16.209365 [debug] [Thread-3 (]: SQL status: OK in 1.710 seconds
[0m02:42:16.210928 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18d-b4b5-1bc4-b75d-10058ef016f7, command-id=01f0f18d-b4dc-154f-8edf-b29094fead66) - Closing
[0m02:42:16.220126 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:42:16.221979 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: Close
[0m02:42:16.222614 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18d-b4b5-1bc4-b75d-10058ef016f7) - Closing
[0m02:42:16.464480 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bbe2779d0>]}
[0m02:42:16.465836 [info ] [Thread-3 (]: 1 of 5 OK created sql view model customer_bi.bronze_customer ................... [[32mOK[0m in 2.03s]
[0m02:42:16.467010 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_customer
[0m02:42:16.467746 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_date
[0m02:42:16.468489 [info ] [Thread-3 (]: 2 of 5 START sql view model customer_bi.bronze_date ............................ [RUN]
[0m02:42:16.469194 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m02:42:16.469525 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m02:42:16.469796 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_date
[0m02:42:16.471915 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m02:42:16.472395 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_date
[0m02:42:16.473658 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:42:16.474786 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_date`
[0m02:42:16.475461 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m02:42:16.480687 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m02:42:16.481077 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_date`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_date`
  )

[0m02:42:16.481390 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:42:17.257236 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18d-b601-1f2c-8b20-dafdf49fcbdb) - Created
[0m02:42:18.182771 [debug] [Thread-3 (]: SQL status: OK in 1.700 seconds
[0m02:42:18.184636 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18d-b601-1f2c-8b20-dafdf49fcbdb, command-id=01f0f18d-b629-112d-9a45-4ae7f2720997) - Closing
[0m02:42:18.185827 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:42:18.186955 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: Close
[0m02:42:18.187261 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18d-b601-1f2c-8b20-dafdf49fcbdb) - Closing
[0m02:42:18.429986 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b97437e00>]}
[0m02:42:18.430747 [info ] [Thread-3 (]: 2 of 5 OK created sql view model customer_bi.bronze_date ....................... [[32mOK[0m in 1.96s]
[0m02:42:18.431243 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_date
[0m02:42:18.431566 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_returns
[0m02:42:18.431986 [info ] [Thread-3 (]: 3 of 5 START sql view model customer_bi.bronze_returns ......................... [RUN]
[0m02:42:18.432426 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m02:42:18.432658 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m02:42:18.432883 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_returns
[0m02:42:18.434711 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m02:42:18.435160 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_returns
[0m02:42:18.439438 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:42:18.440548 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_returns`
[0m02:42:18.441755 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m02:42:18.442282 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m02:42:18.442600 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_returns`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  )

[0m02:42:18.442895 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:42:19.237607 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18d-b748-17f8-aea0-a4913ec84dea) - Created
[0m02:42:20.150867 [debug] [Thread-3 (]: SQL status: OK in 1.710 seconds
[0m02:42:20.151966 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18d-b748-17f8-aea0-a4913ec84dea, command-id=01f0f18d-b776-161c-832f-3865358de138) - Closing
[0m02:42:20.152743 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:42:20.153438 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: Close
[0m02:42:20.153721 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18d-b748-17f8-aea0-a4913ec84dea) - Closing
[0m02:42:20.404275 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b973dfa70>]}
[0m02:42:20.405158 [info ] [Thread-3 (]: 3 of 5 OK created sql view model customer_bi.bronze_returns .................... [[32mOK[0m in 1.97s]
[0m02:42:20.405800 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_returns
[0m02:42:20.406244 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_sales
[0m02:42:20.406767 [info ] [Thread-3 (]: 4 of 5 START sql view model customer_bi.bronze_sales ........................... [RUN]
[0m02:42:20.407390 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m02:42:20.407725 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m02:42:20.408025 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_sales
[0m02:42:20.410402 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m02:42:20.410978 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_sales
[0m02:42:20.413456 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:42:20.422552 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_sales`
[0m02:42:20.423412 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m02:42:20.423936 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m02:42:20.424261 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_sales`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  )

[0m02:42:20.424535 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:42:21.260150 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18d-b89f-16da-bf28-d9005b20d1e0) - Created
[0m02:42:22.152883 [debug] [Thread-3 (]: SQL status: OK in 1.730 seconds
[0m02:42:22.154331 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18d-b89f-16da-bf28-d9005b20d1e0, command-id=01f0f18d-b8c7-124c-8cb8-f2569428e7c6) - Closing
[0m02:42:22.155462 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:42:22.156520 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: Close
[0m02:42:22.157058 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18d-b89f-16da-bf28-d9005b20d1e0) - Closing
[0m02:42:22.425030 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bbeffd8b0>]}
[0m02:42:22.425909 [info ] [Thread-3 (]: 4 of 5 OK created sql view model customer_bi.bronze_sales ...................... [[32mOK[0m in 2.02s]
[0m02:42:22.426658 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_sales
[0m02:42:22.427174 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_store
[0m02:42:22.427795 [info ] [Thread-3 (]: 5 of 5 START sql table model customer_bi.bronze_store .......................... [RUN]
[0m02:42:22.428464 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m02:42:22.428876 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m02:42:22.429222 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_store
[0m02:42:22.432860 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m02:42:22.433653 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_store
[0m02:42:22.466874 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m02:42:22.540086 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m02:42:22.542621 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m02:42:22.543449 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_store`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_store`
  
[0m02:42:22.544389 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:42:23.341373 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18d-b9f8-1a03-b5bd-a0aefea4a368) - Created
[0m02:42:28.203418 [debug] [Thread-3 (]: SQL status: OK in 5.660 seconds
[0m02:42:28.204707 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18d-b9f8-1a03-b5bd-a0aefea4a368, command-id=01f0f18d-ba20-1df7-9945-72f84752fb1f) - Closing
[0m02:42:28.217894 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:42:28.250809 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: Close
[0m02:42:28.251362 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18d-b9f8-1a03-b5bd-a0aefea4a368) - Closing
[0m02:42:28.495969 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e0fd3d8b-79a4-4c79-a512-68e2d534f782', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b9742e810>]}
[0m02:42:28.497106 [info ] [Thread-3 (]: 5 of 5 OK created sql table model customer_bi.bronze_store ..................... [[32mOK[0m in 6.07s]
[0m02:42:28.497996 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_store
[0m02:42:28.499939 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:42:28.500328 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:42:28.500874 [info ] [MainThread]: 
[0m02:42:28.501275 [info ] [MainThread]: Finished running 1 table model, 4 view models in 0 hours 0 minutes and 17.52 seconds (17.52s).
[0m02:42:28.502658 [debug] [MainThread]: Command end result
[0m02:42:28.564553 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:42:28.566848 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:42:28.584625 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m02:42:28.585129 [info ] [MainThread]: 
[0m02:42:28.585685 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:42:28.586097 [info ] [MainThread]: 
[0m02:42:28.588871 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m02:42:28.595010 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingPlusPrefixDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m02:42:28.596837 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 16.085232, "process_in_blocks": "0", "process_kernel_time": 0.36158, "process_mem_max_rss": "263776", "process_out_blocks": "3448", "process_user_time": 3.878768}
[0m02:42:28.597486 [debug] [MainThread]: Command `dbt run` succeeded at 02:42:28.597365 after 16.09 seconds
[0m02:42:28.597986 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bbe22c350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0bbdb3f110>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f0b97a37b90>]}
[0m02:42:28.598456 [debug] [MainThread]: Flushing usage events
[0m02:42:29.570527 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m02:44:18.461897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8e1374c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8e2d7c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8e030bc50>]}


============================== 02:44:18.464362 | 011a4e7c-ac60-4d54-be56-ae3fab5e0392 ==============================
[0m02:44:18.464362 [info ] [MainThread]: Running with dbt=1.11.2
[0m02:44:18.464789 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'log_format': 'default', 'debug': 'False', 'version_check': 'True', 'use_colors': 'True', 'static_parser': 'True', 'log_cache_events': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'cache_selected_only': 'False', 'printer_width': '80', 'quiet': 'False', 'empty': 'False', 'partial_parse': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'use_experimental_parser': 'False', 'fail_fast': 'False', 'invocation_command': 'dbt run', 'introspect': 'True', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'indirect_selection': 'eager', 'warn_error': 'None', 'no_print': 'None', 'send_anonymous_usage_stats': 'True'}
[0m02:44:19.216436 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m02:44:19.216864 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m02:44:19.217165 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m02:44:19.782676 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8e0ffcfc0>]}
[0m02:44:19.855165 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba8fa140>]}
[0m02:44:19.855893 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m02:44:19.939014 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:44:19.939583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba8c5e50>]}
[0m02:44:19.947659 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m02:44:20.031199 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m02:44:20.031543 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m02:44:20.031787 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m02:44:20.039967 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.dev.customer_bi
- models.dev.bronze
[0m02:44:20.066593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba3b5310>]}
[0m02:44:20.139685 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:44:20.141373 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:44:20.148023 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba9584b0>]}
[0m02:44:20.148571 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m02:44:20.148876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba4c9f30>]}
[0m02:44:20.150695 [info ] [MainThread]: 
[0m02:44:20.151034 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m02:44:20.151304 [info ] [MainThread]: 
[0m02:44:20.151768 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:44:20.152006 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:44:20.157238 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m02:44:20.157970 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m02:44:20.173461 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m02:44:20.174322 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m02:44:20.174818 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:44:20.954056 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18d-febe-1593-a585-f66131c8c518) - Created
[0m02:44:21.398995 [debug] [ThreadPool]: SQL status: OK in 1.220 seconds
[0m02:44:21.402335 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18d-febe-1593-a585-f66131c8c518, command-id=01f0f18d-fee4-1521-b8d4-0734f5773ab9) - Closing
[0m02:44:21.402867 [debug] [ThreadPool]: On list_dev: Close
[0m02:44:21.403152 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18d-febe-1593-a585-f66131c8c518) - Closing
[0m02:44:21.652701 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m02:44:21.653278 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m02:44:21.660135 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m02:44:21.660738 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m02:44:21.668340 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m02:44:22.466678 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18d-ffb8-1deb-b817-bef345bfe713) - Created
[0m02:44:23.045436 [debug] [ThreadPool]: SQL status: OK in 1.380 seconds
[0m02:44:23.055174 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f18d-ffb8-1deb-b817-bef345bfe713, command-id=01f0f18d-ffdf-1136-a722-5e92a1c1a5a0) - Closing
[0m02:44:23.056006 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m02:44:23.056416 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f18d-ffb8-1deb-b817-bef345bfe713) - Closing
[0m02:44:23.322886 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba57e390>]}
[0m02:44:23.325161 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_customer
[0m02:44:23.325660 [info ] [Thread-3 (]: 1 of 5 START sql view model customer_bi.bronze_customer ........................ [RUN]
[0m02:44:23.326194 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m02:44:23.326513 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m02:44:23.326807 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_customer
[0m02:44:23.332559 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m02:44:23.333211 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_customer
[0m02:44:23.370886 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:44:23.372817 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m02:44:23.373309 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba50d2e0>]}
[0m02:44:23.404985 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_customer`
[0m02:44:23.432179 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m02:44:23.433032 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m02:44:23.433432 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_customer`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  )

[0m02:44:23.433726 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:44:24.235034 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18e-00df-12da-9d5b-8174f789afd0) - Created
[0m02:44:25.118216 [debug] [Thread-3 (]: SQL status: OK in 1.680 seconds
[0m02:44:25.119222 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18e-00df-12da-9d5b-8174f789afd0, command-id=01f0f18e-0105-1c7a-8dc9-b89b77d438a6) - Closing
[0m02:44:25.136265 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:44:25.138240 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: Close
[0m02:44:25.138759 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18e-00df-12da-9d5b-8174f789afd0) - Closing
[0m02:44:25.406826 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8e1182530>]}
[0m02:44:25.407773 [info ] [Thread-3 (]: 1 of 5 OK created sql view model customer_bi.bronze_customer ................... [[32mOK[0m in 2.08s]
[0m02:44:25.408498 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_customer
[0m02:44:25.409016 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_date
[0m02:44:25.409645 [info ] [Thread-3 (]: 2 of 5 START sql view model customer_bi.bronze_date ............................ [RUN]
[0m02:44:25.410347 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m02:44:25.410777 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m02:44:25.411136 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_date
[0m02:44:25.420467 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m02:44:25.421376 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_date
[0m02:44:25.423618 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:44:25.425429 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_date`
[0m02:44:25.426538 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m02:44:25.427369 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m02:44:25.427845 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_date`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_date`
  )

[0m02:44:25.428247 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:44:26.251927 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18e-022b-1154-8016-61c5af327f50) - Created
[0m02:44:27.258538 [debug] [Thread-3 (]: SQL status: OK in 1.830 seconds
[0m02:44:27.259851 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18e-022b-1154-8016-61c5af327f50, command-id=01f0f18e-0258-1cf9-80aa-76744971a089) - Closing
[0m02:44:27.260810 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:44:27.261630 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: Close
[0m02:44:27.262019 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18e-022b-1154-8016-61c5af327f50) - Closing
[0m02:44:27.512270 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba4b3050>]}
[0m02:44:27.513173 [info ] [Thread-3 (]: 2 of 5 OK created sql view model customer_bi.bronze_date ....................... [[32mOK[0m in 2.10s]
[0m02:44:27.513954 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_date
[0m02:44:27.514440 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_returns
[0m02:44:27.515091 [info ] [Thread-3 (]: 3 of 5 START sql view model customer_bi.bronze_returns ......................... [RUN]
[0m02:44:27.515831 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m02:44:27.516228 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m02:44:27.516606 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_returns
[0m02:44:27.520390 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m02:44:27.543202 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_returns
[0m02:44:27.551434 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:44:27.553192 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_returns`
[0m02:44:27.554415 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m02:44:27.555097 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m02:44:27.564060 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_returns`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  )

[0m02:44:27.564683 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:44:28.358860 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18e-0392-1f00-9b68-4496c1895ca9) - Created
[0m02:44:29.204123 [debug] [Thread-3 (]: SQL status: OK in 1.640 seconds
[0m02:44:29.205156 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18e-0392-1f00-9b68-4496c1895ca9, command-id=01f0f18e-03bc-1253-8106-5a6b052fa0f3) - Closing
[0m02:44:29.205869 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:44:29.206696 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: Close
[0m02:44:29.207003 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18e-0392-1f00-9b68-4496c1895ca9) - Closing
[0m02:44:29.439504 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba21cc20>]}
[0m02:44:29.440322 [info ] [Thread-3 (]: 3 of 5 OK created sql view model customer_bi.bronze_returns .................... [[32mOK[0m in 1.92s]
[0m02:44:29.441000 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_returns
[0m02:44:29.441475 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_sales
[0m02:44:29.442075 [info ] [Thread-3 (]: 4 of 5 START sql view model customer_bi.bronze_sales ........................... [RUN]
[0m02:44:29.442689 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m02:44:29.443025 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m02:44:29.443359 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_sales
[0m02:44:29.446326 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m02:44:29.446975 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_sales
[0m02:44:29.450052 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m02:44:29.451346 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_sales`
[0m02:44:29.459714 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m02:44:29.473870 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m02:44:29.474431 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_sales`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  )

[0m02:44:29.474817 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:44:30.270663 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18e-04d7-1b1b-82fc-98dab7d92021) - Created
[0m02:44:31.129349 [debug] [Thread-3 (]: SQL status: OK in 1.650 seconds
[0m02:44:31.130505 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18e-04d7-1b1b-82fc-98dab7d92021, command-id=01f0f18e-04ff-188a-8a4d-2125298baa03) - Closing
[0m02:44:31.131219 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:44:31.131950 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: Close
[0m02:44:31.132249 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18e-04d7-1b1b-82fc-98dab7d92021) - Closing
[0m02:44:31.386903 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba87f9b0>]}
[0m02:44:31.387901 [info ] [Thread-3 (]: 4 of 5 OK created sql view model customer_bi.bronze_sales ...................... [[32mOK[0m in 1.94s]
[0m02:44:31.388588 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_sales
[0m02:44:31.389133 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_store
[0m02:44:31.389901 [info ] [Thread-3 (]: 5 of 5 START sql table model customer_bi.bronze_store .......................... [RUN]
[0m02:44:31.390733 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m02:44:31.391251 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m02:44:31.391644 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_store
[0m02:44:31.395144 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m02:44:31.395976 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_store
[0m02:44:31.440878 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m02:44:31.525269 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m02:44:31.526099 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m02:44:31.526586 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_store`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_store`
  
[0m02:44:31.526971 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m02:44:32.319407 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18e-0631-1132-9dbb-c4d0fc92f0f1) - Created
[0m02:44:34.048144 [debug] [Thread-3 (]: SQL status: OK in 2.520 seconds
[0m02:44:34.049462 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f18e-0631-1132-9dbb-c4d0fc92f0f1, command-id=01f0f18e-065f-1ade-940d-aaf785ac8eb4) - Closing
[0m02:44:34.055048 [debug] [Thread-3 (]: Applying tags to relation None
[0m02:44:34.107594 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: Close
[0m02:44:34.108300 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f18e-0631-1132-9dbb-c4d0fc92f0f1) - Closing
[0m02:44:34.372763 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '011a4e7c-ac60-4d54-be56-ae3fab5e0392', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba357590>]}
[0m02:44:34.374047 [info ] [Thread-3 (]: 5 of 5 OK created sql table model customer_bi.bronze_store ..................... [[32mOK[0m in 2.98s]
[0m02:44:34.375017 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_store
[0m02:44:34.377870 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m02:44:34.378742 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m02:44:34.379659 [info ] [MainThread]: 
[0m02:44:34.380354 [info ] [MainThread]: Finished running 1 table model, 4 view models in 0 hours 0 minutes and 14.23 seconds (14.23s).
[0m02:44:34.382455 [debug] [MainThread]: Command end result
[0m02:44:34.485947 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m02:44:34.488607 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m02:44:34.514120 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m02:44:34.514727 [info ] [MainThread]: 
[0m02:44:34.515322 [info ] [MainThread]: [32mCompleted successfully[0m
[0m02:44:34.515800 [info ] [MainThread]: 
[0m02:44:34.516312 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m02:44:34.517321 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 16.09516, "process_in_blocks": "0", "process_kernel_time": 0.508286, "process_mem_max_rss": "263424", "process_out_blocks": "3440", "process_user_time": 4.498878}
[0m02:44:34.517962 [debug] [MainThread]: Command `dbt run` succeeded at 02:44:34.517834 after 16.10 seconds
[0m02:44:34.526618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba97b0b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8e15f2450>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa8ba20bef0>]}
[0m02:44:34.527570 [debug] [MainThread]: Flushing usage events
[0m02:44:35.496738 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:00:52.963806 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf0c73cad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf0df44190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf0b6c3c50>]}


============================== 11:00:52.967151 | 751eb9d8-c8d1-4e2e-8e5d-919ca2b73119 ==============================
[0m11:00:52.967151 [info ] [MainThread]: Running with dbt=1.11.2
[0m11:00:52.967688 [debug] [MainThread]: running dbt with arguments {'empty': 'None', 'version_check': 'True', 'warn_error': 'None', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'partial_parse': 'True', 'debug': 'False', 'static_parser': 'True', 'send_anonymous_usage_stats': 'True', 'no_print': 'None', 'use_experimental_parser': 'False', 'cache_selected_only': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'quiet': 'False', 'use_colors': 'True', 'invocation_command': 'dbt ', 'target_path': 'None', 'write_json': 'True', 'printer_width': '80', 'introspect': 'True', 'fail_fast': 'False', 'profiles_dir': '/home/ubuntu/.dbt'}
[0m11:00:53.106337 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '751eb9d8-c8d1-4e2e-8e5d-919ca2b73119', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf0c3a8fc0>]}
[0m11:00:53.121798 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m11:00:53.122854 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m11:00:53.123480 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.21571214, "process_in_blocks": "392", "process_kernel_time": 0.149293, "process_mem_max_rss": "103124", "process_out_blocks": "8", "process_user_time": 1.81285}
[0m11:00:53.123855 [debug] [MainThread]: Command `cli deps` succeeded at 11:00:53.123760 after 0.22 seconds
[0m11:00:53.124163 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf0b4708d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf0b470f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbf0b90fd50>]}
[0m11:00:53.124553 [debug] [MainThread]: Flushing usage events
[0m11:00:53.968251 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:09:35.154801 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f836fb90c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f83713b0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f836eb27c50>]}


============================== 11:09:35.157734 | 01b6a437-f78a-43cb-9972-2a4788a97e51 ==============================
[0m11:09:35.157734 [info ] [MainThread]: Running with dbt=1.11.2
[0m11:09:35.158311 [debug] [MainThread]: running dbt with arguments {'cache_selected_only': 'False', 'fail_fast': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'warn_error': 'None', 'printer_width': '80', 'invocation_command': 'dbt run', 'target_path': 'None', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'static_parser': 'True', 'version_check': 'True', 'no_print': 'None', 'empty': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'quiet': 'False', 'introspect': 'True', 'debug': 'False', 'write_json': 'True', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'indirect_selection': 'eager'}
[0m11:09:35.809156 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:09:35.809592 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:09:35.810031 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:09:36.270390 [warn ] [MainThread]: [[33mWARNING[0m][MissingPlusPrefixDeprecation]: Deprecated functionality
Missing '+' prefix on `customer_bi` found at `dev.customer_bi` in file `/home/ub
untu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/dbt_project.yml`.
Hierarchical config values without a '+' prefix are deprecated in
dbt_project.yml.
[0m11:09:36.270862 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834d24fce0>]}
[0m11:09:36.350381 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834d126360>]}
[0m11:09:36.391717 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834d0f1950>]}
[0m11:09:36.392297 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m11:09:36.462380 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m11:09:36.463166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834d1198b0>]}
[0m11:09:36.471437 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m11:09:36.555612 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:09:36.555960 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:09:36.556187 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:09:36.564286 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 2 unused configuration paths:
- models.dev
- models.dev.bronze
[0m11:09:36.591971 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834d03c830>]}
[0m11:09:36.660718 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m11:09:36.662696 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m11:09:36.668643 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834cbb75f0>]}
[0m11:09:36.669034 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m11:09:36.669333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834cc50d10>]}
[0m11:09:36.670814 [info ] [MainThread]: 
[0m11:09:36.671199 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:09:36.671488 [info ] [MainThread]: 
[0m11:09:36.672083 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m11:09:36.672335 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:09:36.677819 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m11:09:36.678324 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m11:09:36.691502 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m11:09:36.691855 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m11:09:36.692084 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:09:38.117915 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d4-94ef-1aef-9e26-119970e00257) - Created
[0m11:09:48.241596 [debug] [ThreadPool]: SQL status: OK in 11.550 seconds
[0m11:09:48.253653 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f1d4-94ef-1aef-9e26-119970e00257, command-id=01f0f1d4-9532-19e4-98e4-2f0579af632b) - Closing
[0m11:09:48.619604 [debug] [ThreadPool]: On list_dev: Close
[0m11:09:48.620073 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d4-94ef-1aef-9e26-119970e00257) - Closing
[0m11:09:48.962332 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m11:09:48.962715 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m11:09:48.967213 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m11:09:48.967597 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m11:09:48.967847 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:09:49.925183 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d4-9c50-1fec-b6f1-66e05d70562d) - Created
[0m11:09:51.961920 [debug] [ThreadPool]: SQL status: OK in 2.990 seconds
[0m11:09:51.964293 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f1d4-9c50-1fec-b6f1-66e05d70562d, command-id=01f0f1d4-9c82-113a-b7e7-4876dce0c1b4) - Closing
[0m11:09:51.964956 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m11:09:51.965212 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d4-9c50-1fec-b6f1-66e05d70562d) - Closing
[0m11:09:52.275726 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834cd692e0>]}
[0m11:09:52.282493 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_customer
[0m11:09:52.283047 [info ] [Thread-3 (]: 1 of 5 START sql view model customer_bi.bronze_customer ........................ [RUN]
[0m11:09:52.283606 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m11:09:52.283888 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m11:09:52.284189 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_customer
[0m11:09:52.289595 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m11:09:52.290563 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_customer
[0m11:09:52.307588 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m11:09:52.309790 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m11:09:52.310350 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834c9c0230>]}
[0m11:09:52.320609 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_customer`
[0m11:09:52.331057 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m11:09:52.331638 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m11:09:52.332058 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_customer`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  )

[0m11:09:52.332328 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:09:53.228012 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d4-9e5e-1763-b264-b15bd9ee04ba) - Created
[0m11:09:57.092738 [debug] [Thread-3 (]: SQL status: OK in 4.760 seconds
[0m11:09:57.094279 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f1d4-9e5e-1763-b264-b15bd9ee04ba, command-id=01f0f1d4-9e8d-18d1-8339-56db5bd23462) - Closing
[0m11:09:57.104594 [debug] [Thread-3 (]: Applying tags to relation None
[0m11:09:57.106737 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: Close
[0m11:09:57.107108 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d4-9e5e-1763-b264-b15bd9ee04ba) - Closing
[0m11:09:57.394085 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f836f99b9d0>]}
[0m11:09:57.394929 [info ] [Thread-3 (]: 1 of 5 OK created sql view model customer_bi.bronze_customer ................... [[32mOK[0m in 5.10s]
[0m11:09:57.395631 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_customer
[0m11:09:57.396027 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_date
[0m11:09:57.396639 [info ] [Thread-3 (]: 2 of 5 START sql view model customer_bi.bronze_date ............................ [RUN]
[0m11:09:57.397198 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m11:09:57.397457 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m11:09:57.397727 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_date
[0m11:09:57.400472 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m11:09:57.401357 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_date
[0m11:09:57.403531 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m11:09:57.405126 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_date`
[0m11:09:57.406386 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m11:09:57.406904 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m11:09:57.407252 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_date`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_date`
  )

[0m11:09:57.407524 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:09:58.265946 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d4-a182-1932-82ba-7448022d3eb6) - Created
[0m11:09:59.498396 [debug] [Thread-3 (]: SQL status: OK in 2.090 seconds
[0m11:09:59.499418 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f1d4-a182-1932-82ba-7448022d3eb6, command-id=01f0f1d4-a1a9-1974-9f5a-d68440462895) - Closing
[0m11:09:59.500090 [debug] [Thread-3 (]: Applying tags to relation None
[0m11:09:59.500883 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: Close
[0m11:09:59.501166 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d4-a182-1932-82ba-7448022d3eb6) - Closing
[0m11:10:01.069150 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834c9bbe00>]}
[0m11:10:01.070554 [info ] [Thread-3 (]: 2 of 5 OK created sql view model customer_bi.bronze_date ....................... [[32mOK[0m in 3.67s]
[0m11:10:01.071832 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_date
[0m11:10:01.072787 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_returns
[0m11:10:01.073645 [info ] [Thread-3 (]: 3 of 5 START sql view model customer_bi.bronze_returns ......................... [RUN]
[0m11:10:01.074780 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m11:10:01.075580 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m11:10:01.076505 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_returns
[0m11:10:01.079640 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m11:10:01.080418 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_returns
[0m11:10:01.088818 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m11:10:01.089879 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_returns`
[0m11:10:01.090544 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m11:10:01.090987 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m11:10:01.091280 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_returns`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  )

[0m11:10:01.091522 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:10:01.999036 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d4-a300-158e-bc52-eff71aabeecc) - Created
[0m11:10:03.107688 [debug] [Thread-3 (]: SQL status: OK in 2.020 seconds
[0m11:10:03.108680 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f1d4-a300-158e-bc52-eff71aabeecc, command-id=01f0f1d4-a32f-1bf4-b34f-47d5b45898f8) - Closing
[0m11:10:03.109249 [debug] [Thread-3 (]: Applying tags to relation None
[0m11:10:03.109843 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: Close
[0m11:10:03.110101 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d4-a300-158e-bc52-eff71aabeecc) - Closing
[0m11:10:03.378030 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834cb32810>]}
[0m11:10:03.378848 [info ] [Thread-3 (]: 3 of 5 OK created sql view model customer_bi.bronze_returns .................... [[32mOK[0m in 2.30s]
[0m11:10:03.379478 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_returns
[0m11:10:03.379799 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_sales
[0m11:10:03.380146 [info ] [Thread-3 (]: 4 of 5 START sql view model customer_bi.bronze_sales ........................... [RUN]
[0m11:10:03.380631 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m11:10:03.380877 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m11:10:03.381136 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_sales
[0m11:10:03.382982 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m11:10:03.383698 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_sales
[0m11:10:03.385536 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m11:10:03.386341 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_sales`
[0m11:10:03.387197 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m11:10:03.387622 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m11:10:03.388006 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_sales`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  )

[0m11:10:03.388294 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:10:04.352875 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d4-a46b-18b2-8b00-a08cc86a6f24) - Created
[0m11:10:05.410292 [debug] [Thread-3 (]: SQL status: OK in 2.020 seconds
[0m11:10:05.411131 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f1d4-a46b-18b2-8b00-a08cc86a6f24, command-id=01f0f1d4-a4a5-16c8-8d24-e9e1ff29ba11) - Closing
[0m11:10:05.411698 [debug] [Thread-3 (]: Applying tags to relation None
[0m11:10:05.412374 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: Close
[0m11:10:05.412655 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d4-a46b-18b2-8b00-a08cc86a6f24) - Closing
[0m11:10:05.728524 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834cb32810>]}
[0m11:10:05.730070 [info ] [Thread-3 (]: 4 of 5 OK created sql view model customer_bi.bronze_sales ...................... [[32mOK[0m in 2.35s]
[0m11:10:05.731394 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_sales
[0m11:10:05.732047 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_store
[0m11:10:05.733033 [info ] [Thread-3 (]: 5 of 5 START sql table model customer_bi.bronze_store .......................... [RUN]
[0m11:10:05.733971 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m11:10:05.734539 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m11:10:05.734870 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_store
[0m11:10:05.737419 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m11:10:05.738094 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_store
[0m11:10:05.753325 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m11:10:05.774530 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m11:10:05.775138 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m11:10:05.775441 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_store`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_store`
  
[0m11:10:05.775687 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:10:06.731869 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d4-a5ec-14ce-80cf-c029fee6b311) - Created
[0m11:10:10.910480 [debug] [Thread-3 (]: SQL status: OK in 5.130 seconds
[0m11:10:10.911927 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f1d4-a5ec-14ce-80cf-c029fee6b311, command-id=01f0f1d4-a61e-17f6-b6e5-c26b81c09c31) - Closing
[0m11:10:10.915540 [debug] [Thread-3 (]: Applying tags to relation None
[0m11:10:10.925393 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: Close
[0m11:10:10.925683 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d4-a5ec-14ce-80cf-c029fee6b311) - Closing
[0m11:10:11.203330 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '01b6a437-f78a-43cb-9972-2a4788a97e51', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f836f2399d0>]}
[0m11:10:11.204540 [info ] [Thread-3 (]: 5 of 5 OK created sql table model customer_bi.bronze_store ..................... [[32mOK[0m in 5.47s]
[0m11:10:11.205499 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_store
[0m11:10:11.208188 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m11:10:11.208567 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:10:11.209049 [info ] [MainThread]: 
[0m11:10:11.209424 [info ] [MainThread]: Finished running 1 table model, 4 view models in 0 hours 0 minutes and 34.54 seconds (34.54s).
[0m11:10:11.210467 [debug] [MainThread]: Command end result
[0m11:10:11.239312 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m11:10:11.241577 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m11:10:11.246344 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m11:10:11.246672 [info ] [MainThread]: 
[0m11:10:11.247028 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:10:11.247422 [info ] [MainThread]: 
[0m11:10:11.248269 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m11:10:11.249257 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingPlusPrefixDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m11:10:11.250003 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 34.86241, "process_in_blocks": "6304", "process_kernel_time": 0.461565, "process_mem_max_rss": "265396", "process_out_blocks": "3448", "process_user_time": 3.630822}
[0m11:10:11.250410 [debug] [MainThread]: Command `dbt run` succeeded at 11:10:11.250316 after 34.86 seconds
[0m11:10:11.250799 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834d04aff0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f836fe04350>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f834cceba10>]}
[0m11:10:11.251078 [debug] [MainThread]: Flushing usage events
[0m11:10:12.253634 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:15:14.743264 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87ccad8c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87ce2fc190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87cba6bc50>]}


============================== 11:15:14.747882 | 726f1caa-0078-4093-9054-53bcb1d6f65a ==============================
[0m11:15:14.747882 [info ] [MainThread]: Running with dbt=1.11.2
[0m11:15:14.748359 [debug] [MainThread]: running dbt with arguments {'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'version_check': 'True', 'use_colors': 'True', 'target_path': 'None', 'warn_error': 'None', 'write_json': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'introspect': 'True', 'debug': 'False', 'no_print': 'None', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'fail_fast': 'False', 'log_cache_events': 'False', 'printer_width': '80', 'quiet': 'False', 'invocation_command': 'dbt run', 'static_parser': 'True', 'cache_selected_only': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'empty': 'False', 'send_anonymous_usage_stats': 'True'}
[0m11:15:15.425892 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:15:15.426443 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:15:15.426993 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:15:16.102512 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87cc760fc0>]}
[0m11:15:16.142101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a6072140>]}
[0m11:15:16.142678 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m11:15:16.201579 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m11:15:16.202111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a603de50>]}
[0m11:15:16.209508 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m11:15:16.288264 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:15:16.288772 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:15:16.289016 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:15:16.297001 [warn ] [MainThread]: [[33mWARNING[0m]: Configuration paths exist in your dbt_project.yml file which do not apply to any resources.
There are 1 unused configuration paths:
- models.customer_bi.dev.bronze
[0m11:15:16.326583 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a5b2d310>]}
[0m11:15:16.393484 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m11:15:16.395512 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m11:15:16.404404 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a60d04b0>]}
[0m11:15:16.404760 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m11:15:16.405038 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a5afdf30>]}
[0m11:15:16.406430 [info ] [MainThread]: 
[0m11:15:16.406719 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:15:16.406948 [info ] [MainThread]: 
[0m11:15:16.407352 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m11:15:16.407578 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:15:16.412476 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m11:15:16.412979 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m11:15:16.423215 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m11:15:16.423584 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m11:15:16.423818 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:15:17.327745 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d5-5f5d-1d0f-835e-ee0599173f0f) - Created
[0m11:15:18.003419 [debug] [ThreadPool]: SQL status: OK in 1.580 seconds
[0m11:15:18.012377 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f1d5-5f5d-1d0f-835e-ee0599173f0f, command-id=01f0f1d5-5f8d-1688-a29b-8561d96d97d0) - Closing
[0m11:15:18.012906 [debug] [ThreadPool]: On list_dev: Close
[0m11:15:18.013185 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d5-5f5d-1d0f-835e-ee0599173f0f) - Closing
[0m11:15:18.289667 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m11:15:18.290460 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m11:15:18.296846 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m11:15:18.297260 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m11:15:18.297491 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:15:19.147880 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d5-607c-1d8e-a52d-271b195f13bc) - Created
[0m11:15:19.782548 [debug] [ThreadPool]: SQL status: OK in 1.480 seconds
[0m11:15:19.785986 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f1d5-607c-1d8e-a52d-271b195f13bc, command-id=01f0f1d5-60ab-10d3-8286-63190a9a1f10) - Closing
[0m11:15:19.786550 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m11:15:19.786819 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d5-607c-1d8e-a52d-271b195f13bc) - Closing
[0m11:15:20.080285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a5b7a990>]}
[0m11:15:20.091434 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_customer
[0m11:15:20.091967 [info ] [Thread-3 (]: 1 of 5 START sql view model customer_bi.bronze_customer ........................ [RUN]
[0m11:15:20.092505 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m11:15:20.092770 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m11:15:20.093048 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_customer
[0m11:15:20.098052 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m11:15:20.098703 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_customer
[0m11:15:20.113908 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m11:15:20.116592 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m11:15:20.117130 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a5c452e0>]}
[0m11:15:20.127122 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_customer`
[0m11:15:20.136656 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m11:15:20.137477 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m11:15:20.138067 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_customer`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  )

[0m11:15:20.138559 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:15:21.010792 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d5-61ac-19df-8fac-8d21bd16d3e8) - Created
[0m11:15:21.992517 [debug] [Thread-3 (]: SQL status: OK in 1.850 seconds
[0m11:15:21.994230 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f1d5-61ac-19df-8fac-8d21bd16d3e8, command-id=01f0f1d5-61d6-18a1-b26a-6e708a38c4b9) - Closing
[0m11:15:22.002975 [debug] [Thread-3 (]: Applying tags to relation None
[0m11:15:22.004282 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: Close
[0m11:15:22.004562 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d5-61ac-19df-8fac-8d21bd16d3e8) - Closing
[0m11:15:22.332521 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87cc8ea530>]}
[0m11:15:22.333669 [info ] [Thread-3 (]: 1 of 5 OK created sql view model customer_bi.bronze_customer ................... [[32mOK[0m in 2.23s]
[0m11:15:22.334653 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_customer
[0m11:15:22.335329 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_date
[0m11:15:22.336080 [info ] [Thread-3 (]: 2 of 5 START sql view model customer_bi.bronze_date ............................ [RUN]
[0m11:15:22.337207 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m11:15:22.337722 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m11:15:22.338194 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_date
[0m11:15:22.340559 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m11:15:22.341065 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_date
[0m11:15:22.342714 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m11:15:22.343900 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_date`
[0m11:15:22.344493 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m11:15:22.344826 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m11:15:22.345116 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_date`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_date`
  )

[0m11:15:22.345475 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:15:23.234386 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d5-6310-1ec8-8537-e19837fe589d) - Created
[0m11:15:24.220669 [debug] [Thread-3 (]: SQL status: OK in 1.870 seconds
[0m11:15:24.222800 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f1d5-6310-1ec8-8537-e19837fe589d, command-id=01f0f1d5-633c-1d2f-9f4a-97dec96b6bb6) - Closing
[0m11:15:24.223908 [debug] [Thread-3 (]: Applying tags to relation None
[0m11:15:24.225070 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: Close
[0m11:15:24.225569 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d5-6310-1ec8-8537-e19837fe589d) - Closing
[0m11:15:24.516536 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a5c88550>]}
[0m11:15:24.518140 [info ] [Thread-3 (]: 2 of 5 OK created sql view model customer_bi.bronze_date ....................... [[32mOK[0m in 2.18s]
[0m11:15:24.519468 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_date
[0m11:15:24.520098 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_returns
[0m11:15:24.520792 [info ] [Thread-3 (]: 3 of 5 START sql view model customer_bi.bronze_returns ......................... [RUN]
[0m11:15:24.521700 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m11:15:24.522321 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m11:15:24.522632 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_returns
[0m11:15:24.524889 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m11:15:24.525563 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_returns
[0m11:15:24.529977 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m11:15:24.530913 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_returns`
[0m11:15:24.531532 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m11:15:24.531916 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m11:15:24.532254 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_returns`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  )

[0m11:15:24.532577 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:15:25.438780 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d5-646a-131c-80c4-c93cbd9ee9e1) - Created
[0m11:15:26.393034 [debug] [Thread-3 (]: SQL status: OK in 1.860 seconds
[0m11:15:26.394765 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f1d5-646a-131c-80c4-c93cbd9ee9e1, command-id=01f0f1d5-649e-1d38-943b-4a9d1effe138) - Closing
[0m11:15:26.396061 [debug] [Thread-3 (]: Applying tags to relation None
[0m11:15:26.396951 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: Close
[0m11:15:26.397304 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d5-646a-131c-80c4-c93cbd9ee9e1) - Closing
[0m11:15:26.708861 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a5994750>]}
[0m11:15:26.709457 [info ] [Thread-3 (]: 3 of 5 OK created sql view model customer_bi.bronze_returns .................... [[32mOK[0m in 2.19s]
[0m11:15:26.709938 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_returns
[0m11:15:26.710257 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_sales
[0m11:15:26.710644 [info ] [Thread-3 (]: 4 of 5 START sql view model customer_bi.bronze_sales ........................... [RUN]
[0m11:15:26.711044 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m11:15:26.711290 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m11:15:26.711519 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_sales
[0m11:15:26.713385 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m11:15:26.713783 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_sales
[0m11:15:26.715564 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m11:15:26.716327 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi`.`bronze_sales`
[0m11:15:26.716895 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m11:15:26.717264 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m11:15:26.717528 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
  
  create or replace view `dev`.`customer_bi`.`bronze_sales`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  )

[0m11:15:26.717782 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:15:27.572987 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d5-65cd-1680-8d32-015b17dd17ce) - Created
[0m11:15:28.540897 [debug] [Thread-3 (]: SQL status: OK in 1.820 seconds
[0m11:15:28.541818 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f1d5-65cd-1680-8d32-015b17dd17ce, command-id=01f0f1d5-65f5-19f9-9678-f8c6cd5a26c1) - Closing
[0m11:15:28.542406 [debug] [Thread-3 (]: Applying tags to relation None
[0m11:15:28.543028 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: Close
[0m11:15:28.543293 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d5-65cd-1680-8d32-015b17dd17ce) - Closing
[0m11:15:28.836256 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a60a7b90>]}
[0m11:15:28.837774 [info ] [Thread-3 (]: 4 of 5 OK created sql view model customer_bi.bronze_sales ...................... [[32mOK[0m in 2.13s]
[0m11:15:28.838899 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_sales
[0m11:15:28.839592 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_store
[0m11:15:28.839926 [info ] [Thread-3 (]: 5 of 5 START sql table model customer_bi.bronze_store .......................... [RUN]
[0m11:15:28.840485 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m11:15:28.840718 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m11:15:28.840941 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_store
[0m11:15:28.843006 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m11:15:28.843437 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_store
[0m11:15:28.856593 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m11:15:28.878256 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m11:15:28.878772 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m11:15:28.879133 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_store`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_store`
  
[0m11:15:28.879407 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m11:15:29.754837 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d5-672b-15f2-b159-544c469704f8) - Created
[0m11:15:32.054039 [debug] [Thread-3 (]: SQL status: OK in 3.170 seconds
[0m11:15:32.054936 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f1d5-672b-15f2-b159-544c469704f8, command-id=01f0f1d5-6759-1ed1-ba25-4f59f6851bc5) - Closing
[0m11:15:32.057926 [debug] [Thread-3 (]: Applying tags to relation None
[0m11:15:32.069697 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: Close
[0m11:15:32.070156 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f1d5-672b-15f2-b159-544c469704f8) - Closing
[0m11:15:32.363541 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '726f1caa-0078-4093-9054-53bcb1d6f65a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a5acf2f0>]}
[0m11:15:32.364538 [info ] [Thread-3 (]: 5 of 5 OK created sql table model customer_bi.bronze_store ..................... [[32mOK[0m in 3.52s]
[0m11:15:32.365348 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_store
[0m11:15:32.367404 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m11:15:32.367714 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:15:32.368160 [info ] [MainThread]: 
[0m11:15:32.368503 [info ] [MainThread]: Finished running 1 table model, 4 view models in 0 hours 0 minutes and 15.96 seconds (15.96s).
[0m11:15:32.369512 [debug] [MainThread]: Command end result
[0m11:15:32.407937 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m11:15:32.410898 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m11:15:32.415735 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m11:15:32.416071 [info ] [MainThread]: 
[0m11:15:32.416437 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:15:32.416741 [info ] [MainThread]: 
[0m11:15:32.417032 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m11:15:32.417743 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 17.714321, "process_in_blocks": "127112", "process_kernel_time": 0.247982, "process_mem_max_rss": "266948", "process_out_blocks": "3440", "process_user_time": 3.343227}
[0m11:15:32.418262 [debug] [MainThread]: Command `dbt run` succeeded at 11:15:32.418146 after 17.71 seconds
[0m11:15:32.418728 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a5c133b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87cc12e750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f87a59bbe90>]}
[0m11:15:32.419225 [debug] [MainThread]: Flushing usage events
[0m11:15:33.513709 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:18:06.510543 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8440e10c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f84426a8190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f843fdabc50>]}


============================== 11:18:06.512792 | 6521b213-82cb-47ab-b07b-eb5b2c17b9a4 ==============================
[0m11:18:06.512792 [info ] [MainThread]: Running with dbt=1.11.2
[0m11:18:06.513276 [debug] [MainThread]: running dbt with arguments {'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'use_colors': 'True', 'quiet': 'False', 'version_check': 'True', 'cache_selected_only': 'False', 'debug': 'False', 'use_experimental_parser': 'False', 'warn_error': 'None', 'empty': 'False', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'partial_parse': 'True', 'static_parser': 'True', 'fail_fast': 'False', 'printer_width': '80', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'write_json': 'True', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'target_path': 'None', 'invocation_command': 'dbt run', 'profiles_dir': '/home/ubuntu/.dbt', 'introspect': 'True', 'log_format': 'default'}
[0m11:18:07.137101 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:18:07.137529 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:18:07.138058 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:18:07.623399 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8440a9cfc0>]}
[0m11:18:07.668873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f841a3be140>]}
[0m11:18:07.669505 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m11:18:07.724647 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m11:18:07.725141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f841a585e50>]}
[0m11:18:07.732525 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m11:18:07.809891 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:18:07.810235 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:18:07.810455 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:18:07.840811 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f841a075310>]}
[0m11:18:07.908081 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m11:18:07.909816 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m11:18:07.915555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f841a41c4b0>]}
[0m11:18:07.916007 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m11:18:07.916418 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8419fbdf30>]}
[0m11:18:07.917973 [info ] [MainThread]: 
[0m11:18:07.918296 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:18:07.918553 [info ] [MainThread]: 
[0m11:18:07.919060 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m11:18:07.919338 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:18:07.924606 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m11:18:07.925104 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m11:18:07.936365 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m11:18:07.936791 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m11:18:07.937069 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:18:08.834719 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d5-c59d-102c-84e9-606fb0cd2fb1) - Created
[0m11:18:09.327108 [debug] [ThreadPool]: SQL status: OK in 1.390 seconds
[0m11:18:09.329810 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f1d5-c59d-102c-84e9-606fb0cd2fb1, command-id=01f0f1d5-c5cf-1b4e-8b1d-611a8d186115) - Closing
[0m11:18:09.330188 [debug] [ThreadPool]: On list_dev: Close
[0m11:18:09.330430 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d5-c59d-102c-84e9-606fb0cd2fb1) - Closing
[0m11:18:09.615605 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_dev_customer_bi) - Creating connection
[0m11:18:09.616074 [debug] [ThreadPool]: Acquiring new databricks connection 'create_dev_customer_bi'
[0m11:18:09.618429 [debug] [ThreadPool]: Creating schema "database: "dev"
schema: "customer_bi"
"
[0m11:18:09.623386 [debug] [ThreadPool]: Using databricks connection "create_dev_customer_bi"
[0m11:18:09.623724 [debug] [ThreadPool]: On create_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "create_dev_customer_bi"} */
create schema if not exists `dev`.`customer_bi`
  
[0m11:18:09.623988 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:18:10.511179 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d5-c6a7-1b0d-b1cd-71bf00884f4e) - Created
[0m11:18:11.256322 [debug] [ThreadPool]: SQL status: OK in 1.630 seconds
[0m11:18:11.257762 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f1d5-c6a7-1b0d-b1cd-71bf00884f4e, command-id=01f0f1d5-c6da-19f3-86ed-a763ed2adc5d) - Closing
[0m11:18:11.258462 [debug] [ThreadPool]: On create_dev_customer_bi: Close
[0m11:18:11.258831 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d5-c6a7-1b0d-b1cd-71bf00884f4e) - Closing
[0m11:18:11.539622 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi) - Creating connection
[0m11:18:11.540556 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi'
[0m11:18:11.545139 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi"
[0m11:18:11.545395 [debug] [ThreadPool]: On list_dev_customer_bi: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi'

  
[0m11:18:11.545618 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:18:12.456466 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d5-c7e4-16ad-a1a1-9eb0f7f48627) - Created
[0m11:18:13.047542 [debug] [ThreadPool]: SQL status: OK in 1.500 seconds
[0m11:18:13.051497 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f1d5-c7e4-16ad-a1a1-9eb0f7f48627, command-id=01f0f1d5-c810-1cb1-9350-d5cb5b1a1b4b) - Closing
[0m11:18:13.051922 [debug] [ThreadPool]: On list_dev_customer_bi: Close
[0m11:18:13.052235 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f1d5-c7e4-16ad-a1a1-9eb0f7f48627) - Closing
[0m11:18:13.332427 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f841a072510>]}
[0m11:18:13.335831 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_customer
[0m11:18:13.336427 [info ] [Thread-4 (]: 1 of 5 START sql table model customer_bi.bronze_customer ....................... [RUN]
[0m11:18:13.337064 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m11:18:13.337467 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m11:18:13.337828 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_customer
[0m11:18:13.344527 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m11:18:13.345173 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_customer
[0m11:18:13.359176 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m11:18:13.359713 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m11:18:13.360084 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f841a08da70>]}
[0m11:18:13.392320 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m11:18:13.392883 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m11:18:13.393212 [debug] [Thread-4 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_customer`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  
[0m11:18:13.393435 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:18:14.276302 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f1d5-c908-1932-b79a-56a9740681a9) - Created
[0m11:18:17.174733 [debug] [Thread-4 (]: SQL status: OK in 3.780 seconds
[0m11:18:17.175965 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f1d5-c908-1932-b79a-56a9740681a9, command-id=01f0f1d5-c92a-1304-ac81-af1e910152ff) - Closing
[0m11:18:17.183333 [debug] [Thread-4 (]: Applying tags to relation None
[0m11:18:17.193282 [debug] [Thread-4 (]: On model.customer_bi.bronze_customer: Close
[0m11:18:17.193561 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f1d5-c908-1932-b79a-56a9740681a9) - Closing
[0m11:18:17.482317 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8440c22530>]}
[0m11:18:17.482997 [info ] [Thread-4 (]: 1 of 5 OK created sql table model customer_bi.bronze_customer .................. [[32mOK[0m in 4.14s]
[0m11:18:17.483460 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_customer
[0m11:18:17.483811 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_date
[0m11:18:17.484182 [info ] [Thread-4 (]: 2 of 5 START sql table model customer_bi.bronze_date ........................... [RUN]
[0m11:18:17.484724 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m11:18:17.485012 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m11:18:17.485274 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_date
[0m11:18:17.487477 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m11:18:17.488223 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_date
[0m11:18:17.492796 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m11:18:17.494618 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m11:18:17.495133 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m11:18:17.495511 [debug] [Thread-4 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_date`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_date`
  
[0m11:18:17.495840 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:18:18.384977 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f1d5-cb8f-1682-a4c7-55f1fd6c69d3) - Created
[0m11:18:20.603528 [debug] [Thread-4 (]: SQL status: OK in 3.110 seconds
[0m11:18:20.604997 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f1d5-cb8f-1682-a4c7-55f1fd6c69d3, command-id=01f0f1d5-cbb7-1014-b95f-f2f9d77cdb2e) - Closing
[0m11:18:20.605951 [debug] [Thread-4 (]: Applying tags to relation None
[0m11:18:20.607188 [debug] [Thread-4 (]: On model.customer_bi.bronze_date: Close
[0m11:18:20.607552 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f1d5-cb8f-1682-a4c7-55f1fd6c69d3) - Closing
[0m11:18:20.893271 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f843fdb6650>]}
[0m11:18:20.894838 [info ] [Thread-4 (]: 2 of 5 OK created sql table model customer_bi.bronze_date ...................... [[32mOK[0m in 3.41s]
[0m11:18:20.895897 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_date
[0m11:18:20.896520 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_returns
[0m11:18:20.897234 [info ] [Thread-4 (]: 3 of 5 START sql table model customer_bi.bronze_returns ........................ [RUN]
[0m11:18:20.898175 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m11:18:20.898673 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m11:18:20.898957 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_returns
[0m11:18:20.901091 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m11:18:20.902237 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_returns
[0m11:18:20.904657 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m11:18:20.906234 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m11:18:20.906658 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m11:18:20.906994 [debug] [Thread-4 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_returns`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  
[0m11:18:20.907262 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:18:21.804856 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f1d5-cdb2-119c-8982-3d25a173411a) - Created
[0m11:18:25.281615 [debug] [Thread-4 (]: SQL status: OK in 4.370 seconds
[0m11:18:25.282591 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f1d5-cdb2-119c-8982-3d25a173411a, command-id=01f0f1d5-cdd5-1f0e-84ee-9ae07183bf27) - Closing
[0m11:18:25.283367 [debug] [Thread-4 (]: Applying tags to relation None
[0m11:18:25.284737 [debug] [Thread-4 (]: On model.customer_bi.bronze_returns: Close
[0m11:18:25.285035 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f1d5-cdb2-119c-8982-3d25a173411a) - Closing
[0m11:18:25.566718 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f84184f1be0>]}
[0m11:18:25.567705 [info ] [Thread-4 (]: 3 of 5 OK created sql table model customer_bi.bronze_returns ................... [[32mOK[0m in 4.67s]
[0m11:18:25.568488 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_returns
[0m11:18:25.569087 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_sales
[0m11:18:25.569710 [info ] [Thread-4 (]: 4 of 5 START sql table model customer_bi.bronze_sales .......................... [RUN]
[0m11:18:25.570872 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m11:18:25.571186 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m11:18:25.571420 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_sales
[0m11:18:25.573913 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m11:18:25.574346 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_sales
[0m11:18:25.576762 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m11:18:25.578356 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m11:18:25.578875 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m11:18:25.579220 [debug] [Thread-4 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_sales`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  
[0m11:18:25.579476 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:18:26.448832 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f1d5-cfb5-1bf2-9f72-65f97db0363c) - Created
[0m11:18:28.771612 [debug] [Thread-4 (]: SQL status: OK in 3.190 seconds
[0m11:18:28.772896 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f1d5-cfb5-1bf2-9f72-65f97db0363c, command-id=01f0f1d5-cfe6-11cd-998c-64a4d2f7e7ff) - Closing
[0m11:18:28.773709 [debug] [Thread-4 (]: Applying tags to relation None
[0m11:18:28.774630 [debug] [Thread-4 (]: On model.customer_bi.bronze_sales: Close
[0m11:18:28.774889 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f1d5-cfb5-1bf2-9f72-65f97db0363c) - Closing
[0m11:18:29.047149 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8419e321b0>]}
[0m11:18:29.047814 [info ] [Thread-4 (]: 4 of 5 OK created sql table model customer_bi.bronze_sales ..................... [[32mOK[0m in 3.48s]
[0m11:18:29.048341 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_sales
[0m11:18:29.048704 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_store
[0m11:18:29.049064 [info ] [Thread-4 (]: 5 of 5 START sql table model customer_bi.bronze_store .......................... [RUN]
[0m11:18:29.049590 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m11:18:29.049913 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m11:18:29.050354 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_store
[0m11:18:29.052907 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m11:18:29.053363 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_store
[0m11:18:29.054801 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m11:18:29.056349 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m11:18:29.056905 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m11:18:29.057320 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
    
        create or replace table `dev`.`customer_bi`.`bronze_store`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_store`
  
[0m11:18:29.057617 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:18:29.943804 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f1d5-d1e7-16d7-bcd0-2279d232cc7d) - Created
[0m11:18:31.940109 [debug] [Thread-4 (]: SQL status: OK in 2.880 seconds
[0m11:18:31.940979 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f1d5-d1e7-16d7-bcd0-2279d232cc7d, command-id=01f0f1d5-d20a-1592-8c27-729c8f0dd72e) - Closing
[0m11:18:31.941560 [debug] [Thread-4 (]: Applying tags to relation None
[0m11:18:31.942479 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: Close
[0m11:18:31.942713 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f1d5-d1e7-16d7-bcd0-2279d232cc7d) - Closing
[0m11:18:32.217130 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6521b213-82cb-47ab-b07b-eb5b2c17b9a4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8419e33290>]}
[0m11:18:32.217877 [info ] [Thread-4 (]: 5 of 5 OK created sql table model customer_bi.bronze_store ..................... [[32mOK[0m in 3.17s]
[0m11:18:32.218531 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_store
[0m11:18:32.220334 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m11:18:32.220651 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:18:32.221065 [info ] [MainThread]: 
[0m11:18:32.221319 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 24.30 seconds (24.30s).
[0m11:18:32.222124 [debug] [MainThread]: Command end result
[0m11:18:32.254733 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m11:18:32.256339 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m11:18:32.260964 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m11:18:32.261281 [info ] [MainThread]: 
[0m11:18:32.261666 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:18:32.261922 [info ] [MainThread]: 
[0m11:18:32.262222 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m11:18:32.262861 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 24.435093, "process_in_blocks": "32", "process_kernel_time": 0.190989, "process_mem_max_rss": "262736", "process_out_blocks": "3448", "process_user_time": 3.328838}
[0m11:18:32.263247 [debug] [MainThread]: Command `dbt run` succeeded at 11:18:32.263168 after 24.44 seconds
[0m11:18:32.263589 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8419dcb050>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8441094c50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f841a340bf0>]}
[0m11:18:32.263914 [debug] [MainThread]: Flushing usage events
[0m11:18:33.209300 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:24:59.192363 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4702770c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4704000190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4701707c50>]}


============================== 18:24:59.196800 | ca07443c-4016-4432-8e0b-f4048daddaa6 ==============================
[0m18:24:59.196800 [info ] [MainThread]: Running with dbt=1.11.2
[0m18:24:59.197257 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'profiles_dir': '/home/ubuntu/.dbt', 'static_parser': 'True', 'printer_width': '80', 'empty': 'False', 'fail_fast': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'debug': 'False', 'version_check': 'True', 'log_cache_events': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run', 'target_path': 'None', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'no_print': 'None', 'introspect': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'write_json': 'True', 'log_format': 'default', 'partial_parse': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False'}
[0m18:24:59.835340 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m18:24:59.835703 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m18:24:59.835974 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m18:25:00.564221 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f47023f8fc0>]}
[0m18:25:00.606908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46dbd16360>]}
[0m18:25:00.607438 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m18:25:00.661923 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m18:25:00.662441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46dbee2250>]}
[0m18:25:00.669833 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m18:25:00.745415 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:25:00.745762 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m18:25:00.745979 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:25:00.778276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46db9bd310>]}
[0m18:25:00.838856 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m18:25:00.840620 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m18:25:00.848610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46dbd744b0>]}
[0m18:25:00.848957 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m18:25:00.849231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46db919f30>]}
[0m18:25:00.850626 [info ] [MainThread]: 
[0m18:25:00.850896 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:25:00.851123 [info ] [MainThread]: 
[0m18:25:00.851488 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m18:25:00.851685 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:25:00.856472 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m18:25:00.856907 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m18:25:00.867777 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m18:25:00.868179 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m18:25:00.868400 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:25:02.312825 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f211-6903-1af6-9afe-d67c3d0aceab) - Created
[0m18:25:08.879206 [debug] [ThreadPool]: SQL status: OK in 8.010 seconds
[0m18:25:08.892475 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f211-6903-1af6-9afe-d67c3d0aceab, command-id=01f0f211-694f-1d54-b7b8-a565a6227955) - Closing
[0m18:25:09.262937 [debug] [ThreadPool]: On list_dev: Close
[0m18:25:09.263380 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f211-6903-1af6-9afe-d67c3d0aceab) - Closing
[0m18:25:09.578617 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_dev_customer_bi_bronze) - Creating connection
[0m18:25:09.579339 [debug] [ThreadPool]: Acquiring new databricks connection 'create_dev_customer_bi_bronze'
[0m18:25:09.588895 [debug] [ThreadPool]: Creating schema "database: "dev"
schema: "customer_bi_bronze"
"
[0m18:25:09.594672 [debug] [ThreadPool]: Using databricks connection "create_dev_customer_bi_bronze"
[0m18:25:09.594961 [debug] [ThreadPool]: On create_dev_customer_bi_bronze: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "create_dev_customer_bi_bronze"} */
create schema if not exists `dev`.`customer_bi_bronze`
  
[0m18:25:09.595166 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:25:12.955953 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f211-6e55-193b-8609-f27751f13dc2) - Created
[0m18:25:13.989662 [debug] [ThreadPool]: SQL status: OK in 4.390 seconds
[0m18:25:13.993812 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f211-6e55-193b-8609-f27751f13dc2, command-id=01f0f211-6e8a-1cda-8fb4-bbb98fc7ef8b) - Closing
[0m18:25:13.994335 [debug] [ThreadPool]: On create_dev_customer_bi_bronze: Close
[0m18:25:13.994746 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f211-6e55-193b-8609-f27751f13dc2) - Closing
[0m18:25:14.277829 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_bronze) - Creating connection
[0m18:25:14.278512 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_bronze'
[0m18:25:14.283991 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_bronze"
[0m18:25:14.284531 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_bronze'

  
[0m18:25:14.284915 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:25:15.174891 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f211-6fb4-1dfd-b28c-ff2646829faa) - Created
[0m18:25:16.404504 [debug] [ThreadPool]: SQL status: OK in 2.120 seconds
[0m18:25:16.409609 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f211-6fb4-1dfd-b28c-ff2646829faa, command-id=01f0f211-6fdb-187a-8ef1-276596628b6f) - Closing
[0m18:25:16.410051 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: Close
[0m18:25:16.410342 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f211-6fb4-1dfd-b28c-ff2646829faa) - Closing
[0m18:25:16.684908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46dbad68d0>]}
[0m18:25:16.691845 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_customer
[0m18:25:16.692536 [info ] [Thread-4 (]: 1 of 5 START sql table model customer_bi_bronze.bronze_customer ................ [RUN]
[0m18:25:16.693625 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m18:25:16.693991 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m18:25:16.694322 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_customer
[0m18:25:16.700682 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m18:25:16.701383 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_customer
[0m18:25:16.716702 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m18:25:16.717348 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m18:25:16.717859 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46db98dd30>]}
[0m18:25:16.758021 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m18:25:16.758592 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m18:25:16.758999 [debug] [Thread-4 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_customer`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  
[0m18:25:16.759271 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m18:25:17.687974 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f211-713d-1bf0-81b9-f9ef03075085) - Created
[0m18:25:25.884896 [debug] [Thread-4 (]: SQL status: OK in 9.130 seconds
[0m18:25:25.886178 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f211-713d-1bf0-81b9-f9ef03075085, command-id=01f0f211-716e-10bf-a582-cde61238c529) - Closing
[0m18:25:26.180921 [debug] [Thread-4 (]: Applying tags to relation None
[0m18:25:26.191384 [debug] [Thread-4 (]: On model.customer_bi.bronze_customer: Close
[0m18:25:26.191707 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f211-713d-1bf0-81b9-f9ef03075085) - Closing
[0m18:25:26.501043 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4702582530>]}
[0m18:25:26.502021 [info ] [Thread-4 (]: 1 of 5 OK created sql table model customer_bi_bronze.bronze_customer ........... [[32mOK[0m in 9.80s]
[0m18:25:26.502945 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_customer
[0m18:25:26.503655 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_date
[0m18:25:26.504718 [info ] [Thread-4 (]: 2 of 5 START sql table model customer_bi_bronze.bronze_date .................... [RUN]
[0m18:25:26.505611 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m18:25:26.506114 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m18:25:26.506549 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_date
[0m18:25:26.509150 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m18:25:26.509750 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_date
[0m18:25:26.513736 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m18:25:26.515043 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m18:25:26.515482 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m18:25:26.515779 [debug] [Thread-4 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_date`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_date`
  
[0m18:25:26.515988 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m18:25:27.355074 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f211-778b-17ba-940d-593ded752287) - Created
[0m18:25:29.937623 [debug] [Thread-4 (]: SQL status: OK in 3.420 seconds
[0m18:25:29.939577 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f211-778b-17ba-940d-593ded752287, command-id=01f0f211-77b4-1d44-8666-186d8f7cdcf7) - Closing
[0m18:25:29.940723 [debug] [Thread-4 (]: Applying tags to relation None
[0m18:25:29.942251 [debug] [Thread-4 (]: On model.customer_bi.bronze_date: Close
[0m18:25:29.942675 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f211-778b-17ba-940d-593ded752287) - Closing
[0m18:25:30.201856 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46db922350>]}
[0m18:25:30.203508 [info ] [Thread-4 (]: 2 of 5 OK created sql table model customer_bi_bronze.bronze_date ............... [[32mOK[0m in 3.70s]
[0m18:25:30.204986 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_date
[0m18:25:30.205807 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_returns
[0m18:25:30.206710 [info ] [Thread-4 (]: 3 of 5 START sql table model customer_bi_bronze.bronze_returns ................. [RUN]
[0m18:25:30.207653 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m18:25:30.208180 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m18:25:30.208851 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_returns
[0m18:25:30.211265 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m18:25:30.211710 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_returns
[0m18:25:30.213762 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m18:25:30.215064 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m18:25:30.215429 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m18:25:30.215737 [debug] [Thread-4 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_returns`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  
[0m18:25:30.216043 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m18:25:31.079523 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f211-79fd-15a0-9d00-be6764e83494) - Created
[0m18:25:34.905930 [debug] [Thread-4 (]: SQL status: OK in 4.690 seconds
[0m18:25:34.907399 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f211-79fd-15a0-9d00-be6764e83494, command-id=01f0f211-7a31-12cb-a25e-c683e4048ad1) - Closing
[0m18:25:34.908264 [debug] [Thread-4 (]: Applying tags to relation None
[0m18:25:34.909689 [debug] [Thread-4 (]: On model.customer_bi.bronze_returns: Close
[0m18:25:34.909920 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f211-79fd-15a0-9d00-be6764e83494) - Closing
[0m18:25:35.223739 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46db796970>]}
[0m18:25:35.224873 [info ] [Thread-4 (]: 3 of 5 OK created sql table model customer_bi_bronze.bronze_returns ............ [[32mOK[0m in 5.02s]
[0m18:25:35.225764 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_returns
[0m18:25:35.226387 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_sales
[0m18:25:35.226948 [info ] [Thread-4 (]: 4 of 5 START sql table model customer_bi_bronze.bronze_sales ................... [RUN]
[0m18:25:35.227650 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m18:25:35.227977 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m18:25:35.228293 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_sales
[0m18:25:35.230742 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m18:25:35.231195 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_sales
[0m18:25:35.233006 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m18:25:35.234602 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m18:25:35.235084 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m18:25:35.235529 [debug] [Thread-4 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_sales`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  
[0m18:25:35.235881 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m18:25:36.077349 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f211-7d47-1630-9875-6d60885eaae7) - Created
[0m18:25:38.867895 [debug] [Thread-4 (]: SQL status: OK in 3.630 seconds
[0m18:25:38.868800 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f211-7d47-1630-9875-6d60885eaae7, command-id=01f0f211-7d7c-19bf-9c53-77a5e6bc3bba) - Closing
[0m18:25:38.869373 [debug] [Thread-4 (]: Applying tags to relation None
[0m18:25:38.870289 [debug] [Thread-4 (]: On model.customer_bi.bronze_sales: Close
[0m18:25:38.870569 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f211-7d47-1630-9875-6d60885eaae7) - Closing
[0m18:25:39.143355 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46db771cd0>]}
[0m18:25:39.143951 [info ] [Thread-4 (]: 4 of 5 OK created sql table model customer_bi_bronze.bronze_sales .............. [[32mOK[0m in 3.92s]
[0m18:25:39.144404 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_sales
[0m18:25:39.144708 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_store
[0m18:25:39.145142 [info ] [Thread-4 (]: 5 of 5 START sql table model customer_bi_bronze.bronze_store ................... [RUN]
[0m18:25:39.145603 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m18:25:39.145860 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m18:25:39.146108 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_store
[0m18:25:39.149464 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m18:25:39.150252 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_store
[0m18:25:39.151982 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m18:25:39.154125 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m18:25:39.154783 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m18:25:39.155132 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_store`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_store`
  
[0m18:25:39.155735 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m18:25:40.060737 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f211-7fcf-1cb7-840e-ad63d5cc435c) - Created
[0m18:25:42.560098 [debug] [Thread-4 (]: SQL status: OK in 3.400 seconds
[0m18:25:42.561350 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f211-7fcf-1cb7-840e-ad63d5cc435c, command-id=01f0f211-800b-10d9-a2b2-3f1e7a4a7477) - Closing
[0m18:25:42.562145 [debug] [Thread-4 (]: Applying tags to relation None
[0m18:25:42.563566 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: Close
[0m18:25:42.563854 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f211-7fcf-1cb7-840e-ad63d5cc435c) - Closing
[0m18:25:42.822049 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'ca07443c-4016-4432-8e0b-f4048daddaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46db771550>]}
[0m18:25:42.822650 [info ] [Thread-4 (]: 5 of 5 OK created sql table model customer_bi_bronze.bronze_store .............. [[32mOK[0m in 3.68s]
[0m18:25:42.823139 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_store
[0m18:25:42.824649 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m18:25:42.825055 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:25:42.825586 [info ] [MainThread]: 
[0m18:25:42.825899 [info ] [MainThread]: Finished running 5 table models in 0 hours 0 minutes and 41.97 seconds (41.97s).
[0m18:25:42.826779 [debug] [MainThread]: Command end result
[0m18:25:42.861903 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m18:25:42.864727 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m18:25:42.873349 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m18:25:42.874275 [info ] [MainThread]: 
[0m18:25:42.874768 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:25:42.875139 [info ] [MainThread]: 
[0m18:25:42.875588 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m18:25:42.876950 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 41.35192, "process_in_blocks": "126648", "process_kernel_time": 0.391461, "process_mem_max_rss": "262532", "process_out_blocks": "3448", "process_user_time": 3.438516}
[0m18:25:42.877647 [debug] [MainThread]: Command `dbt run` succeeded at 18:25:42.877541 after 41.35 seconds
[0m18:25:42.878111 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46dbc968d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4703a600b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f46dbeff830>]}
[0m18:25:42.878523 [debug] [MainThread]: Flushing usage events
[0m18:25:43.858357 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:26:45.439520 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd4784c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd6014190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd371bc50>]}


============================== 18:26:45.441898 | 9625ef62-ef7d-4df2-846b-53ed0b555d7c ==============================
[0m18:26:45.441898 [info ] [MainThread]: Running with dbt=1.11.2
[0m18:26:45.442325 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'quiet': 'False', 'version_check': 'True', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'warn_error': 'None', 'target_path': 'None', 'indirect_selection': 'eager', 'write_json': 'True', 'partial_parse': 'True', 'cache_selected_only': 'False', 'empty': 'False', 'profiles_dir': '/home/ubuntu/.dbt', 'invocation_command': 'dbt run', 'log_format': 'default', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'static_parser': 'True', 'introspect': 'True', 'printer_width': '80', 'use_experimental_parser': 'False', 'debug': 'False', 'fail_fast': 'False'}
[0m18:26:46.022722 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m18:26:46.023161 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m18:26:46.023469 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m18:26:46.488204 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd440cfc0>]}
[0m18:26:46.529166 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fadd46360>]}
[0m18:26:46.529720 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m18:26:46.583273 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m18:26:46.583755 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fadf12250>]}
[0m18:26:46.592513 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m18:26:46.667619 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:26:46.667951 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m18:26:46.668166 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:26:46.699117 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fad9ed310>]}
[0m18:26:46.763682 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m18:26:46.765450 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m18:26:46.771296 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fadd8c4b0>]}
[0m18:26:46.771664 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m18:26:46.771947 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fad935f30>]}
[0m18:26:46.773397 [info ] [MainThread]: 
[0m18:26:46.773685 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:26:46.773914 [info ] [MainThread]: 
[0m18:26:46.774311 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m18:26:46.774531 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:26:46.779207 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m18:26:46.779682 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m18:26:46.795182 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m18:26:46.795679 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m18:26:46.795936 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:26:47.675119 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f211-a812-1850-a85a-6a3d29ae29d2) - Created
[0m18:26:48.176754 [debug] [ThreadPool]: SQL status: OK in 1.380 seconds
[0m18:26:48.181340 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f211-a812-1850-a85a-6a3d29ae29d2, command-id=01f0f211-a83a-188c-a528-99ade2af7ba2) - Closing
[0m18:26:48.181829 [debug] [ThreadPool]: On list_dev: Close
[0m18:26:48.182124 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f211-a812-1850-a85a-6a3d29ae29d2) - Closing
[0m18:26:48.457573 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_bronze) - Creating connection
[0m18:26:48.458373 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_bronze'
[0m18:26:48.465900 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_bronze"
[0m18:26:48.466226 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_bronze'

  
[0m18:26:48.466491 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:26:49.395068 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f211-a920-1df3-8023-b30db3aacc3e) - Created
[0m18:26:50.159312 [debug] [ThreadPool]: SQL status: OK in 1.690 seconds
[0m18:26:50.162472 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f211-a920-1df3-8023-b30db3aacc3e, command-id=01f0f211-a950-1b21-9625-d46a3f627a32) - Closing
[0m18:26:50.163022 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: Close
[0m18:26:50.163294 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f211-a920-1df3-8023-b30db3aacc3e) - Closing
[0m18:26:50.446740 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fad9ea150>]}
[0m18:26:50.459013 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_customer
[0m18:26:50.459519 [info ] [Thread-3 (]: 1 of 5 START sql table model customer_bi_bronze.bronze_customer ................ [RUN]
[0m18:26:50.460063 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m18:26:50.460378 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m18:26:50.460645 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_customer
[0m18:26:50.465424 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m18:26:50.465932 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_customer
[0m18:26:50.480365 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:26:50.481035 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m18:26:50.481450 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fada01b20>]}
[0m18:26:50.513859 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m18:26:50.514353 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m18:26:50.514669 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_customer`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  
[0m18:26:50.514910 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:26:51.381161 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f211-aa66-11e9-aea1-310c4779240b) - Created
[0m18:26:53.534564 [debug] [Thread-3 (]: SQL status: OK in 3.020 seconds
[0m18:26:53.536418 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f211-aa66-11e9-aea1-310c4779240b, command-id=01f0f211-aa8e-1ee5-9f77-964279ad7474) - Closing
[0m18:26:53.546949 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:26:53.557024 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: Close
[0m18:26:53.557329 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f211-aa66-11e9-aea1-310c4779240b) - Closing
[0m18:26:53.860114 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd4592530>]}
[0m18:26:53.861542 [info ] [Thread-3 (]: 1 of 5 OK created sql table model customer_bi_bronze.bronze_customer ........... [[32mOK[0m in 3.40s]
[0m18:26:53.862702 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_customer
[0m18:26:53.863318 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_date
[0m18:26:53.863950 [info ] [Thread-3 (]: 2 of 5 START sql table model customer_bi_bronze.bronze_date .................... [RUN]
[0m18:26:53.864900 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m18:26:53.865346 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m18:26:53.865750 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_date
[0m18:26:53.869054 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m18:26:53.870575 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_date
[0m18:26:53.873035 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:26:53.875642 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m18:26:53.876241 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m18:26:53.876704 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_date`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_date`
  
[0m18:26:53.877157 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:26:57.167622 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f211-ac7c-16cf-8d34-613114c3185f) - Created
[0m18:26:59.393319 [debug] [Thread-3 (]: SQL status: OK in 5.520 seconds
[0m18:26:59.394319 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f211-ac7c-16cf-8d34-613114c3185f, command-id=01f0f211-aca5-1aed-9e4f-f7910158eca7) - Closing
[0m18:26:59.395100 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:26:59.396112 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: Close
[0m18:26:59.396380 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f211-ac7c-16cf-8d34-613114c3185f) - Closing
[0m18:26:59.682078 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fad968550>]}
[0m18:26:59.683743 [info ] [Thread-3 (]: 2 of 5 OK created sql table model customer_bi_bronze.bronze_date ............... [[32mOK[0m in 5.82s]
[0m18:26:59.685173 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_date
[0m18:26:59.686025 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_returns
[0m18:26:59.687026 [info ] [Thread-3 (]: 3 of 5 START sql table model customer_bi_bronze.bronze_returns ................. [RUN]
[0m18:26:59.687915 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m18:26:59.688456 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m18:26:59.688955 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_returns
[0m18:26:59.694191 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m18:26:59.695021 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_returns
[0m18:26:59.696396 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:26:59.697673 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m18:26:59.698009 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m18:26:59.698694 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_returns`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  
[0m18:26:59.699120 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:27:00.675212 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f211-ae94-1f05-ba15-6484df027043) - Created
[0m18:27:02.579855 [debug] [Thread-3 (]: SQL status: OK in 2.880 seconds
[0m18:27:02.581976 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f211-ae94-1f05-ba15-6484df027043, command-id=01f0f211-aec7-1469-a044-cb10e469cdd1) - Closing
[0m18:27:02.583873 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:27:02.585538 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: Close
[0m18:27:02.585809 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f211-ae94-1f05-ba15-6484df027043) - Closing
[0m18:27:02.866731 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fad7c3230>]}
[0m18:27:02.868156 [info ] [Thread-3 (]: 3 of 5 OK created sql table model customer_bi_bronze.bronze_returns ............ [[32mOK[0m in 3.18s]
[0m18:27:02.869466 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_returns
[0m18:27:02.870373 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_sales
[0m18:27:02.871243 [info ] [Thread-3 (]: 4 of 5 START sql table model customer_bi_bronze.bronze_sales ................... [RUN]
[0m18:27:02.872283 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m18:27:02.872946 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m18:27:02.873334 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_sales
[0m18:27:02.876365 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m18:27:02.877231 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_sales
[0m18:27:02.878922 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:27:02.880684 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m18:27:02.881123 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m18:27:02.881444 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_sales`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  
[0m18:27:02.881665 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:27:03.831106 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f211-b07f-1676-b3de-02dec86d86bf) - Created
[0m18:27:05.697260 [debug] [Thread-3 (]: SQL status: OK in 2.820 seconds
[0m18:27:05.698543 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f211-b07f-1676-b3de-02dec86d86bf, command-id=01f0f211-b0b8-114e-b73a-d6bca4192f49) - Closing
[0m18:27:05.699531 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:27:05.700679 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: Close
[0m18:27:05.701012 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f211-b07f-1676-b3de-02dec86d86bf) - Closing
[0m18:27:05.992608 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fad7a2b70>]}
[0m18:27:05.994132 [info ] [Thread-3 (]: 4 of 5 OK created sql table model customer_bi_bronze.bronze_sales .............. [[32mOK[0m in 3.12s]
[0m18:27:05.995690 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_sales
[0m18:27:05.996669 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_store
[0m18:27:05.997399 [info ] [Thread-3 (]: 5 of 5 START sql view model customer_bi_bronze.bronze_store .................... [RUN]
[0m18:27:05.998234 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m18:27:05.998790 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m18:27:05.999319 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_store
[0m18:27:06.001903 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m18:27:06.002563 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_store
[0m18:27:06.013187 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m18:27:06.017519 [debug] [Thread-3 (]: Dropping relation `dev`.`customer_bi_bronze`.`bronze_store` because it is of type table
[0m18:27:06.021334 [debug] [Thread-3 (]: Applying DROP to: `dev`.`customer_bi_bronze`.`bronze_store`
[0m18:27:06.024909 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m18:27:06.025203 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */
drop table if exists `dev`.`customer_bi_bronze`.`bronze_store`
[0m18:27:06.025541 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:27:06.878773 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f211-b278-1a26-84ee-61d38a1e8b67) - Created
[0m18:27:07.544574 [debug] [Thread-3 (]: SQL status: OK in 1.520 seconds
[0m18:27:07.546382 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f211-b278-1a26-84ee-61d38a1e8b67, command-id=01f0f211-b2ab-1085-87a4-b87342b60cc8) - Closing
[0m18:27:07.555057 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi_bronze`.`bronze_store`
[0m18:27:07.556226 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m18:27:07.556768 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m18:27:07.557052 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
  
  create or replace view `dev`.`customer_bi_bronze`.`bronze_store`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_store`
  )

[0m18:27:08.256354 [debug] [Thread-3 (]: SQL status: OK in 0.700 seconds
[0m18:27:08.258787 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f211-b278-1a26-84ee-61d38a1e8b67, command-id=01f0f211-b317-1de0-b19c-06be383cc2d9) - Closing
[0m18:27:08.260045 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:27:08.262000 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: Close
[0m18:27:08.262413 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f211-b278-1a26-84ee-61d38a1e8b67) - Closing
[0m18:27:08.539944 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9625ef62-ef7d-4df2-846b-53ed0b555d7c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fad7a17f0>]}
[0m18:27:08.541384 [info ] [Thread-3 (]: 5 of 5 OK created sql view model customer_bi_bronze.bronze_store ............... [[32mOK[0m in 2.54s]
[0m18:27:08.542619 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_store
[0m18:27:08.545187 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m18:27:08.545636 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:27:08.546303 [info ] [MainThread]: 
[0m18:27:08.546808 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 21.77 seconds (21.77s).
[0m18:27:08.548133 [debug] [MainThread]: Command end result
[0m18:27:08.579177 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m18:27:08.581200 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m18:27:08.585415 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m18:27:08.585698 [info ] [MainThread]: 
[0m18:27:08.586038 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:27:08.586269 [info ] [MainThread]: 
[0m18:27:08.586557 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m18:27:08.587191 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 20.78651, "process_in_blocks": "0", "process_kernel_time": 0.278173, "process_mem_max_rss": "265968", "process_out_blocks": "3440", "process_user_time": 3.220393}
[0m18:27:08.587594 [debug] [MainThread]: Command `dbt run` succeeded at 18:27:08.587497 after 20.79 seconds
[0m18:27:08.587905 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fd3e6ed50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fac668410>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f6fac669310>]}
[0m18:27:08.588161 [debug] [MainThread]: Flushing usage events
[0m18:27:09.579305 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:29:39.081981 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb07a11cc20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb07bb44190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb07926fc50>]}


============================== 18:29:39.084072 | 9b89418b-7397-4087-bd2f-cbe40edd97da ==============================
[0m18:29:39.084072 [info ] [MainThread]: Running with dbt=1.11.2
[0m18:29:39.084486 [debug] [MainThread]: running dbt with arguments {'no_print': 'None', 'log_cache_events': 'False', 'send_anonymous_usage_stats': 'True', 'log_format': 'default', 'indirect_selection': 'eager', 'invocation_command': 'dbt run', 'cache_selected_only': 'False', 'target_path': 'None', 'empty': 'False', 'partial_parse': 'True', 'use_experimental_parser': 'False', 'printer_width': '80', 'version_check': 'True', 'use_colors': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'fail_fast': 'False', 'quiet': 'False', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'write_json': 'True', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'warn_error': 'None', 'debug': 'False', 'introspect': 'True'}
[0m18:29:39.669909 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m18:29:39.670448 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m18:29:39.670799 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m18:29:40.025200 [warn ] [MainThread]: [[33mWARNING[0m][MissingPlusPrefixDeprecation]: Deprecated functionality
Missing '+' prefix on `schema` found at `customer_bi.bronze.schema` in file `/ho
me/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/dbt_project.yml`
. Hierarchical config values without a '+' prefix are deprecated in
dbt_project.yml.
[0m18:29:40.025617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb053777ce0>]}
[0m18:29:40.101910 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05364e360>]}
[0m18:29:40.149711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb053815950>]}
[0m18:29:40.150237 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m18:29:40.198332 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m18:29:40.198781 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0536419a0>]}
[0m18:29:40.205036 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m18:29:40.283345 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:29:40.283741 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m18:29:40.284069 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:29:40.314387 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb053568670>]}
[0m18:29:40.414561 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m18:29:40.416729 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m18:29:40.423035 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05324f5f0>]}
[0m18:29:40.423405 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m18:29:40.423692 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0532f0d10>]}
[0m18:29:40.425023 [info ] [MainThread]: 
[0m18:29:40.425297 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:29:40.425520 [info ] [MainThread]: 
[0m18:29:40.425899 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m18:29:40.426105 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:29:40.431136 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m18:29:40.431577 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m18:29:40.442078 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m18:29:40.442629 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m18:29:40.442953 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:29:41.340415 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f212-0f8f-1171-a2e8-bb400e03a72a) - Created
[0m18:29:41.816875 [debug] [ThreadPool]: SQL status: OK in 1.370 seconds
[0m18:29:41.821715 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f212-0f8f-1171-a2e8-bb400e03a72a, command-id=01f0f212-0fbb-1a39-9aba-47c0c1fe9c87) - Closing
[0m18:29:41.822278 [debug] [ThreadPool]: On list_dev: Close
[0m18:29:41.822607 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f212-0f8f-1171-a2e8-bb400e03a72a) - Closing
[0m18:29:42.117745 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_bronze) - Creating connection
[0m18:29:42.118596 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_bronze'
[0m18:29:42.125635 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_bronze"
[0m18:29:42.125917 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_bronze'

  
[0m18:29:42.126190 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:29:42.986700 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f212-109d-1f83-b72a-228f2e5d6f18) - Created
[0m18:29:43.566754 [debug] [ThreadPool]: SQL status: OK in 1.440 seconds
[0m18:29:43.570183 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f212-109d-1f83-b72a-228f2e5d6f18, command-id=01f0f212-10c5-11ef-8399-61a18b74102d) - Closing
[0m18:29:43.570659 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: Close
[0m18:29:43.570888 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f212-109d-1f83-b72a-228f2e5d6f18) - Closing
[0m18:29:43.845186 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0533252e0>]}
[0m18:29:43.850331 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_customer
[0m18:29:43.851122 [info ] [Thread-3 (]: 1 of 5 START sql table model customer_bi_bronze.bronze_customer ................ [RUN]
[0m18:29:43.852186 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m18:29:43.852696 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m18:29:43.852976 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_customer
[0m18:29:43.859394 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m18:29:43.860335 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_customer
[0m18:29:43.874845 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:29:43.875356 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m18:29:43.875790 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0530e0550>]}
[0m18:29:43.905129 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m18:29:43.905642 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m18:29:43.906048 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_customer`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  
[0m18:29:43.906430 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:29:44.782531 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-11bd-1abe-9639-453df32ca37c) - Created
[0m18:29:46.713616 [debug] [Thread-3 (]: SQL status: OK in 2.810 seconds
[0m18:29:46.715500 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f212-11bd-1abe-9639-453df32ca37c, command-id=01f0f212-11e7-1255-ad44-4d9835f9b0f4) - Closing
[0m18:29:46.726099 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:29:46.736313 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: Close
[0m18:29:46.736620 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-11bd-1abe-9639-453df32ca37c) - Closing
[0m18:29:47.013125 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb079f1f9d0>]}
[0m18:29:47.013917 [info ] [Thread-3 (]: 1 of 5 OK created sql table model customer_bi_bronze.bronze_customer ........... [[32mOK[0m in 3.16s]
[0m18:29:47.014488 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_customer
[0m18:29:47.014841 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_date
[0m18:29:47.015282 [info ] [Thread-3 (]: 2 of 5 START sql table model customer_bi_bronze.bronze_date .................... [RUN]
[0m18:29:47.015981 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m18:29:47.016344 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m18:29:47.016692 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_date
[0m18:29:47.019231 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m18:29:47.019680 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_date
[0m18:29:47.021580 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:29:47.023184 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m18:29:47.023655 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m18:29:47.024026 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_date`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_date`
  
[0m18:29:47.024286 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:29:50.343561 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-13b0-19d7-8fec-7ab482bcdd4f) - Created
[0m18:29:52.257018 [debug] [Thread-3 (]: SQL status: OK in 5.230 seconds
[0m18:29:52.258077 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f212-13b0-19d7-8fec-7ab482bcdd4f, command-id=01f0f212-13dd-104b-8bd8-61e3c5fe49b9) - Closing
[0m18:29:52.258993 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:29:52.260875 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: Close
[0m18:29:52.261442 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-13b0-19d7-8fec-7ab482bcdd4f) - Closing
[0m18:29:52.629100 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0530f2c10>]}
[0m18:29:52.629682 [info ] [Thread-3 (]: 2 of 5 OK created sql table model customer_bi_bronze.bronze_date ............... [[32mOK[0m in 5.61s]
[0m18:29:52.630324 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_date
[0m18:29:52.630658 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_returns
[0m18:29:52.631188 [info ] [Thread-3 (]: 3 of 5 START sql table model customer_bi_bronze.bronze_returns ................. [RUN]
[0m18:29:52.631641 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m18:29:52.631908 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m18:29:52.632151 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_returns
[0m18:29:52.636342 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m18:29:52.637073 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_returns
[0m18:29:52.638499 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:29:52.640135 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m18:29:52.640709 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m18:29:52.641081 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_returns`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  
[0m18:29:52.641577 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:29:53.529648 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-15b1-1b6d-adb0-67138c1f0aee) - Created
[0m18:29:55.346565 [debug] [Thread-3 (]: SQL status: OK in 2.700 seconds
[0m18:29:55.347297 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f212-15b1-1b6d-adb0-67138c1f0aee, command-id=01f0f212-15d4-1751-8613-fda5501d88ef) - Closing
[0m18:29:55.348191 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:29:55.349080 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: Close
[0m18:29:55.349353 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-15b1-1b6d-adb0-67138c1f0aee) - Closing
[0m18:29:55.616609 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb05357a210>]}
[0m18:29:55.617251 [info ] [Thread-3 (]: 3 of 5 OK created sql table model customer_bi_bronze.bronze_returns ............ [[32mOK[0m in 2.98s]
[0m18:29:55.617807 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_returns
[0m18:29:55.618207 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_sales
[0m18:29:55.618601 [info ] [Thread-3 (]: 4 of 5 START sql table model customer_bi_bronze.bronze_sales ................... [RUN]
[0m18:29:55.619158 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m18:29:55.619428 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m18:29:55.619694 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_sales
[0m18:29:55.621863 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m18:29:55.622637 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_sales
[0m18:29:55.624990 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:29:55.627469 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m18:29:55.628007 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m18:29:55.628333 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_sales`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  
[0m18:29:55.628598 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:29:56.491527 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-178f-1665-b73b-0410fe53546c) - Created
[0m18:29:58.363029 [debug] [Thread-3 (]: SQL status: OK in 2.730 seconds
[0m18:29:58.364251 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f212-178f-1665-b73b-0410fe53546c, command-id=01f0f212-17ba-1983-b339-07a1c97a2421) - Closing
[0m18:29:58.365295 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:29:58.366301 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: Close
[0m18:29:58.366535 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-178f-1665-b73b-0410fe53546c) - Closing
[0m18:29:58.643245 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb0530c7830>]}
[0m18:29:58.643824 [info ] [Thread-3 (]: 4 of 5 OK created sql table model customer_bi_bronze.bronze_sales .............. [[32mOK[0m in 3.02s]
[0m18:29:58.644261 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_sales
[0m18:29:58.644565 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_store
[0m18:29:58.644944 [info ] [Thread-3 (]: 5 of 5 START sql view model customer_bi_bronze.bronze_store .................... [RUN]
[0m18:29:58.645354 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m18:29:58.645606 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m18:29:58.645915 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_store
[0m18:29:58.648738 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m18:29:58.649250 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_store
[0m18:29:58.660160 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m18:29:58.670983 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi_bronze`.`bronze_store`
[0m18:29:58.671784 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m18:29:58.672187 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m18:29:58.672459 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
  
  create or replace view `dev`.`customer_bi_bronze`.`bronze_store`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_store`
  )

[0m18:29:58.672701 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:29:59.549706 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-1978-12f0-b218-472b7902af3d) - Created
[0m18:30:00.506896 [debug] [Thread-3 (]: SQL status: OK in 1.830 seconds
[0m18:30:00.508584 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f212-1978-12f0-b218-472b7902af3d, command-id=01f0f212-19a5-1fdb-8b7a-ab85f0186e81) - Closing
[0m18:30:00.509486 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:30:00.510423 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: Close
[0m18:30:00.510888 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-1978-12f0-b218-472b7902af3d) - Closing
[0m18:30:00.774318 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '9b89418b-7397-4087-bd2f-cbe40edd97da', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb053071490>]}
[0m18:30:00.775322 [info ] [Thread-3 (]: 5 of 5 OK created sql view model customer_bi_bronze.bronze_store ............... [[32mOK[0m in 2.13s]
[0m18:30:00.776091 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_store
[0m18:30:00.777985 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m18:30:00.778366 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:30:00.778899 [info ] [MainThread]: 
[0m18:30:00.779300 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 20.35 seconds (20.35s).
[0m18:30:00.780350 [debug] [MainThread]: Command end result
[0m18:30:00.805173 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m18:30:00.806940 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m18:30:00.811532 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m18:30:00.812086 [info ] [MainThread]: 
[0m18:30:00.812557 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:30:00.813078 [info ] [MainThread]: 
[0m18:30:00.813491 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m18:30:00.814042 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- MissingPlusPrefixDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m18:30:00.815970 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 19.333546, "process_in_blocks": "256", "process_kernel_time": 0.285307, "process_mem_max_rss": "265488", "process_out_blocks": "3440", "process_user_time": 3.176687}
[0m18:30:00.817901 [debug] [MainThread]: Command `dbt run` succeeded at 18:30:00.817510 after 19.34 seconds
[0m18:30:00.818596 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb07a516690>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb03a755fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fb03a7559d0>]}
[0m18:30:00.819084 [debug] [MainThread]: Flushing usage events
[0m18:30:01.803870 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:31:26.738912 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18cf8c4c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18d10e8190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18ce857c50>]}


============================== 18:31:26.741165 | 457c259d-2190-46f4-b7cc-d1b84a2f8aa0 ==============================
[0m18:31:26.741165 [info ] [MainThread]: Running with dbt=1.11.2
[0m18:31:26.741608 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'indirect_selection': 'eager', 'introspect': 'True', 'fail_fast': 'False', 'partial_parse': 'True', 'printer_width': '80', 'write_json': 'True', 'empty': 'False', 'invocation_command': 'dbt run', 'profiles_dir': '/home/ubuntu/.dbt', 'version_check': 'True', 'send_anonymous_usage_stats': 'True', 'quiet': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'no_print': 'None', 'warn_error': 'None', 'target_path': 'None', 'cache_selected_only': 'False', 'static_parser': 'True', 'debug': 'False', 'use_colors': 'True', 'use_experimental_parser': 'False', 'log_format': 'default', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])'}
[0m18:31:27.300584 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m18:31:27.300942 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m18:31:27.301198 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m18:31:27.724446 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18cf524fc0>]}
[0m18:31:27.763997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18ace72360>]}
[0m18:31:27.764566 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m18:31:27.812272 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m18:31:27.812739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18ace3e250>]}
[0m18:31:27.819051 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m18:31:27.892994 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:31:27.893349 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m18:31:27.893563 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:31:27.924114 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18ac92d310>]}
[0m18:31:27.989155 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m18:31:27.990741 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m18:31:27.996499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18aceb84b0>]}
[0m18:31:27.996900 [info ] [MainThread]: Found 5 models, 1 analysis, 6 sources, 731 macros
[0m18:31:27.997215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18ac901f30>]}
[0m18:31:27.998525 [info ] [MainThread]: 
[0m18:31:27.998818 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:31:27.999074 [info ] [MainThread]: 
[0m18:31:27.999502 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m18:31:27.999723 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:31:28.004911 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m18:31:28.005400 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m18:31:28.020790 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m18:31:28.021358 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m18:31:28.021628 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:31:28.886670 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f212-4fb5-10d8-988d-b148f0f02f27) - Created
[0m18:31:29.368852 [debug] [ThreadPool]: SQL status: OK in 1.350 seconds
[0m18:31:29.371475 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f212-4fb5-10d8-988d-b148f0f02f27, command-id=01f0f212-4fdf-1475-9dcb-4d6367581e77) - Closing
[0m18:31:29.371868 [debug] [ThreadPool]: On list_dev: Close
[0m18:31:29.372105 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f212-4fb5-10d8-988d-b148f0f02f27) - Closing
[0m18:31:29.658360 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_bronze) - Creating connection
[0m18:31:29.659050 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_bronze'
[0m18:31:29.665174 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_bronze"
[0m18:31:29.665515 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_bronze'

  
[0m18:31:29.665782 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:31:30.536590 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f212-50c0-1d8b-853c-732c99123551) - Created
[0m18:31:33.454927 [debug] [ThreadPool]: SQL status: OK in 3.790 seconds
[0m18:31:33.458057 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f212-50c0-1d8b-853c-732c99123551, command-id=01f0f212-50eb-1f0c-9197-8320811bf8a9) - Closing
[0m18:31:33.458717 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: Close
[0m18:31:33.459048 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f212-50c0-1d8b-853c-732c99123551) - Closing
[0m18:31:33.806082 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18ac97e990>]}
[0m18:31:33.809659 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_customer
[0m18:31:33.810470 [info ] [Thread-3 (]: 1 of 5 START sql table model customer_bi_bronze.bronze_customer ................ [RUN]
[0m18:31:33.811201 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m18:31:33.811598 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m18:31:33.811944 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_customer
[0m18:31:33.818716 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m18:31:33.821042 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_customer
[0m18:31:33.845458 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:31:33.846217 [warn ] [Thread-3 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m18:31:33.846730 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18aca4db20>]}
[0m18:31:33.891227 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m18:31:33.891900 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m18:31:33.892222 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_customer`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  
[0m18:31:33.892479 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:31:34.797222 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-5202-194f-9a4d-3834af311406) - Created
[0m18:31:36.842208 [debug] [Thread-3 (]: SQL status: OK in 2.950 seconds
[0m18:31:36.843040 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f212-5202-194f-9a4d-3834af311406, command-id=01f0f212-522d-189f-8ada-8041acfb6744) - Closing
[0m18:31:36.851351 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:31:36.865755 [debug] [Thread-3 (]: On model.customer_bi.bronze_customer: Close
[0m18:31:36.866147 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-5202-194f-9a4d-3834af311406) - Closing
[0m18:31:37.162966 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18cf6d6530>]}
[0m18:31:37.164218 [info ] [Thread-3 (]: 1 of 5 OK created sql table model customer_bi_bronze.bronze_customer ........... [[32mOK[0m in 3.35s]
[0m18:31:37.164881 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_customer
[0m18:31:37.165346 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_date
[0m18:31:37.165877 [info ] [Thread-3 (]: 2 of 5 START sql table model customer_bi_bronze.bronze_date .................... [RUN]
[0m18:31:37.166686 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m18:31:37.167083 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m18:31:37.167313 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_date
[0m18:31:37.169661 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m18:31:37.170136 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_date
[0m18:31:37.171347 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:31:37.172745 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m18:31:37.173227 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m18:31:37.173634 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_date`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_date`
  
[0m18:31:37.173886 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:31:38.041635 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-5414-1c51-9c5f-342d49812869) - Created
[0m18:31:39.921127 [debug] [Thread-3 (]: SQL status: OK in 2.750 seconds
[0m18:31:39.921943 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f212-5414-1c51-9c5f-342d49812869, command-id=01f0f212-543d-1eff-a284-35fcc22baed9) - Closing
[0m18:31:39.922621 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:31:39.923656 [debug] [Thread-3 (]: On model.customer_bi.bronze_date: Close
[0m18:31:39.923964 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-5414-1c51-9c5f-342d49812869) - Closing
[0m18:31:40.213269 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18acaa4d50>]}
[0m18:31:40.213848 [info ] [Thread-3 (]: 2 of 5 OK created sql table model customer_bi_bronze.bronze_date ............... [[32mOK[0m in 3.05s]
[0m18:31:40.214293 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_date
[0m18:31:40.214591 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_returns
[0m18:31:40.215044 [info ] [Thread-3 (]: 3 of 5 START sql table model customer_bi_bronze.bronze_returns ................. [RUN]
[0m18:31:40.215657 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m18:31:40.215988 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m18:31:40.216238 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_returns
[0m18:31:40.220644 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m18:31:40.221192 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_returns
[0m18:31:40.222522 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:31:40.223778 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m18:31:40.224159 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m18:31:40.224428 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_returns`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  
[0m18:31:40.224716 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:31:41.123022 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-560d-1be4-8ba4-02dd2b15dc61) - Created
[0m18:31:42.969486 [debug] [Thread-3 (]: SQL status: OK in 2.740 seconds
[0m18:31:42.970952 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f212-560d-1be4-8ba4-02dd2b15dc61, command-id=01f0f212-5636-1bdb-9c25-f440e74061b5) - Closing
[0m18:31:42.972745 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:31:42.974021 [debug] [Thread-3 (]: On model.customer_bi.bronze_returns: Close
[0m18:31:42.974316 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-560d-1be4-8ba4-02dd2b15dc61) - Closing
[0m18:31:43.267139 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18ac715f60>]}
[0m18:31:43.268394 [info ] [Thread-3 (]: 3 of 5 OK created sql table model customer_bi_bronze.bronze_returns ............ [[32mOK[0m in 3.05s]
[0m18:31:43.269471 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_returns
[0m18:31:43.270108 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_sales
[0m18:31:43.270630 [info ] [Thread-3 (]: 4 of 5 START sql table model customer_bi_bronze.bronze_sales ................... [RUN]
[0m18:31:43.271475 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m18:31:43.271950 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m18:31:43.272338 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_sales
[0m18:31:43.274451 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m18:31:43.274901 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_sales
[0m18:31:43.276573 [debug] [Thread-3 (]: MATERIALIZING TABLE
[0m18:31:43.278607 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m18:31:43.279152 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m18:31:43.279469 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_sales`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  
[0m18:31:43.279726 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:31:44.129725 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-57f5-1f9b-8c74-b6abc05ef9b9) - Created
[0m18:31:46.031317 [debug] [Thread-3 (]: SQL status: OK in 2.750 seconds
[0m18:31:46.032100 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f212-57f5-1f9b-8c74-b6abc05ef9b9, command-id=01f0f212-581e-135f-8bc6-f8d65e85b955) - Closing
[0m18:31:46.032801 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:31:46.033752 [debug] [Thread-3 (]: On model.customer_bi.bronze_sales: Close
[0m18:31:46.034011 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-57f5-1f9b-8c74-b6abc05ef9b9) - Closing
[0m18:31:46.312391 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18ac8e72f0>]}
[0m18:31:46.313245 [info ] [Thread-3 (]: 4 of 5 OK created sql table model customer_bi_bronze.bronze_sales .............. [[32mOK[0m in 3.04s]
[0m18:31:46.313801 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_sales
[0m18:31:46.314133 [debug] [Thread-3 (]: Began running node model.customer_bi.bronze_store
[0m18:31:46.314498 [info ] [Thread-3 (]: 5 of 5 START sql view model customer_bi_bronze.bronze_store .................... [RUN]
[0m18:31:46.315106 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m18:31:46.315460 [debug] [Thread-3 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m18:31:46.315735 [debug] [Thread-3 (]: Began compiling node model.customer_bi.bronze_store
[0m18:31:46.317667 [debug] [Thread-3 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m18:31:46.318043 [debug] [Thread-3 (]: Began executing node model.customer_bi.bronze_store
[0m18:31:46.327511 [debug] [Thread-3 (]: MATERIALIZING VIEW
[0m18:31:46.334860 [debug] [Thread-3 (]: Creating view `dev`.`customer_bi_bronze`.`bronze_store`
[0m18:31:46.335692 [debug] [Thread-3 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m18:31:46.336098 [debug] [Thread-3 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m18:31:46.336454 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
  
  create or replace view `dev`.`customer_bi_bronze`.`bronze_store`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_store`
  )

[0m18:31:46.336674 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m18:31:47.192949 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-59e9-10b5-b273-1ff3cd0a9232) - Created
[0m18:31:48.214608 [debug] [Thread-3 (]: SQL status: OK in 1.880 seconds
[0m18:31:48.215638 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f212-59e9-10b5-b273-1ff3cd0a9232, command-id=01f0f212-5a14-1f98-80a6-4c939f6f8fa7) - Closing
[0m18:31:48.216429 [debug] [Thread-3 (]: Applying tags to relation None
[0m18:31:48.217343 [debug] [Thread-3 (]: On model.customer_bi.bronze_store: Close
[0m18:31:48.217681 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f212-59e9-10b5-b273-1ff3cd0a9232) - Closing
[0m18:31:48.498718 [debug] [Thread-3 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '457c259d-2190-46f4-b7cc-d1b84a2f8aa0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18ac8e5790>]}
[0m18:31:48.500347 [info ] [Thread-3 (]: 5 of 5 OK created sql view model customer_bi_bronze.bronze_store ............... [[32mOK[0m in 2.18s]
[0m18:31:48.501820 [debug] [Thread-3 (]: Finished running node model.customer_bi.bronze_store
[0m18:31:48.504796 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m18:31:48.505217 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:31:48.505865 [info ] [MainThread]: 
[0m18:31:48.506296 [info ] [MainThread]: Finished running 4 table models, 1 view model in 0 hours 0 minutes and 20.51 seconds (20.51s).
[0m18:31:48.507571 [debug] [MainThread]: Command end result
[0m18:31:48.534022 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m18:31:48.535459 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m18:31:48.539259 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m18:31:48.539527 [info ] [MainThread]: 
[0m18:31:48.539844 [info ] [MainThread]: [32mCompleted successfully[0m
[0m18:31:48.540093 [info ] [MainThread]: 
[0m18:31:48.540345 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=5
[0m18:31:48.540893 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 19.550354, "process_in_blocks": "0", "process_kernel_time": 0.256348, "process_mem_max_rss": "264104", "process_out_blocks": "3440", "process_user_time": 3.106851}
[0m18:31:48.541213 [debug] [MainThread]: Command `dbt run` succeeded at 18:31:48.541134 after 19.55 seconds
[0m18:31:48.541467 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18cefac0b0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18ac7b3ef0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f18ac7b0ef0>]}
[0m18:31:48.541831 [debug] [MainThread]: Flushing usage events
[0m18:31:49.517307 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m18:47:36.590276 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8194998c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81961cc190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f819392fc50>]}


============================== 18:47:36.594011 | 1fa17df8-6370-47f8-bc27-aa1b82534815 ==============================
[0m18:47:36.594011 [info ] [MainThread]: Running with dbt=1.11.2
[0m18:47:36.594526 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'use_experimental_parser': 'False', 'version_check': 'True', 'log_format': 'default', 'use_colors': 'True', 'empty': 'False', 'static_parser': 'True', 'cache_selected_only': 'False', 'target_path': 'None', 'warn_error': 'None', 'no_print': 'None', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'introspect': 'True', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'quiet': 'False', 'indirect_selection': 'eager', 'debug': 'False', 'partial_parse': 'True', 'invocation_command': 'dbt run', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'printer_width': '80', 'fail_fast': 'False'}
[0m18:47:37.164837 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m18:47:37.165312 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m18:47:37.165678 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m18:47:37.584538 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8194600fc0>]}
[0m18:47:37.624284 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816ded6360>]}
[0m18:47:37.624882 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m18:47:37.680992 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m18:47:37.681529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816e09e250>]}
[0m18:47:37.689267 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m18:47:37.759052 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m18:47:37.759423 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m18:47:37.759645 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m18:47:37.791657 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816db8d310>]}
[0m18:47:37.854959 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m18:47:37.856987 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m18:47:37.862884 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816df350f0>]}
[0m18:47:37.863273 [info ] [MainThread]: Found 6 models, 1 analysis, 6 sources, 731 macros
[0m18:47:37.863573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816dad5f30>]}
[0m18:47:37.864934 [info ] [MainThread]: 
[0m18:47:37.865239 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m18:47:37.865468 [info ] [MainThread]: 
[0m18:47:37.865863 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m18:47:37.866092 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:47:37.870791 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m18:47:37.871316 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m18:47:37.881440 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m18:47:37.881782 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m18:47:37.881998 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:47:38.842783 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f214-91f7-1558-98d1-e1ef68dbc183) - Created
[0m18:47:39.422037 [debug] [ThreadPool]: SQL status: OK in 1.540 seconds
[0m18:47:39.427048 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f214-91f7-1558-98d1-e1ef68dbc183, command-id=01f0f214-922f-13e5-94c3-d32615a3a28e) - Closing
[0m18:47:39.427810 [debug] [ThreadPool]: On list_dev: Close
[0m18:47:39.428212 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f214-91f7-1558-98d1-e1ef68dbc183) - Closing
[0m18:47:42.254324 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m18:47:42.255543 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m18:47:42.259992 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m18:47:42.260348 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m18:47:42.260613 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:47:43.158451 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f214-931c-1037-a2a5-8896ff7b337a) - Created
[0m18:47:43.662609 [debug] [ThreadPool]: SQL status: OK in 1.400 seconds
[0m18:47:43.665984 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f214-931c-1037-a2a5-8896ff7b337a, command-id=01f0f214-9345-1d53-b93e-1861970c6f3c) - Closing
[0m18:47:43.666646 [debug] [ThreadPool]: On list_dev: Close
[0m18:47:43.667090 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f214-931c-1037-a2a5-8896ff7b337a) - Closing
[0m18:47:44.028615 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=create_dev_customer_bi_silver) - Creating connection
[0m18:47:44.029554 [debug] [ThreadPool]: Acquiring new databricks connection 'create_dev_customer_bi_silver'
[0m18:47:44.040913 [debug] [ThreadPool]: Creating schema "database: "dev"
schema: "customer_bi_silver"
"
[0m18:47:44.048351 [debug] [ThreadPool]: Using databricks connection "create_dev_customer_bi_silver"
[0m18:47:44.048825 [debug] [ThreadPool]: On create_dev_customer_bi_silver: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "create_dev_customer_bi_silver"} */
create schema if not exists `dev`.`customer_bi_silver`
  
[0m18:47:44.049100 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:47:45.033084 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f214-943c-1a02-96fd-d62a0ab45937) - Created
[0m18:47:45.768576 [debug] [ThreadPool]: SQL status: OK in 1.720 seconds
[0m18:47:45.770368 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f214-943c-1a02-96fd-d62a0ab45937, command-id=01f0f214-946e-130a-95f9-98110e7dac78) - Closing
[0m18:47:45.771036 [debug] [ThreadPool]: On create_dev_customer_bi_silver: Close
[0m18:47:45.771666 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f214-943c-1a02-96fd-d62a0ab45937) - Closing
[0m18:47:46.102056 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_bronze) - Creating connection
[0m18:47:46.103407 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_bronze'
[0m18:47:46.109670 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_bronze"
[0m18:47:46.109985 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_bronze'

  
[0m18:47:46.110208 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:47:47.025305 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f214-9573-1ccf-a68b-1ec2cccae237) - Created
[0m18:47:48.745302 [debug] [ThreadPool]: SQL status: OK in 2.640 seconds
[0m18:47:48.748742 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f214-9573-1ccf-a68b-1ec2cccae237, command-id=01f0f214-959e-1fee-8c9f-fa83d42de05c) - Closing
[0m18:47:48.749340 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: Close
[0m18:47:48.749610 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f214-9573-1ccf-a68b-1ec2cccae237) - Closing
[0m18:47:49.038349 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_silver) - Creating connection
[0m18:47:49.039012 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_silver'
[0m18:47:49.041610 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_silver"
[0m18:47:49.042193 [debug] [ThreadPool]: On list_dev_customer_bi_silver: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_silver'

  
[0m18:47:49.042546 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m18:47:49.952939 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f214-9736-15e2-8831-320b7148d611) - Created
[0m18:47:50.540422 [debug] [ThreadPool]: SQL status: OK in 1.500 seconds
[0m18:47:50.544087 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f214-9736-15e2-8831-320b7148d611, command-id=01f0f214-9761-1544-9b66-9acff4b6a42f) - Closing
[0m18:47:50.544740 [debug] [ThreadPool]: On list_dev_customer_bi_silver: Close
[0m18:47:50.545079 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f214-9736-15e2-8831-320b7148d611) - Closing
[0m18:47:50.806647 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816db7e210>]}
[0m18:47:50.821397 [debug] [Thread-6 (]: Began running node model.customer_bi.bronze_customer
[0m18:47:50.822143 [info ] [Thread-6 (]: 1 of 6 START sql table model customer_bi_bronze.bronze_customer ................ [RUN]
[0m18:47:50.822801 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_customer) - Creating connection
[0m18:47:50.823221 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.customer_bi.bronze_customer'
[0m18:47:50.823582 [debug] [Thread-6 (]: Began compiling node model.customer_bi.bronze_customer
[0m18:47:50.828635 [debug] [Thread-6 (]: Writing injected SQL for node "model.customer_bi.bronze_customer"
[0m18:47:50.829089 [debug] [Thread-6 (]: Began executing node model.customer_bi.bronze_customer
[0m18:47:50.841275 [debug] [Thread-6 (]: MATERIALIZING TABLE
[0m18:47:50.842391 [warn ] [Thread-6 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m18:47:50.843038 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816dba5de0>]}
[0m18:47:50.871577 [debug] [Thread-6 (]: Writing runtime sql for node "model.customer_bi.bronze_customer"
[0m18:47:50.872367 [debug] [Thread-6 (]: Using databricks connection "model.customer_bi.bronze_customer"
[0m18:47:50.872822 [debug] [Thread-6 (]: On model.customer_bi.bronze_customer: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_customer"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_customer`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_customer`
  
[0m18:47:50.873227 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:47:51.693728 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0f214-985c-15c9-b5d3-33e03b309e27) - Created
[0m18:47:55.548591 [debug] [Thread-6 (]: SQL status: OK in 4.680 seconds
[0m18:47:55.549828 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0f214-985c-15c9-b5d3-33e03b309e27, command-id=01f0f214-988d-1c93-a720-ff91adb202c5) - Closing
[0m18:47:55.560838 [debug] [Thread-6 (]: Applying tags to relation None
[0m18:47:55.570308 [debug] [Thread-6 (]: On model.customer_bi.bronze_customer: Close
[0m18:47:55.570606 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0f214-985c-15c9-b5d3-33e03b309e27) - Closing
[0m18:47:55.842276 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f81947a6530>]}
[0m18:47:55.843136 [info ] [Thread-6 (]: 1 of 6 OK created sql table model customer_bi_bronze.bronze_customer ........... [[32mOK[0m in 5.02s]
[0m18:47:55.843607 [debug] [Thread-6 (]: Finished running node model.customer_bi.bronze_customer
[0m18:47:55.843922 [debug] [Thread-6 (]: Began running node model.customer_bi.bronze_date
[0m18:47:55.844427 [info ] [Thread-6 (]: 2 of 6 START sql table model customer_bi_bronze.bronze_date .................... [RUN]
[0m18:47:55.844982 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_date) - Creating connection
[0m18:47:55.845219 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.customer_bi.bronze_date'
[0m18:47:55.845436 [debug] [Thread-6 (]: Began compiling node model.customer_bi.bronze_date
[0m18:47:55.847340 [debug] [Thread-6 (]: Writing injected SQL for node "model.customer_bi.bronze_date"
[0m18:47:55.847722 [debug] [Thread-6 (]: Began executing node model.customer_bi.bronze_date
[0m18:47:55.849179 [debug] [Thread-6 (]: MATERIALIZING TABLE
[0m18:47:55.850490 [debug] [Thread-6 (]: Writing runtime sql for node "model.customer_bi.bronze_date"
[0m18:47:55.850992 [debug] [Thread-6 (]: Using databricks connection "model.customer_bi.bronze_date"
[0m18:47:55.851320 [debug] [Thread-6 (]: On model.customer_bi.bronze_date: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_date"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_date`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_date`
  
[0m18:47:55.851656 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:47:56.710561 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0f214-9bb0-16d1-9d2f-d39ef5a5f03e) - Created
[0m18:47:59.208960 [debug] [Thread-6 (]: SQL status: OK in 3.360 seconds
[0m18:47:59.210417 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0f214-9bb0-16d1-9d2f-d39ef5a5f03e, command-id=01f0f214-9be0-18ca-9862-aa59b4ade74a) - Closing
[0m18:47:59.211949 [debug] [Thread-6 (]: Applying tags to relation None
[0m18:47:59.213316 [debug] [Thread-6 (]: On model.customer_bi.bronze_date: Close
[0m18:47:59.213652 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0f214-9bb0-16d1-9d2f-d39ef5a5f03e) - Closing
[0m18:47:59.483393 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816dc9f550>]}
[0m18:47:59.484715 [info ] [Thread-6 (]: 2 of 6 OK created sql table model customer_bi_bronze.bronze_date ............... [[32mOK[0m in 3.64s]
[0m18:47:59.485844 [debug] [Thread-6 (]: Finished running node model.customer_bi.bronze_date
[0m18:47:59.486638 [debug] [Thread-6 (]: Began running node model.customer_bi.bronze_returns
[0m18:47:59.487243 [info ] [Thread-6 (]: 3 of 6 START sql table model customer_bi_bronze.bronze_returns ................. [RUN]
[0m18:47:59.488301 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_returns) - Creating connection
[0m18:47:59.488791 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.customer_bi.bronze_returns'
[0m18:47:59.489221 [debug] [Thread-6 (]: Began compiling node model.customer_bi.bronze_returns
[0m18:47:59.492151 [debug] [Thread-6 (]: Writing injected SQL for node "model.customer_bi.bronze_returns"
[0m18:47:59.492805 [debug] [Thread-6 (]: Began executing node model.customer_bi.bronze_returns
[0m18:47:59.494937 [debug] [Thread-6 (]: MATERIALIZING TABLE
[0m18:47:59.496740 [debug] [Thread-6 (]: Writing runtime sql for node "model.customer_bi.bronze_returns"
[0m18:47:59.497281 [debug] [Thread-6 (]: Using databricks connection "model.customer_bi.bronze_returns"
[0m18:47:59.497718 [debug] [Thread-6 (]: On model.customer_bi.bronze_returns: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_returns"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_returns`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_returns`
  
[0m18:47:59.498067 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:48:00.346124 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0f214-9e1e-1909-8420-392a828e5a83) - Created
[0m18:48:02.582300 [debug] [Thread-6 (]: SQL status: OK in 3.080 seconds
[0m18:48:02.583896 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0f214-9e1e-1909-8420-392a828e5a83, command-id=01f0f214-9e4a-190b-b849-e65fc298c498) - Closing
[0m18:48:02.584977 [debug] [Thread-6 (]: Applying tags to relation None
[0m18:48:02.586364 [debug] [Thread-6 (]: On model.customer_bi.bronze_returns: Close
[0m18:48:02.586645 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0f214-9e1e-1909-8420-392a828e5a83) - Closing
[0m18:48:02.859663 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816d976eb0>]}
[0m18:48:02.860390 [info ] [Thread-6 (]: 3 of 6 OK created sql table model customer_bi_bronze.bronze_returns ............ [[32mOK[0m in 3.37s]
[0m18:48:02.860885 [debug] [Thread-6 (]: Finished running node model.customer_bi.bronze_returns
[0m18:48:02.861191 [debug] [Thread-6 (]: Began running node model.customer_bi.bronze_sales
[0m18:48:02.861534 [info ] [Thread-6 (]: 4 of 6 START sql table model customer_bi_bronze.bronze_sales ................... [RUN]
[0m18:48:02.862192 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_sales) - Creating connection
[0m18:48:02.862478 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.customer_bi.bronze_sales'
[0m18:48:02.862735 [debug] [Thread-6 (]: Began compiling node model.customer_bi.bronze_sales
[0m18:48:02.864946 [debug] [Thread-6 (]: Writing injected SQL for node "model.customer_bi.bronze_sales"
[0m18:48:02.865476 [debug] [Thread-6 (]: Began executing node model.customer_bi.bronze_sales
[0m18:48:02.866749 [debug] [Thread-6 (]: MATERIALIZING TABLE
[0m18:48:02.868126 [debug] [Thread-6 (]: Writing runtime sql for node "model.customer_bi.bronze_sales"
[0m18:48:02.868587 [debug] [Thread-6 (]: Using databricks connection "model.customer_bi.bronze_sales"
[0m18:48:02.868850 [debug] [Thread-6 (]: On model.customer_bi.bronze_sales: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_sales"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_sales`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`fact_sales`
  
[0m18:48:02.869095 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:48:03.710008 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0f214-a059-1124-8571-9eabd9a4adfb) - Created
[0m18:48:05.966984 [debug] [Thread-6 (]: SQL status: OK in 3.100 seconds
[0m18:48:05.968882 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0f214-a059-1124-8571-9eabd9a4adfb, command-id=01f0f214-a083-1a74-bc99-60ebbc816694) - Closing
[0m18:48:05.970298 [debug] [Thread-6 (]: Applying tags to relation None
[0m18:48:05.971617 [debug] [Thread-6 (]: On model.customer_bi.bronze_sales: Close
[0m18:48:05.971874 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0f214-a059-1124-8571-9eabd9a4adfb) - Closing
[0m18:48:06.240117 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816d94e1b0>]}
[0m18:48:06.241746 [info ] [Thread-6 (]: 4 of 6 OK created sql table model customer_bi_bronze.bronze_sales .............. [[32mOK[0m in 3.38s]
[0m18:48:06.242984 [debug] [Thread-6 (]: Finished running node model.customer_bi.bronze_sales
[0m18:48:06.243711 [debug] [Thread-6 (]: Began running node model.customer_bi.bronze_store
[0m18:48:06.244525 [info ] [Thread-6 (]: 5 of 6 START sql view model customer_bi_bronze.bronze_store .................... [RUN]
[0m18:48:06.245602 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m18:48:06.246095 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m18:48:06.246479 [debug] [Thread-6 (]: Began compiling node model.customer_bi.bronze_store
[0m18:48:06.249302 [debug] [Thread-6 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m18:48:06.249933 [debug] [Thread-6 (]: Began executing node model.customer_bi.bronze_store
[0m18:48:06.261805 [debug] [Thread-6 (]: MATERIALIZING VIEW
[0m18:48:06.274605 [debug] [Thread-6 (]: Creating view `dev`.`customer_bi_bronze`.`bronze_store`
[0m18:48:06.275364 [debug] [Thread-6 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m18:48:06.275869 [debug] [Thread-6 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m18:48:06.276228 [debug] [Thread-6 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
  
  create or replace view `dev`.`customer_bi_bronze`.`bronze_store`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_store`
  )

[0m18:48:06.276479 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:48:07.138917 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0f214-a293-15ad-879c-45a037e2db61) - Created
[0m18:48:08.228571 [debug] [Thread-6 (]: SQL status: OK in 1.950 seconds
[0m18:48:08.229515 [debug] [Thread-6 (]: Databricks adapter: Cursor(session-id=01f0f214-a293-15ad-879c-45a037e2db61, command-id=01f0f214-a2bd-1b10-b353-fa4353580427) - Closing
[0m18:48:08.230125 [debug] [Thread-6 (]: Applying tags to relation None
[0m18:48:08.230779 [debug] [Thread-6 (]: On model.customer_bi.bronze_store: Close
[0m18:48:08.231038 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0f214-a293-15ad-879c-45a037e2db61) - Closing
[0m18:48:08.495743 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816d94dd90>]}
[0m18:48:08.496359 [info ] [Thread-6 (]: 5 of 6 OK created sql view model customer_bi_bronze.bronze_store ............... [[32mOK[0m in 2.25s]
[0m18:48:08.496804 [debug] [Thread-6 (]: Finished running node model.customer_bi.bronze_store
[0m18:48:08.497099 [debug] [Thread-6 (]: Began running node model.customer_bi.customer_agg
[0m18:48:08.497477 [info ] [Thread-6 (]: 6 of 6 START sql table model customer_bi_silver.customer_agg ................... [RUN]
[0m18:48:08.497889 [debug] [Thread-6 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.customer_agg) - Creating connection
[0m18:48:08.498138 [debug] [Thread-6 (]: Acquiring new databricks connection 'model.customer_bi.customer_agg'
[0m18:48:08.498363 [debug] [Thread-6 (]: Began compiling node model.customer_bi.customer_agg
[0m18:48:08.500214 [debug] [Thread-6 (]: Writing injected SQL for node "model.customer_bi.customer_agg"
[0m18:48:08.500861 [debug] [Thread-6 (]: Began executing node model.customer_bi.customer_agg
[0m18:48:08.503836 [debug] [Thread-6 (]: MATERIALIZING TABLE
[0m18:48:08.505711 [debug] [Thread-6 (]: Writing runtime sql for node "model.customer_bi.customer_agg"
[0m18:48:08.506521 [debug] [Thread-6 (]: Using databricks connection "model.customer_bi.customer_agg"
[0m18:48:08.506826 [debug] [Thread-6 (]: On model.customer_bi.customer_agg: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.customer_agg"} */

  
    
        create or replace table `dev`.`customer_bi_silver`.`customer_agg`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT payment_method, sum(gross_amount) FROM   
`dev`.`source_db`.`fact_sales`
group by payment_method
  
[0m18:48:08.507059 [debug] [Thread-6 (]: Opening a new connection, currently in state init
[0m18:48:09.413676 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0f214-a3fe-13ce-be17-162d0ef90a01) - Created
[0m18:48:10.749942 [debug] [Thread-6 (]: Databricks adapter: Exception while trying to execute query
/* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.customer_agg"} */

  
    
        create or replace table `dev`.`customer_bi_silver`.`customer_agg`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT payment_method, sum(gross_amount) FROM   
`dev`.`source_db`.`fact_sales`
group by payment_method
  
: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema.
Invalid column names: sum(gross_amount).
Please use other characters and try again.
Alternatively, enable Column Mapping to keep using these characters.
Error properties: diagnostic-info=org.apache.hive.service.cli.HiveSQLException: Error running query: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema.
Invalid column names: sum(gross_amount).
Please use other characters and try again.
Alternatively, enable Column Mapping to keep using these characters.
	at org.apache.spark.sql.hive.thriftserver.HiveThriftServerErrors$.runningQueryError(HiveThriftServerErrors.scala:49)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:1050)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)
	at com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:787)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$5(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.SQLExecution$.withRootExecution(SQLExecution.scala:869)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:578)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:62)
	at com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:89)
	at com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$15(ThriftLocalProperties.scala:238)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags0(DatabricksSparkUsageLogger.scala:108)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.withAttributionTags(DatabricksSparkUsageLogger.scala:216)
	at com.databricks.spark.util.UsageLogging.$anonfun$withAttributionTags$1(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:780)
	at com.databricks.spark.util.UsageLogging$.withAttributionTags(UsageLogger.scala:789)
	at com.databricks.spark.util.UsageLogging.withAttributionTags(UsageLogger.scala:668)
	at com.databricks.spark.util.UsageLogging.withAttributionTags$(UsageLogger.scala:666)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withAttributionTags(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.$anonfun$withLocalProperties$12(ThriftLocalProperties.scala:233)
	at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties(ThriftLocalProperties.scala:229)
	at org.apache.spark.sql.hive.thriftserver.ThriftLocalProperties.withLocalProperties$(ThriftLocalProperties.scala:89)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:76)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:555)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:541)
	at java.base/java.security.AccessController.doPrivileged(AccessController.java:712)
	at java.base/javax.security.auth.Subject.doAs(Subject.java:439)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1953)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:591)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: com.databricks.sql.transaction.tahoe.DeltaAnalysisException: [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema.
Invalid column names: sum(gross_amount).
Please use other characters and try again.
Alternatively, enable Column Mapping to keep using these characters.
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2467)
	at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2466)
	at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:4111)
	at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1488)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:1108)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:1091)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:212)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:973)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:667)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:212)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:643)
	at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:636)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:212)
	at com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:151)
	at com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:92)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.updateMetadata(WriteIntoDeltaEdge.scala:120)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:411)
	at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:256)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:670)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:758)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:406)
	at com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:257)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:387)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$11(CreateDeltaTableCommand.scala:310)
	at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)
	at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:60)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$7(CreateDeltaTableCommand.scala:306)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:493)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:480)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:101)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:194)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:101)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:193)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
	at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
	at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
	at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
	at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
	at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
	at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
	at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
	at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
	at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
	at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
	at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
	at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
	at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:29)
	at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:104)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:194)
	at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
	at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
	at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:153)
	at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
	at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:101)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:192)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:182)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:171)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:101)
	at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:267)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:774)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:336)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1906)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)
	at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)
	at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1889)
	at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:351)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:841)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
	at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:77)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:841)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:828)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:845)
	at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:816)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:270)
	at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:343)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)
	at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)
	at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
	at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
	at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
	at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)
	at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)
	at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)
	at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
	at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)
	at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)
	at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)
	at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)
	at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:702)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:694)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:694)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:694)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)
	at scala.util.Try$.apply(Try.scala:217)
	at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
	at org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)
	at org.apache.spark.util.LazyTry.get(LazyTry.scala:75)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:489)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:431)
	at org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:411)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.getOrCreateDF(SparkExecuteStatementOperation.scala:674)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.analyzeQuery(SparkExecuteStatementOperation.scala:701)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$5(SparkExecuteStatementOperation.scala:847)
	at org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:583)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.$anonfun$execute$1(SparkExecuteStatementOperation.scala:847)
	... 53 more
	Suppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller
		at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames(DeltaErrors.scala:2467)
		at com.databricks.sql.transaction.tahoe.DeltaErrorsBase.foundInvalidCharsInColumnNames$(DeltaErrors.scala:2466)
		at com.databricks.sql.transaction.tahoe.DeltaErrors$.foundInvalidCharsInColumnNames(DeltaErrors.scala:4111)
		at com.databricks.sql.transaction.tahoe.schema.SchemaUtils$.checkSchemaFieldNames(SchemaUtils.scala:1488)
		at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata(OptimisticTransaction.scala:1108)
		at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.assertMetadata$(OptimisticTransaction.scala:1091)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.assertMetadata(OptimisticTransaction.scala:212)
		at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal(OptimisticTransaction.scala:973)
		at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadataInternal$(OptimisticTransaction.scala:667)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadataInternal(OptimisticTransaction.scala:212)
		at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata(OptimisticTransaction.scala:643)
		at com.databricks.sql.transaction.tahoe.OptimisticTransactionImpl.updateMetadata$(OptimisticTransaction.scala:636)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction.updateMetadata(OptimisticTransaction.scala:212)
		at com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata(ImplicitMetadataOperation.scala:151)
		at com.databricks.sql.transaction.tahoe.schema.ImplicitMetadataOperation.updateMetadata$(ImplicitMetadataOperation.scala:92)
		at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.updateMetadata(WriteIntoDeltaEdge.scala:120)
		at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitDataAndMaterializationPlans(WriteIntoDeltaEdge.scala:411)
		at com.databricks.sql.transaction.tahoe.commands.WriteIntoDeltaEdge.writeAndReturnCommitData(WriteIntoDeltaEdge.scala:256)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.doDeltaWrite$1(CreateDeltaTableCommand.scala:670)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCreateTableAsSelect(CreateDeltaTableCommand.scala:758)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$handleCommit$1(CreateDeltaTableCommand.scala:406)
		at com.databricks.sql.transaction.tahoe.OptimisticTransaction$.withActive(OptimisticTransaction.scala:257)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.handleCommit(CreateDeltaTableCommand.scala:387)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$11(CreateDeltaTableCommand.scala:310)
		at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf(SQLConfHelper.scala:75)
		at org.apache.spark.sql.catalyst.SQLConfHelper.withSQLConf$(SQLConfHelper.scala:38)
		at org.apache.spark.sql.catalyst.plans.QueryPlan.withSQLConf(QueryPlan.scala:60)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$7(CreateDeltaTableCommand.scala:306)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:493)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:480)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:101)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:194)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:101)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:193)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:510)
		at com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:616)
		at com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:643)
		at com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:80)
		at com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)
		at scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)
		at com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:78)
		at com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:75)
		at com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:29)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:127)
		at com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:108)
		at com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:29)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:611)
		at com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:519)
		at com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:29)
		at com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:511)
		at com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:475)
		at com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:29)
		at com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:104)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:194)
		at com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)
		at com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)
		at com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:153)
		at com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)
		at com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:101)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:192)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:182)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:171)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:101)
		at com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:267)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.$anonfun$createDeltaTable$1(DeltaCatalog.scala:774)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:336)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.$anonfun$commitStagedChanges$1(DeltaCatalog.scala:1906)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:586)
		at com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:584)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.recordFrameProfile(DeltaCatalog.scala:152)
		at com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:1889)
		at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.commitStagedChanges(DataSourceV2Utils.scala:351)
		at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$6(WriteToDataSourceV2Exec.scala:841)
		at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
		at org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec$.handleConcurrentCreateExceptions(WriteToDataSourceV2Exec.scala:77)
		at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$2(WriteToDataSourceV2Exec.scala:841)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1629)
		at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:828)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:845)
		at org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:816)
		at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:270)
		at org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:343)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$2(V2CommandExec.scala:48)
		at org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.$anonfun$result$1(V2CommandExec.scala:48)
		at com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)
		at com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:198)
		at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:47)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:45)
		at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:56)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:596)
		at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:596)
		at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:595)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$18(SQLExecution.scala:600)
		at com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$16(SQLExecution.scala:513)
		at org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:932)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:434)
		at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)
		at org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:124)
		at org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:118)
		at org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:123)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:434)
		at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:967)
		at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:433)
		at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:255)
		at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:885)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:591)
		at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1607)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:587)
		at org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:528)
		at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:585)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:702)
		at org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:694)
		at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:543)
		at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:543)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)
		at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:519)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:694)
		at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)
		at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:694)
		at org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:484)
		at scala.util.Try$.apply(Try.scala:217)
		at org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)
		at org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)
		at org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)
		... 62 more
, operation-id=01f0f214-a431-1038-a193-3eaca1c73006
[0m18:48:10.751515 [debug] [Thread-6 (]: On model.customer_bi.customer_agg: Close
[0m18:48:10.751865 [debug] [Thread-6 (]: Databricks adapter: Connection(session-id=01f0f214-a3fe-13ce-be17-162d0ef90a01) - Closing
[0m18:48:11.050976 [debug] [Thread-6 (]: Database Error in model customer_agg (models/silver/customer_agg.sql)
  [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema.
  Invalid column names: sum(gross_amount).
  Please use other characters and try again.
  Alternatively, enable Column Mapping to keep using these characters.
  compiled code at target/run/customer_bi/models/silver/customer_agg.sql
[0m18:48:11.051462 [debug] [Thread-6 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '1fa17df8-6370-47f8-bc27-aa1b82534815', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f815cf842f0>]}
[0m18:48:11.051945 [error] [Thread-6 (]: 6 of 6 ERROR creating sql table model customer_bi_silver.customer_agg .......... [[31mERROR[0m in 2.55s]
[0m18:48:11.052485 [debug] [Thread-6 (]: Finished running node model.customer_bi.customer_agg
[0m18:48:11.052987 [debug] [Thread-9 (]: Marking all children of 'model.customer_bi.customer_agg' to be skipped because of status 'error'.  Reason: Database Error in model customer_agg (models/silver/customer_agg.sql)
  [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema.
  Invalid column names: sum(gross_amount).
  Please use other characters and try again.
  Alternatively, enable Column Mapping to keep using these characters.
  compiled code at target/run/customer_bi/models/silver/customer_agg.sql.
[0m18:48:11.054655 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m18:48:11.054877 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m18:48:11.055250 [info ] [MainThread]: 
[0m18:48:11.055491 [info ] [MainThread]: Finished running 5 table models, 1 view model in 0 hours 0 minutes and 33.19 seconds (33.19s).
[0m18:48:11.056269 [debug] [MainThread]: Command end result
[0m18:48:11.081820 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m18:48:11.083931 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m18:48:11.088591 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m18:48:11.088892 [info ] [MainThread]: 
[0m18:48:11.089217 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m18:48:11.090095 [info ] [MainThread]: 
[0m18:48:11.090613 [error] [MainThread]: [31mFailure in model customer_agg (models/silver/customer_agg.sql)[0m
[0m18:48:11.091057 [error] [MainThread]:   Database Error in model customer_agg (models/silver/customer_agg.sql)
  [DELTA_INVALID_CHARACTERS_IN_COLUMN_NAMES] Found invalid character(s) among ' ,;{}()\n\t=' in the column names of your schema.
  Invalid column names: sum(gross_amount).
  Please use other characters and try again.
  Alternatively, enable Column Mapping to keep using these characters.
  compiled code at target/run/customer_bi/models/silver/customer_agg.sql
[0m18:48:11.091416 [info ] [MainThread]: 
[0m18:48:11.091991 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/silver/customer_agg.sql
[0m18:48:11.092506 [info ] [MainThread]: 
[0m18:48:11.092949 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=6
[0m18:48:11.093711 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": false, "command_wall_clock_time": 32.086697, "process_in_blocks": "4728", "process_kernel_time": 0.22215, "process_mem_max_rss": "265784", "process_out_blocks": "3560", "process_user_time": 3.23705}
[0m18:48:11.094137 [debug] [MainThread]: Command `dbt run` failed at 18:48:11.094041 after 32.09 seconds
[0m18:48:11.094465 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816d94ee70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f819408c2f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f816d982f30>]}
[0m18:48:11.094757 [debug] [MainThread]: Flushing usage events
[0m18:48:12.057369 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:41:26.585959 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe73a494c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe73bd24190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe739427c50>]}


============================== 10:41:26.589945 | 584312f1-e5cb-437d-9931-a36430a07eae ==============================
[0m10:41:26.589945 [info ] [MainThread]: Running with dbt=1.11.2
[0m10:41:26.590489 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'empty': 'False', 'printer_width': '80', 'quiet': 'False', 'warn_error': 'None', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'fail_fast': 'False', 'partial_parse': 'True', 'log_format': 'default', 'use_experimental_parser': 'False', 'invocation_command': 'dbt run --select bronze_store', 'version_check': 'True', 'introspect': 'True', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'use_colors': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'debug': 'False', 'static_parser': 'True', 'write_json': 'True', 'profiles_dir': '/home/ubuntu/.dbt'}
[0m10:41:27.466633 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m10:41:27.467214 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m10:41:27.467675 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m10:41:28.367339 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '584312f1-e5cb-437d-9931-a36430a07eae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe73a11cfc0>]}
[0m10:41:28.410734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '584312f1-e5cb-437d-9931-a36430a07eae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe713a56360>]}
[0m10:41:28.411477 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m10:41:28.500785 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m10:41:28.502212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '584312f1-e5cb-437d-9931-a36430a07eae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe713a22250>]}
[0m10:41:28.514252 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m10:41:28.623876 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key '-name' in "<unicode string>", line 9, column 5 in file
`models/bronze/properties.yml`
[0m10:41:28.624354 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '584312f1-e5cb-437d-9931-a36430a07eae', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe737294f50>]}
[0m10:41:28.630972 [warn ] [MainThread]: [[33mWARNING[0m][CustomTopLevelKeyDeprecation]: Deprecated functionality
Unexpected top-level key model in file `models/bronze/properties.yml`
[0m10:41:28.631511 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '584312f1-e5cb-437d-9931-a36430a07eae', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71396c9f0>]}
[0m10:41:28.639428 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m10:41:28.639820 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m10:41:28.640108 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m10:41:28.681430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '584312f1-e5cb-437d-9931-a36430a07eae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe713561230>]}
[0m10:41:28.759165 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m10:41:28.762036 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m10:41:28.773802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '584312f1-e5cb-437d-9931-a36430a07eae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe713498f50>]}
[0m10:41:28.774228 [info ] [MainThread]: Found 6 models, 1 analysis, 6 sources, 731 macros
[0m10:41:28.774539 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '584312f1-e5cb-437d-9931-a36430a07eae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe7134e0c00>]}
[0m10:41:28.775733 [info ] [MainThread]: 
[0m10:41:28.776115 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m10:41:28.776370 [info ] [MainThread]: 
[0m10:41:28.776850 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m10:41:28.777119 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m10:41:28.778015 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m10:41:28.778376 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m10:41:28.789751 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m10:41:28.790155 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m10:41:28.790421 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:41:31.105959 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f299-d1a7-1efc-8c71-38985a5ac26f) - Created
[0m10:41:37.858962 [debug] [ThreadPool]: SQL status: OK in 9.070 seconds
[0m10:41:37.876523 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f299-d1a7-1efc-8c71-38985a5ac26f, command-id=01f0f299-d1f3-160c-a32c-73d74b677a7c) - Closing
[0m10:41:38.318741 [debug] [ThreadPool]: On list_dev: Close
[0m10:41:38.319117 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f299-d1a7-1efc-8c71-38985a5ac26f) - Closing
[0m10:41:38.663035 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_silver) - Creating connection
[0m10:41:38.663607 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_silver'
[0m10:41:38.667921 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_silver"
[0m10:41:38.668240 [debug] [ThreadPool]: On list_dev_customer_bi_silver: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_silver'

  
[0m10:41:38.668525 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:41:39.641020 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f299-d6d0-1454-a458-52b0d4379a6e) - Created
[0m10:41:41.327446 [debug] [ThreadPool]: SQL status: OK in 2.660 seconds
[0m10:41:41.330919 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f299-d6d0-1454-a458-52b0d4379a6e, command-id=01f0f299-d6ff-1c84-82f1-9b6d6300f4e2) - Closing
[0m10:41:41.331422 [debug] [ThreadPool]: On list_dev_customer_bi_silver: Close
[0m10:41:41.331733 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f299-d6d0-1454-a458-52b0d4379a6e) - Closing
[0m10:41:41.628257 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_bronze) - Creating connection
[0m10:41:41.628625 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_bronze'
[0m10:41:41.630175 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_bronze"
[0m10:41:41.630488 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_bronze'

  
[0m10:41:41.630794 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m10:41:42.612043 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f299-d892-1fed-992c-303c0bd4b860) - Created
[0m10:41:44.049716 [debug] [ThreadPool]: SQL status: OK in 2.420 seconds
[0m10:41:44.052817 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f299-d892-1fed-992c-303c0bd4b860, command-id=01f0f299-d8c4-1788-bba4-3857d1f62596) - Closing
[0m10:41:44.053732 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: Close
[0m10:41:44.054246 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f299-d892-1fed-992c-303c0bd4b860) - Closing
[0m10:41:44.373573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '584312f1-e5cb-437d-9931-a36430a07eae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe73bce62b0>]}
[0m10:41:44.377951 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_store
[0m10:41:44.378457 [info ] [Thread-4 (]: 1 of 1 START sql view model customer_bi_bronze.bronze_store .................... [RUN]
[0m10:41:44.378959 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m10:41:44.379229 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m10:41:44.379480 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_store
[0m10:41:44.385961 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m10:41:44.387020 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_store
[0m10:41:44.401526 [debug] [Thread-4 (]: MATERIALIZING VIEW
[0m10:41:44.403365 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m10:41:44.403838 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '584312f1-e5cb-437d-9931-a36430a07eae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe713397050>]}
[0m10:41:44.414712 [debug] [Thread-4 (]: Creating view `dev`.`customer_bi_bronze`.`bronze_store`
[0m10:41:44.421934 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m10:41:44.422844 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m10:41:44.423234 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
  
  create or replace view `dev`.`customer_bi_bronze`.`bronze_store`
  
  as (
    SELECT * FROM   
`dev`.`source_db`.`dim_store`
  )

[0m10:41:44.423588 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m10:41:45.473976 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f299-da49-14b6-a0b5-cedfd602e3af) - Created
[0m10:41:47.639035 [debug] [Thread-4 (]: SQL status: OK in 3.220 seconds
[0m10:41:47.640004 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f299-da49-14b6-a0b5-cedfd602e3af, command-id=01f0f299-da78-1e42-9711-a301e6a712c1) - Closing
[0m10:41:47.647935 [debug] [Thread-4 (]: Applying tags to relation None
[0m10:41:47.649669 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: Close
[0m10:41:47.649990 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f299-da49-14b6-a0b5-cedfd602e3af) - Closing
[0m10:41:47.967256 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '584312f1-e5cb-437d-9931-a36430a07eae', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe713341cc0>]}
[0m10:41:47.968145 [info ] [Thread-4 (]: 1 of 1 OK created sql view model customer_bi_bronze.bronze_store ............... [[32mOK[0m in 3.58s]
[0m10:41:47.968800 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_store
[0m10:41:47.970527 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m10:41:47.970844 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m10:41:47.971344 [info ] [MainThread]: 
[0m10:41:47.971780 [info ] [MainThread]: Finished running 1 view model in 0 hours 0 minutes and 19.19 seconds (19.19s).
[0m10:41:47.972536 [debug] [MainThread]: Command end result
[0m10:41:48.008496 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m10:41:48.010868 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m10:41:48.016047 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m10:41:48.016406 [info ] [MainThread]: 
[0m10:41:48.016729 [info ] [MainThread]: [32mCompleted successfully[0m
[0m10:41:48.016979 [info ] [MainThread]: 
[0m10:41:48.017215 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m10:41:48.017713 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
- CustomTopLevelKeyDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m10:41:48.018566 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 21.4807, "process_in_blocks": "128472", "process_kernel_time": 0.599081, "process_mem_max_rss": "265572", "process_out_blocks": "3368", "process_user_time": 3.504628}
[0m10:41:48.019009 [debug] [MainThread]: Command `dbt run` succeeded at 10:41:48.018922 after 21.48 seconds
[0m10:41:48.019333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe73a1fa330>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe713a8e750>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe71368fcb0>]}
[0m10:41:48.019670 [debug] [MainThread]: Flushing usage events
[0m10:41:49.023347 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:39:40.749215 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a923b8c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a93c4c190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a9134fc50>]}


============================== 11:39:40.752399 | 8d72f4dd-e84e-41fb-b8ed-04adf241526c ==============================
[0m11:39:40.752399 [info ] [MainThread]: Running with dbt=1.11.2
[0m11:39:40.752895 [debug] [MainThread]: running dbt with arguments {'printer_width': '80', 'use_colors': 'True', 'fail_fast': 'False', 'no_print': 'None', 'partial_parse': 'True', 'warn_error': 'None', 'version_check': 'True', 'quiet': 'False', 'cache_selected_only': 'False', 'static_parser': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'write_json': 'True', 'invocation_command': 'dbt run --select bronze_store', 'log_format': 'default', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'target_path': 'None', 'indirect_selection': 'eager', 'log_cache_events': 'False', 'use_experimental_parser': 'False', 'debug': 'False', 'introspect': 'True'}
[0m11:39:41.353319 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:39:41.353788 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:39:41.354105 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:39:41.872280 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '8d72f4dd-e84e-41fb-b8ed-04adf241526c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a92040fc0>]}
[0m11:39:41.916277 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '8d72f4dd-e84e-41fb-b8ed-04adf241526c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a6b952360>]}
[0m11:39:41.916897 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m11:39:41.982781 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m11:39:41.983411 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '8d72f4dd-e84e-41fb-b8ed-04adf241526c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a6bb1a250>]}
[0m11:39:41.998089 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m11:39:42.075701 [warn ] [MainThread]: [[33mWARNING[0m][DuplicateYAMLKeysDeprecation]: Deprecated functionality
Duplicate key '-name' in "<unicode string>", line 9, column 5 in file
`models/bronze/properties.yml`
[0m11:39:42.076138 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '8d72f4dd-e84e-41fb-b8ed-04adf241526c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a8f368f50>]}
[0m11:39:42.080865 [warn ] [MainThread]: [[33mWARNING[0m][CustomTopLevelKeyDeprecation]: Deprecated functionality
Unexpected top-level key model in file `models/bronze/properties.yml`
[0m11:39:42.081321 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '8d72f4dd-e84e-41fb-b8ed-04adf241526c', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a6b86c9f0>]}
[0m11:39:42.090416 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:39:42.090987 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:39:42.091347 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:39:42.125955 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '8d72f4dd-e84e-41fb-b8ed-04adf241526c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a6b47d230>]}
[0m11:39:42.195198 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m11:39:42.197436 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m11:39:42.203802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '8d72f4dd-e84e-41fb-b8ed-04adf241526c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a6b3acf50>]}
[0m11:39:42.204205 [info ] [MainThread]: Found 6 models, 1 analysis, 6 sources, 731 macros
[0m11:39:42.204508 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8d72f4dd-e84e-41fb-b8ed-04adf241526c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a6b3f0c00>]}
[0m11:39:42.205680 [info ] [MainThread]: 
[0m11:39:42.206005 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:39:42.206278 [info ] [MainThread]: 
[0m11:39:42.206733 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m11:39:42.206957 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:39:42.207836 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m11:39:42.208080 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m11:39:42.218720 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m11:39:42.219109 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m11:39:42.219411 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:39:43.173206 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a1-f343-1088-bfc5-d0388b3cd43e) - Created
[0m11:39:43.843423 [debug] [ThreadPool]: SQL status: OK in 1.620 seconds
[0m11:39:43.848663 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2a1-f343-1088-bfc5-d0388b3cd43e, command-id=01f0f2a1-f36c-128b-b133-ac15304d94d1) - Closing
[0m11:39:43.849206 [debug] [ThreadPool]: On list_dev: Close
[0m11:39:43.849616 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a1-f343-1088-bfc5-d0388b3cd43e) - Closing
[0m11:39:44.162325 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_silver) - Creating connection
[0m11:39:44.162688 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_silver'
[0m11:39:44.166366 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_silver"
[0m11:39:44.166670 [debug] [ThreadPool]: On list_dev_customer_bi_silver: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_silver'

  
[0m11:39:44.166945 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:39:45.107331 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a1-f466-1d1d-b133-2a8b0969b7d9) - Created
[0m11:39:46.298894 [debug] [ThreadPool]: SQL status: OK in 2.130 seconds
[0m11:39:46.302719 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2a1-f466-1d1d-b133-2a8b0969b7d9, command-id=01f0f2a1-f495-100f-9fbe-743a0312f4a9) - Closing
[0m11:39:46.303172 [debug] [ThreadPool]: On list_dev_customer_bi_silver: Close
[0m11:39:46.303456 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a1-f466-1d1d-b133-2a8b0969b7d9) - Closing
[0m11:39:46.617113 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_bronze) - Creating connection
[0m11:39:46.617770 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_bronze'
[0m11:39:46.620522 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_bronze"
[0m11:39:46.621006 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_bronze'

  
[0m11:39:46.621296 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:39:47.587635 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a1-f5e3-1351-bf28-3ee7517e9e21) - Created
[0m11:39:48.819113 [debug] [ThreadPool]: SQL status: OK in 2.200 seconds
[0m11:39:48.822667 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2a1-f5e3-1351-bf28-3ee7517e9e21, command-id=01f0f2a1-f613-11eb-ac1e-628a92cb4f76) - Closing
[0m11:39:48.823571 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: Close
[0m11:39:48.824052 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a1-f5e3-1351-bf28-3ee7517e9e21) - Closing
[0m11:39:49.144873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '8d72f4dd-e84e-41fb-b8ed-04adf241526c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a93c0a2b0>]}
[0m11:39:49.154267 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_store
[0m11:39:49.155298 [info ] [Thread-4 (]: 1 of 1 START sql table model customer_bi_bronze.bronze_store ................... [RUN]
[0m11:39:49.156723 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m11:39:49.157286 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m11:39:49.157649 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_store
[0m11:39:49.164338 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m11:39:49.165217 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_store
[0m11:39:49.180460 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m11:39:49.181285 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m11:39:49.181760 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '8d72f4dd-e84e-41fb-b8ed-04adf241526c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a6b296450>]}
[0m11:39:49.188739 [debug] [Thread-4 (]: Applying DROP to: `dev`.`customer_bi_bronze`.`bronze_store`
[0m11:39:49.192560 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m11:39:49.192936 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */
DROP VIEW IF EXISTS `dev`.`customer_bi_bronze`.`bronze_store`
[0m11:39:49.193247 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:39:50.174244 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f2a1-f770-1c03-87c2-70abed2ff173) - Created
[0m11:39:51.161383 [debug] [Thread-4 (]: SQL status: OK in 1.970 seconds
[0m11:39:51.162874 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f2a1-f770-1c03-87c2-70abed2ff173, command-id=01f0f2a1-f79c-1242-a386-563122720bfe) - Closing
[0m11:39:51.194497 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m11:39:51.195015 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m11:39:51.195340 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_store`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_store`
  
[0m11:39:55.171558 [debug] [Thread-4 (]: SQL status: OK in 3.980 seconds
[0m11:39:55.172964 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f2a1-f770-1c03-87c2-70abed2ff173, command-id=01f0f2a1-f832-1fb0-b50d-e8ccf5e2bd09) - Closing
[0m11:39:55.183777 [debug] [Thread-4 (]: Applying tags to relation None
[0m11:39:55.195303 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: Close
[0m11:39:55.195662 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f2a1-f770-1c03-87c2-70abed2ff173) - Closing
[0m11:39:55.510896 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '8d72f4dd-e84e-41fb-b8ed-04adf241526c', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a6b26f230>]}
[0m11:39:55.511724 [info ] [Thread-4 (]: 1 of 1 OK created sql table model customer_bi_bronze.bronze_store .............. [[32mOK[0m in 6.34s]
[0m11:39:55.512385 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_store
[0m11:39:55.514616 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m11:39:55.514952 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:39:55.515333 [info ] [MainThread]: 
[0m11:39:55.515671 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 13.31 seconds (13.31s).
[0m11:39:55.516309 [debug] [MainThread]: Command end result
[0m11:39:55.550800 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m11:39:55.553199 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m11:39:55.557457 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m11:39:55.557753 [info ] [MainThread]: 
[0m11:39:55.558066 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:39:55.558338 [info ] [MainThread]: 
[0m11:39:55.558633 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m11:39:55.559135 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- DuplicateYAMLKeysDeprecation: 1 occurrence
- CustomTopLevelKeyDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m11:39:55.559895 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 14.86039, "process_in_blocks": "2424", "process_kernel_time": 0.291911, "process_mem_max_rss": "265512", "process_out_blocks": "3360", "process_user_time": 3.140568}
[0m11:39:55.560420 [debug] [MainThread]: Command `dbt run` succeeded at 11:39:55.560284 after 14.86 seconds
[0m11:39:55.560861 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a9211ac90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a6b403ad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5a6b37ab70>]}
[0m11:39:55.561173 [debug] [MainThread]: Flushing usage events
[0m11:39:56.647233 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:42:13.661401 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe5331b8c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe534bc0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe53214bc50>]}


============================== 11:42:13.666382 | c02b32ab-5be3-4f18-95f1-46fea016c9fe ==============================
[0m11:42:13.666382 [info ] [MainThread]: Running with dbt=1.11.2
[0m11:42:13.666848 [debug] [MainThread]: running dbt with arguments {'log_cache_events': 'False', 'debug': 'False', 'version_check': 'True', 'empty': 'False', 'send_anonymous_usage_stats': 'True', 'write_json': 'True', 'static_parser': 'True', 'use_experimental_parser': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'indirect_selection': 'eager', 'invocation_command': 'dbt run --select bronze_store', 'target_path': 'None', 'quiet': 'False', 'introspect': 'True', 'no_print': 'None', 'warn_error': 'None', 'fail_fast': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'printer_width': '80', 'cache_selected_only': 'False', 'partial_parse': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'use_colors': 'True', 'log_format': 'default'}
[0m11:42:14.443842 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:42:14.444277 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:42:14.444564 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:42:15.155823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'c02b32ab-5be3-4f18-95f1-46fea016c9fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe532e20fc0>]}
[0m11:42:15.199231 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'c02b32ab-5be3-4f18-95f1-46fea016c9fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe51073e360>]}
[0m11:42:15.199811 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m11:42:15.261121 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m11:42:15.261644 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'c02b32ab-5be3-4f18-95f1-46fea016c9fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe51070a250>]}
[0m11:42:15.268216 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m11:42:15.366350 [warn ] [MainThread]: [[33mWARNING[0m][CustomTopLevelKeyDeprecation]: Deprecated functionality
Unexpected top-level key model in file `models/bronze/properties.yml`
[0m11:42:15.366987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'c02b32ab-5be3-4f18-95f1-46fea016c9fe', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe5101fcd70>]}
[0m11:42:15.374200 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:42:15.374581 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:42:15.374904 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:42:15.414514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c02b32ab-5be3-4f18-95f1-46fea016c9fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe5106590f0>]}
[0m11:42:15.490220 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m11:42:15.492280 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m11:42:15.501566 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c02b32ab-5be3-4f18-95f1-46fea016c9fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe51014f5f0>]}
[0m11:42:15.501933 [info ] [MainThread]: Found 6 models, 1 analysis, 1 test, 6 sources, 731 macros
[0m11:42:15.502232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c02b32ab-5be3-4f18-95f1-46fea016c9fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe5101c5490>]}
[0m11:42:15.503438 [info ] [MainThread]: 
[0m11:42:15.503746 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:42:15.503990 [info ] [MainThread]: 
[0m11:42:15.504405 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m11:42:15.504651 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:42:15.505467 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m11:42:15.505828 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m11:42:15.516234 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m11:42:15.516607 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m11:42:15.516847 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:42:16.497775 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a2-4e92-1ab9-8afa-8561f015d263) - Created
[0m11:42:17.057009 [debug] [ThreadPool]: SQL status: OK in 1.540 seconds
[0m11:42:17.066071 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2a2-4e92-1ab9-8afa-8561f015d263, command-id=01f0f2a2-4ec1-1df0-a86d-55222d18f893) - Closing
[0m11:42:17.066639 [debug] [ThreadPool]: On list_dev: Close
[0m11:42:17.066964 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a2-4e92-1ab9-8afa-8561f015d263) - Closing
[0m11:42:17.365791 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_silver) - Creating connection
[0m11:42:17.366123 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_silver'
[0m11:42:17.369923 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_silver"
[0m11:42:17.370224 [debug] [ThreadPool]: On list_dev_customer_bi_silver: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_silver'

  
[0m11:42:17.370474 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:42:18.359021 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a2-4faf-1841-9536-71fceaf19aa2) - Created
[0m11:42:19.082462 [debug] [ThreadPool]: SQL status: OK in 1.710 seconds
[0m11:42:19.085813 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2a2-4faf-1841-9536-71fceaf19aa2, command-id=01f0f2a2-4fe1-17ed-b6f4-2f3077a412d7) - Closing
[0m11:42:19.086360 [debug] [ThreadPool]: On list_dev_customer_bi_silver: Close
[0m11:42:19.086787 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a2-4faf-1841-9536-71fceaf19aa2) - Closing
[0m11:42:19.401159 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_bronze) - Creating connection
[0m11:42:19.401594 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_bronze'
[0m11:42:19.403850 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_bronze"
[0m11:42:19.404269 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_bronze'

  
[0m11:42:19.404641 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:42:20.380940 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a2-50e3-1da1-8fe0-cdaf4968a50e) - Created
[0m11:42:21.056339 [debug] [ThreadPool]: SQL status: OK in 1.650 seconds
[0m11:42:21.058131 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2a2-50e3-1da1-8fe0-cdaf4968a50e, command-id=01f0f2a2-5118-1c14-a565-f4ca153bfe08) - Closing
[0m11:42:21.058717 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: Close
[0m11:42:21.059029 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a2-50e3-1da1-8fe0-cdaf4968a50e) - Closing
[0m11:42:21.343096 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c02b32ab-5be3-4f18-95f1-46fea016c9fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe5101c2af0>]}
[0m11:42:21.348877 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_store
[0m11:42:21.349472 [info ] [Thread-4 (]: 1 of 1 START sql table model customer_bi_bronze.bronze_store ................... [RUN]
[0m11:42:21.350169 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m11:42:21.350454 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m11:42:21.351464 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_store
[0m11:42:21.356925 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m11:42:21.357999 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_store
[0m11:42:21.374591 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m11:42:21.375144 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m11:42:21.375541 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'c02b32ab-5be3-4f18-95f1-46fea016c9fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe52f8e5630>]}
[0m11:42:21.412564 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m11:42:21.413184 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m11:42:21.413609 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_store`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_store`
  
[0m11:42:21.413923 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:42:22.358960 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f2a2-5218-155d-a69f-d141d3023948) - Created
[0m11:42:24.533630 [debug] [Thread-4 (]: SQL status: OK in 3.120 seconds
[0m11:42:24.535232 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f2a2-5218-155d-a69f-d141d3023948, command-id=01f0f2a2-5246-1b0e-a7fa-3e91dfda3e96) - Closing
[0m11:42:24.545776 [debug] [Thread-4 (]: Applying tags to relation None
[0m11:42:24.556553 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: Close
[0m11:42:24.556883 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f2a2-5218-155d-a69f-d141d3023948) - Closing
[0m11:42:24.859896 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c02b32ab-5be3-4f18-95f1-46fea016c9fe', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe532fc79d0>]}
[0m11:42:24.861288 [info ] [Thread-4 (]: 1 of 1 OK created sql table model customer_bi_bronze.bronze_store .............. [[32mOK[0m in 3.51s]
[0m11:42:24.862303 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_store
[0m11:42:24.864570 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m11:42:24.864960 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:42:24.865428 [info ] [MainThread]: 
[0m11:42:24.865839 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 9.36 seconds (9.36s).
[0m11:42:24.866458 [debug] [MainThread]: Command end result
[0m11:42:24.895676 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m11:42:24.897783 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m11:42:24.902099 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m11:42:24.902396 [info ] [MainThread]: 
[0m11:42:24.902709 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:42:24.902954 [info ] [MainThread]: 
[0m11:42:24.903202 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m11:42:24.903590 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- CustomTopLevelKeyDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m11:42:24.904214 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 11.288234, "process_in_blocks": "127088", "process_kernel_time": 0.420638, "process_mem_max_rss": "263496", "process_out_blocks": "3376", "process_user_time": 3.139399}
[0m11:42:24.904555 [debug] [MainThread]: Command `dbt run` succeeded at 11:42:24.904481 after 11.29 seconds
[0m11:42:24.904827 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe5321a4360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe535c6e430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe53343b3b0>]}
[0m11:42:24.905092 [debug] [MainThread]: Flushing usage events
[0m11:42:25.937394 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m11:48:54.613514 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe41d64c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe43578190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe40cf7c50>]}


============================== 11:48:54.617200 | e07f02ec-81ba-41bb-9403-dae5ac0b64c9 ==============================
[0m11:48:54.617200 [info ] [MainThread]: Running with dbt=1.11.2
[0m11:48:54.617839 [debug] [MainThread]: running dbt with arguments {'warn_error': 'None', 'version_check': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_format': 'default', 'printer_width': '80', 'cache_selected_only': 'False', 'no_print': 'None', 'use_experimental_parser': 'False', 'send_anonymous_usage_stats': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'target_path': 'None', 'quiet': 'False', 'use_colors': 'True', 'write_json': 'True', 'invocation_command': 'dbt run --select bronze_store', 'debug': 'False', 'log_cache_events': 'False', 'static_parser': 'True', 'indirect_selection': 'eager', 'partial_parse': 'True', 'fail_fast': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'empty': 'False', 'introspect': 'True'}
[0m11:48:55.318492 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m11:48:55.319025 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m11:48:55.319394 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m11:48:56.120953 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'e07f02ec-81ba-41bb-9403-dae5ac0b64c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe419e0fc0>]}
[0m11:48:56.164455 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'e07f02ec-81ba-41bb-9403-dae5ac0b64c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1b2ee360>]}
[0m11:48:56.165071 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m11:48:56.232245 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m11:48:56.232751 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e07f02ec-81ba-41bb-9403-dae5ac0b64c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1b2ba250>]}
[0m11:48:56.240501 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m11:48:56.340688 [warn ] [MainThread]: [[33mWARNING[0m][CustomTopLevelKeyDeprecation]: Deprecated functionality
Unexpected top-level key model in file `models/bronze/properties.yml`
[0m11:48:56.341113 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': 'e07f02ec-81ba-41bb-9403-dae5ac0b64c9', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1adacd70>]}
[0m11:48:56.348098 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:48:56.348476 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m11:48:56.348713 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:48:56.393888 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'e07f02ec-81ba-41bb-9403-dae5ac0b64c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1b2090f0>]}
[0m11:48:56.473799 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m11:48:56.475829 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m11:48:56.485752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'e07f02ec-81ba-41bb-9403-dae5ac0b64c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1acff5f0>]}
[0m11:48:56.486155 [info ] [MainThread]: Found 6 models, 1 analysis, 1 test, 6 sources, 731 macros
[0m11:48:56.486448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e07f02ec-81ba-41bb-9403-dae5ac0b64c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1ad69490>]}
[0m11:48:56.487624 [info ] [MainThread]: 
[0m11:48:56.487918 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m11:48:56.488153 [info ] [MainThread]: 
[0m11:48:56.488562 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m11:48:56.488796 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:48:56.489632 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m11:48:56.489910 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m11:48:56.501458 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m11:48:56.501890 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m11:48:56.502165 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:48:57.861462 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a3-3de3-1826-a848-260a6ed47a95) - Created
[0m11:48:58.613514 [debug] [ThreadPool]: SQL status: OK in 2.110 seconds
[0m11:48:58.621664 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2a3-3de3-1826-a848-260a6ed47a95, command-id=01f0f2a3-3e12-10f6-a9be-787d2f5b6777) - Closing
[0m11:48:58.622186 [debug] [ThreadPool]: On list_dev: Close
[0m11:48:58.622484 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a3-3de3-1826-a848-260a6ed47a95) - Closing
[0m11:48:58.965833 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_bronze) - Creating connection
[0m11:48:58.966215 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_bronze'
[0m11:48:58.970161 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_bronze"
[0m11:48:58.970523 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_bronze'

  
[0m11:48:58.970827 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:48:59.927200 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a3-3f15-1a6c-9ae7-33ad1e865e83) - Created
[0m11:49:00.761160 [debug] [ThreadPool]: SQL status: OK in 1.790 seconds
[0m11:49:00.765929 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2a3-3f15-1a6c-9ae7-33ad1e865e83, command-id=01f0f2a3-3f48-1606-9767-0c43734bf82c) - Closing
[0m11:49:00.766619 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: Close
[0m11:49:00.766964 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a3-3f15-1a6c-9ae7-33ad1e865e83) - Closing
[0m11:49:01.048748 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_silver) - Creating connection
[0m11:49:01.049838 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_silver'
[0m11:49:01.051336 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_silver"
[0m11:49:01.051643 [debug] [ThreadPool]: On list_dev_customer_bi_silver: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_silver'

  
[0m11:49:01.051900 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:49:01.991602 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a3-404c-1f18-a5e9-4364368885b9) - Created
[0m11:49:02.864496 [debug] [ThreadPool]: SQL status: OK in 1.810 seconds
[0m11:49:02.867239 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2a3-404c-1f18-a5e9-4364368885b9, command-id=01f0f2a3-4078-10c8-8b15-25ed58557cdd) - Closing
[0m11:49:02.867983 [debug] [ThreadPool]: On list_dev_customer_bi_silver: Close
[0m11:49:02.868408 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2a3-404c-1f18-a5e9-4364368885b9) - Closing
[0m11:49:03.243617 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'e07f02ec-81ba-41bb-9403-dae5ac0b64c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe1ad66af0>]}
[0m11:49:03.246718 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_store
[0m11:49:03.247180 [info ] [Thread-4 (]: 1 of 1 START sql table model customer_bi_bronze.bronze_store ................... [RUN]
[0m11:49:03.247683 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m11:49:03.247962 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m11:49:03.248888 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_store
[0m11:49:03.254999 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m11:49:03.255755 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_store
[0m11:49:03.273242 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m11:49:03.274145 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m11:49:03.274789 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'e07f02ec-81ba-41bb-9403-dae5ac0b64c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe3e491630>]}
[0m11:49:03.331132 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m11:49:03.332063 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m11:49:03.332604 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_store`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_store`
  
[0m11:49:03.333023 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m11:49:04.341472 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f2a3-41af-1e7b-903c-04924128010b) - Created
[0m11:49:06.500961 [debug] [Thread-4 (]: SQL status: OK in 3.170 seconds
[0m11:49:06.502581 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f2a3-41af-1e7b-903c-04924128010b, command-id=01f0f2a3-41dd-14ab-b8d3-0866dbf5165c) - Closing
[0m11:49:06.513203 [debug] [Thread-4 (]: Applying tags to relation None
[0m11:49:06.523727 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: Close
[0m11:49:06.524024 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f2a3-41af-1e7b-903c-04924128010b) - Closing
[0m11:49:06.833275 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'e07f02ec-81ba-41bb-9403-dae5ac0b64c9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe41b739d0>]}
[0m11:49:06.834751 [info ] [Thread-4 (]: 1 of 1 OK created sql table model customer_bi_bronze.bronze_store .............. [[32mOK[0m in 3.57s]
[0m11:49:06.835872 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_store
[0m11:49:06.838560 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m11:49:06.839091 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m11:49:06.839735 [info ] [MainThread]: 
[0m11:49:06.840230 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 10.35 seconds (10.35s).
[0m11:49:06.841140 [debug] [MainThread]: Command end result
[0m11:49:06.874119 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m11:49:06.876675 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m11:49:06.885228 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m11:49:06.885738 [info ] [MainThread]: 
[0m11:49:06.886239 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:49:06.886630 [info ] [MainThread]: 
[0m11:49:06.887123 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m11:49:06.887793 [warn ] [MainThread]: [[33mWARNING[0m][DeprecationsSummary]: Deprecated functionality
Summary of encountered deprecations:
- CustomTopLevelKeyDeprecation: 1 occurrence
To see all deprecation instances instead of just the first occurrence of each,
run command again with the `--show-all-deprecations` flag. You may also need to
run with `--no-partial-parse` as some deprecations are only encountered during
parsing.
[0m11:49:06.888623 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 12.324787, "process_in_blocks": "127080", "process_kernel_time": 0.394175, "process_mem_max_rss": "265452", "process_out_blocks": "3376", "process_user_time": 3.345687}
[0m11:49:06.889087 [debug] [MainThread]: Command `dbt run` succeeded at 11:49:06.888961 after 12.33 seconds
[0m11:49:06.889490 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe40d4c360>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe4480a430>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe3e48c470>]}
[0m11:49:06.889910 [debug] [MainThread]: Flushing usage events
[0m11:49:07.899387 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:43:27.042610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ed9adcc20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9edb360190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ed8a2fc50>]}


============================== 12:43:27.047157 | f6f4b69b-5790-4ff2-a7d5-388ad8203474 ==============================
[0m12:43:27.047157 [info ] [MainThread]: Running with dbt=1.11.2
[0m12:43:27.047620 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'printer_width': '80', 'write_json': 'True', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'version_check': 'True', 'fail_fast': 'False', 'quiet': 'False', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'empty': 'False', 'use_colors': 'True', 'profiles_dir': '/home/ubuntu/.dbt', 'use_experimental_parser': 'False', 'static_parser': 'True', 'log_format': 'default', 'warn_error': 'None', 'target_path': 'None', 'invocation_command': 'dbt run --select bronze_store', 'log_cache_events': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'debug': 'False', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'introspect': 'True'}
[0m12:43:27.901791 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:43:27.902192 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:43:27.902457 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:43:28.886484 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': 'f6f4b69b-5790-4ff2-a7d5-388ad8203474', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ed9758fc0>]}
[0m12:43:28.936107 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': 'f6f4b69b-5790-4ff2-a7d5-388ad8203474', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9eb30ae360>]}
[0m12:43:28.936704 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m12:43:29.012070 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:43:29.012749 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'f6f4b69b-5790-4ff2-a7d5-388ad8203474', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9eb307a250>]}
[0m12:43:29.022417 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m12:43:29.138162 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:43:29.138569 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m12:43:29.138817 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:43:29.180410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f6f4b69b-5790-4ff2-a7d5-388ad8203474', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9eb2b694f0>]}
[0m12:43:29.260690 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m12:43:29.262910 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m12:43:29.273181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f6f4b69b-5790-4ff2-a7d5-388ad8203474', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9eb310c4b0>]}
[0m12:43:29.273618 [info ] [MainThread]: Found 6 models, 1 analysis, 1 test, 6 sources, 731 macros
[0m12:43:29.273930 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6f4b69b-5790-4ff2-a7d5-388ad8203474', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9eb2abf5f0>]}
[0m12:43:29.275198 [info ] [MainThread]: 
[0m12:43:29.275504 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:43:29.275769 [info ] [MainThread]: 
[0m12:43:29.276185 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:43:29.276427 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:43:29.277276 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev) - Creating connection
[0m12:43:29.277611 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev'
[0m12:43:29.290987 [debug] [ThreadPool]: Using databricks connection "list_dev"
[0m12:43:29.291381 [debug] [ThreadPool]: On list_dev: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev"} */

    

  SHOW SCHEMAS IN `dev`


  
[0m12:43:29.291649 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:43:30.804397 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2aa-dc90-1e99-bfb3-95257e393941) - Created
[0m12:43:36.955902 [debug] [ThreadPool]: SQL status: OK in 7.660 seconds
[0m12:43:36.968476 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2aa-dc90-1e99-bfb3-95257e393941, command-id=01f0f2aa-dcdc-139f-9409-6f347cf8d1d3) - Closing
[0m12:43:37.314599 [debug] [ThreadPool]: On list_dev: Close
[0m12:43:37.315360 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2aa-dc90-1e99-bfb3-95257e393941) - Closing
[0m12:43:37.640542 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_silver) - Creating connection
[0m12:43:37.641006 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_silver'
[0m12:43:37.646913 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_silver"
[0m12:43:37.647295 [debug] [ThreadPool]: On list_dev_customer_bi_silver: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_silver'

  
[0m12:43:37.647579 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:43:38.577231 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2aa-e158-1ce7-bb45-7ab1c5a7775a) - Created
[0m12:43:39.887586 [debug] [ThreadPool]: SQL status: OK in 2.240 seconds
[0m12:43:39.890773 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2aa-e158-1ce7-bb45-7ab1c5a7775a, command-id=01f0f2aa-e184-1ff4-9c85-ea8c221e7bf6) - Closing
[0m12:43:39.891215 [debug] [ThreadPool]: On list_dev_customer_bi_silver: Close
[0m12:43:39.891520 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2aa-e158-1ce7-bb45-7ab1c5a7775a) - Closing
[0m12:43:40.223294 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_bronze) - Creating connection
[0m12:43:40.223769 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_bronze'
[0m12:43:40.225515 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_bronze"
[0m12:43:40.225866 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_bronze'

  
[0m12:43:40.226179 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:43:41.211086 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2aa-e2e9-112f-aafb-42dd39a44372) - Created
[0m12:43:43.253784 [debug] [ThreadPool]: SQL status: OK in 3.030 seconds
[0m12:43:43.255910 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2aa-e2e9-112f-aafb-42dd39a44372, command-id=01f0f2aa-e31a-17d6-9a2f-88d96bbca7bf) - Closing
[0m12:43:43.256588 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: Close
[0m12:43:43.256837 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2aa-e2e9-112f-aafb-42dd39a44372) - Closing
[0m12:43:43.539704 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f6f4b69b-5790-4ff2-a7d5-388ad8203474', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9eb2b23110>]}
[0m12:43:43.554589 [debug] [Thread-4 (]: Began running node model.customer_bi.bronze_store
[0m12:43:43.555329 [info ] [Thread-4 (]: 1 of 1 START sql table model customer_bi_bronze.bronze_store ................... [RUN]
[0m12:43:43.556751 [debug] [Thread-4 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=model.customer_bi.bronze_store) - Creating connection
[0m12:43:43.557023 [debug] [Thread-4 (]: Acquiring new databricks connection 'model.customer_bi.bronze_store'
[0m12:43:43.557318 [debug] [Thread-4 (]: Began compiling node model.customer_bi.bronze_store
[0m12:43:43.565299 [debug] [Thread-4 (]: Writing injected SQL for node "model.customer_bi.bronze_store"
[0m12:43:43.565964 [debug] [Thread-4 (]: Began executing node model.customer_bi.bronze_store
[0m12:43:43.587165 [debug] [Thread-4 (]: MATERIALIZING TABLE
[0m12:43:43.588124 [warn ] [Thread-4 (]: [[33mWARNING[0m]: Use revamped materializations based on separating create and insert.  This allows more performant column comments, as well as new column features.
You may opt into the new behavior sooner by setting `flags.use_materialization_v2` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:43:43.589003 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': 'f6f4b69b-5790-4ff2-a7d5-388ad8203474', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9eb2b1ee60>]}
[0m12:43:43.638093 [debug] [Thread-4 (]: Writing runtime sql for node "model.customer_bi.bronze_store"
[0m12:43:43.638666 [debug] [Thread-4 (]: Using databricks connection "model.customer_bi.bronze_store"
[0m12:43:43.639039 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "model.customer_bi.bronze_store"} */

  
    
        create or replace table `dev`.`customer_bi_bronze`.`bronze_store`
      
      
    using delta
  
      
      
      
      
      
      
      
      as
      SELECT * FROM   
`dev`.`source_db`.`dim_store`
  
[0m12:43:43.639313 [debug] [Thread-4 (]: Opening a new connection, currently in state init
[0m12:43:44.603399 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f2aa-e4f3-10ad-91eb-7fbb7792b31a) - Created
[0m12:43:52.524715 [debug] [Thread-4 (]: SQL status: OK in 8.890 seconds
[0m12:43:52.525969 [debug] [Thread-4 (]: Databricks adapter: Cursor(session-id=01f0f2aa-e4f3-10ad-91eb-7fbb7792b31a, command-id=01f0f2aa-e524-1722-a03c-efb4d7511640) - Closing
[0m12:43:52.862965 [debug] [Thread-4 (]: Applying tags to relation None
[0m12:43:52.876840 [debug] [Thread-4 (]: On model.customer_bi.bronze_store: Close
[0m12:43:52.877308 [debug] [Thread-4 (]: Databricks adapter: Connection(session-id=01f0f2aa-e4f3-10ad-91eb-7fbb7792b31a) - Closing
[0m12:43:53.178416 [debug] [Thread-4 (]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f6f4b69b-5790-4ff2-a7d5-388ad8203474', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ed98e2530>]}
[0m12:43:53.179363 [info ] [Thread-4 (]: 1 of 1 OK created sql table model customer_bi_bronze.bronze_store .............. [[32mOK[0m in 9.62s]
[0m12:43:53.179960 [debug] [Thread-4 (]: Finished running node model.customer_bi.bronze_store
[0m12:43:53.181756 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:43:53.182049 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:43:53.182422 [info ] [MainThread]: 
[0m12:43:53.182719 [info ] [MainThread]: Finished running 1 table model in 0 hours 0 minutes and 23.91 seconds (23.91s).
[0m12:43:53.183327 [debug] [MainThread]: Command end result
[0m12:43:53.211469 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m12:43:53.213872 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m12:43:53.219351 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m12:43:53.219701 [info ] [MainThread]: 
[0m12:43:53.220078 [info ] [MainThread]: [32mCompleted successfully[0m
[0m12:43:53.220342 [info ] [MainThread]: 
[0m12:43:53.220665 [info ] [MainThread]: Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1
[0m12:43:53.221462 [debug] [MainThread]: Resource report: {"command_name": "run", "command_success": true, "command_wall_clock_time": 25.809877, "process_in_blocks": "127080", "process_kernel_time": 0.50215, "process_mem_max_rss": "264912", "process_out_blocks": "3376", "process_user_time": 3.776587}
[0m12:43:53.222040 [debug] [MainThread]: Command `dbt run` succeeded at 12:43:53.221939 after 25.81 seconds
[0m12:43:53.222392 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9eb303fe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ed98ef550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f9ed8a84360>]}
[0m12:43:53.222662 [debug] [MainThread]: Flushing usage events
[0m12:43:54.014498 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m12:44:21.871823 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f35142a8c20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3515cd0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f35133fbc50>]}


============================== 12:44:21.874663 | 91f9c081-fd2b-4ad1-b01c-94ced989ad26 ==============================
[0m12:44:21.874663 [info ] [MainThread]: Running with dbt=1.11.2
[0m12:44:21.875101 [debug] [MainThread]: running dbt with arguments {'partial_parse': 'True', 'printer_width': '80', 'warn_error': 'None', 'fail_fast': 'False', 'use_experimental_parser': 'False', 'indirect_selection': 'eager', 'cache_selected_only': 'False', 'version_check': 'True', 'debug': 'False', 'introspect': 'True', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'quiet': 'False', 'log_format': 'default', 'static_parser': 'True', 'target_path': 'None', 'empty': 'None', 'send_anonymous_usage_stats': 'True', 'log_cache_events': 'False', 'no_print': 'None', 'write_json': 'True', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'profiles_dir': '/home/ubuntu/.dbt', 'invocation_command': 'dbt test -s bronze_store', 'use_colors': 'True'}
[0m12:44:22.676146 [debug] [MainThread]: Spark adapter: Setting pyhive.hive logging to ERROR
[0m12:44:22.676790 [debug] [MainThread]: Spark adapter: Setting thrift.transport logging to ERROR
[0m12:44:22.677269 [debug] [MainThread]: Spark adapter: Setting thrift.protocol logging to ERROR
[0m12:44:23.373529 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '91f9c081-fd2b-4ad1-b01c-94ced989ad26', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f3514100fc0>]}
[0m12:44:23.415756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'adapter_info', 'label': '91f9c081-fd2b-4ad1-b01c-94ced989ad26', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ed7fe360>]}
[0m12:44:23.416285 [info ] [MainThread]: Registered adapter: databricks=1.11.4
[0m12:44:23.477218 [warn ] [MainThread]: [[33mWARNING[0m]: Use managed Iceberg tables when table_format is iceberg. When this flag is disabled, UniForm is used instead.
You may opt into the new behavior sooner by setting `flags.use_managed_iceberg` to `True` in `dbt_project.yml`.
Visit https://docs.getdbt.com/reference/global-configs/behavior-changes for more information.
[0m12:44:23.477813 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'BehaviorChangeEvent', 'label': '91f9c081-fd2b-4ad1-b01c-94ced989ad26', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ed9ca250>]}
[0m12:44:23.485843 [debug] [MainThread]: checksum: 3d40d8bbe1bef07db1c24822f2dbfff8bc07f2a48ed4fe2658a40653aec0b54b, vars: {}, profile: , target: , version: 1.11.2
[0m12:44:23.577041 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:44:23.577376 [debug] [MainThread]: Nothing changed, skipping partial parsing.
[0m12:44:23.577594 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:44:23.617588 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '91f9c081-fd2b-4ad1-b01c-94ced989ad26', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ed4a95e0>]}
[0m12:44:23.727351 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m12:44:23.729021 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m12:44:23.743638 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '91f9c081-fd2b-4ad1-b01c-94ced989ad26', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ed2c2350>]}
[0m12:44:23.744148 [info ] [MainThread]: Found 6 models, 1 analysis, 1 test, 6 sources, 731 macros
[0m12:44:23.744477 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '91f9c081-fd2b-4ad1-b01c-94ced989ad26', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ed125570>]}
[0m12:44:23.745646 [info ] [MainThread]: 
[0m12:44:23.746024 [info ] [MainThread]: Concurrency: 1 threads (target='dev')
[0m12:44:23.746376 [info ] [MainThread]: 
[0m12:44:23.746844 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:44:23.747106 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:44:23.752624 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_bronze) - Creating connection
[0m12:44:23.753170 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_bronze'
[0m12:44:23.762790 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_bronze"
[0m12:44:23.763203 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_bronze"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_bronze'

  
[0m12:44:23.763501 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:44:24.684861 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2aa-fca7-1fc9-9287-a55897f0a883) - Created
[0m12:44:25.341380 [debug] [ThreadPool]: SQL status: OK in 1.580 seconds
[0m12:44:25.350419 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2aa-fca7-1fc9-9287-a55897f0a883, command-id=01f0f2aa-fcd2-1200-958c-38a1ad0fcef5) - Closing
[0m12:44:25.351393 [debug] [ThreadPool]: On list_dev_customer_bi_bronze: Close
[0m12:44:25.351794 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2aa-fca7-1fc9-9287-a55897f0a883) - Closing
[0m12:44:25.632311 [debug] [ThreadPool]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=list_dev_customer_bi_silver) - Creating connection
[0m12:44:25.632866 [debug] [ThreadPool]: Acquiring new databricks connection 'list_dev_customer_bi_silver'
[0m12:44:25.635408 [debug] [ThreadPool]: Using databricks connection "list_dev_customer_bi_silver"
[0m12:44:25.635859 [debug] [ThreadPool]: On list_dev_customer_bi_silver: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "connection_name": "list_dev_customer_bi_silver"} */

    
SELECT
  table_name,
  if(table_type IN ('EXTERNAL', 'MANAGED', 'MANAGED_SHALLOW_CLONE', 'EXTERNAL_SHALLOW_CLONE'), 'table', lower(table_type)) AS table_type,
  lower(data_source_format) AS file_format,
  table_owner,
  if(
    table_type IN (
      'EXTERNAL',
      'MANAGED',
      'MANAGED_SHALLOW_CLONE',
      'EXTERNAL_SHALLOW_CLONE'
    ),
    lower(table_type),
    NULL
  ) AS databricks_table_type
FROM `system`.`information_schema`.`tables`
WHERE table_catalog = 'dev' 
  AND table_schema = 'customer_bi_silver'

  
[0m12:44:25.636226 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:44:26.604853 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2aa-fdcf-1e8b-9e68-9becf38f2c82) - Created
[0m12:44:27.166052 [debug] [ThreadPool]: SQL status: OK in 1.530 seconds
[0m12:44:27.168450 [debug] [ThreadPool]: Databricks adapter: Cursor(session-id=01f0f2aa-fdcf-1e8b-9e68-9becf38f2c82, command-id=01f0f2aa-fdfe-1298-a71d-61ffaeed1454) - Closing
[0m12:44:27.168919 [debug] [ThreadPool]: On list_dev_customer_bi_silver: Close
[0m12:44:27.169224 [debug] [ThreadPool]: Databricks adapter: Connection(session-id=01f0f2aa-fdcf-1e8b-9e68-9becf38f2c82) - Closing
[0m12:44:27.457915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '91f9c081-fd2b-4ad1-b01c-94ced989ad26', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ed158950>]}
[0m12:44:27.462740 [debug] [Thread-3 (]: Began running node test.customer_bi.accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.51e99a2d4b
[0m12:44:27.463244 [info ] [Thread-3 (]: 1 of 1 START test accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL .. [RUN]
[0m12:44:27.463899 [debug] [Thread-3 (]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=test.customer_bi.accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.51e99a2d4b) - Creating connection
[0m12:44:27.464315 [debug] [Thread-3 (]: Acquiring new databricks connection 'test.customer_bi.accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.51e99a2d4b'
[0m12:44:27.464685 [debug] [Thread-3 (]: Began compiling node test.customer_bi.accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.51e99a2d4b
[0m12:44:27.476733 [debug] [Thread-3 (]: Writing injected SQL for node "test.customer_bi.accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.51e99a2d4b"
[0m12:44:27.489207 [debug] [Thread-3 (]: Began executing node test.customer_bi.accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.51e99a2d4b
[0m12:44:27.508730 [debug] [Thread-3 (]: Writing runtime sql for node "test.customer_bi.accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.51e99a2d4b"
[0m12:44:27.511899 [debug] [Thread-3 (]: Using databricks connection "test.customer_bi.accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.51e99a2d4b"
[0m12:44:27.512320 [debug] [Thread-3 (]: On test.customer_bi.accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.51e99a2d4b: /* {"app": "dbt", "dbt_version": "1.11.2", "dbt_databricks_version": "1.11.4", "databricks_sql_connector_version": "4.1.3", "profile_name": "customer_bi", "target_name": "dev", "node_id": "test.customer_bi.accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.51e99a2d4b"} */

    select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
  
    
    

with all_values as (

    select
        store_name as value_field,
        count(*) as n_records

    from `dev`.`customer_bi_bronze`.`bronze_store`
    group by store_name

)

select *
from all_values
where value_field not in (
    'NY','CA','TX','FL','IL'
)



  
  
      
    ) dbt_internal_test
[0m12:44:27.512559 [debug] [Thread-3 (]: Opening a new connection, currently in state init
[0m12:44:28.434054 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f2aa-feed-1de6-bd68-05055488c22b) - Created
[0m12:44:30.224202 [debug] [Thread-3 (]: SQL status: OK in 2.710 seconds
[0m12:44:30.241317 [debug] [Thread-3 (]: Databricks adapter: Cursor(session-id=01f0f2aa-feed-1de6-bd68-05055488c22b, command-id=01f0f2aa-ff1e-1d82-aed4-682a5240e9ca) - Closing
[0m12:44:30.244333 [debug] [Thread-3 (]: On test.customer_bi.accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.51e99a2d4b: Close
[0m12:44:30.244653 [debug] [Thread-3 (]: Databricks adapter: Connection(session-id=01f0f2aa-feed-1de6-bd68-05055488c22b) - Closing
[0m12:44:30.537092 [error] [Thread-3 (]: 1 of 1 FAIL 5 accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL ...... [[31mFAIL 5[0m in 3.07s]
[0m12:44:30.537679 [debug] [Thread-3 (]: Finished running node test.customer_bi.accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.51e99a2d4b
[0m12:44:30.539119 [debug] [MainThread]: Databricks adapter: DatabricksDBTConnection(session-id=None, name=master) - Creating connection
[0m12:44:30.539377 [debug] [MainThread]: Acquiring new databricks connection 'master'
[0m12:44:30.539699 [info ] [MainThread]: 
[0m12:44:30.539953 [info ] [MainThread]: Finished running 1 test in 0 hours 0 minutes and 6.79 seconds (6.79s).
[0m12:44:30.540420 [debug] [MainThread]: Command end result
[0m12:44:30.575002 [debug] [MainThread]: Wrote artifact WritableManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/manifest.json
[0m12:44:30.584024 [debug] [MainThread]: Wrote artifact SemanticManifest to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/semantic_manifest.json
[0m12:44:30.588854 [debug] [MainThread]: Wrote artifact RunExecutionResult to /home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/target/run_results.json
[0m12:44:30.589171 [info ] [MainThread]: 
[0m12:44:30.589594 [info ] [MainThread]: [31mCompleted with 1 error, 0 partial successes, and 0 warnings:[0m
[0m12:44:30.589978 [info ] [MainThread]: 
[0m12:44:30.590430 [error] [MainThread]: [31mFailure in test accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL (models/bronze/properties.yml)[0m
[0m12:44:30.590863 [error] [MainThread]:   Got 5 results, configured to fail if != 0
[0m12:44:30.591188 [info ] [MainThread]: 
[0m12:44:30.591615 [info ] [MainThread]:   compiled code at target/compiled/customer_bi/models/bronze/properties.yml/accepted_values_bronze_store_store_name__NY__CA__TX__FL__IL.sql
[0m12:44:30.591882 [info ] [MainThread]: 
[0m12:44:30.592157 [info ] [MainThread]: Done. PASS=0 WARN=0 ERROR=1 SKIP=0 NO-OP=0 TOTAL=1
[0m12:44:30.593022 [debug] [MainThread]: Resource report: {"command_name": "test", "command_success": false, "command_wall_clock_time": 8.773162, "process_in_blocks": "1800", "process_kernel_time": 0.391822, "process_mem_max_rss": "266300", "process_out_blocks": "3376", "process_user_time": 3.43598}
[0m12:44:30.593568 [debug] [MainThread]: Command `dbt test` failed at 12:44:30.593472 after 8.77 seconds
[0m12:44:30.593931 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ed2da0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ed1ae490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f34ed1ae7b0>]}
[0m12:44:30.594268 [debug] [MainThread]: Flushing usage events
[0m12:44:31.360469 [debug] [MainThread]: An error was encountered while trying to flush usage events
[0m10:48:07.741115 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8c33d5cad0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8c355d0190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8c32c9fc50>]}


============================== 10:48:07.745267 | 3f7cf17a-223c-48b3-b2a6-6d4bb66d7231 ==============================
[0m10:48:07.745267 [info ] [MainThread]: Running with dbt=1.11.2
[0m10:48:07.745953 [debug] [MainThread]: running dbt with arguments {'target_path': 'None', 'log_format': 'default', 'use_experimental_parser': 'False', 'static_parser': 'True', 'warn_error': 'None', 'write_json': 'True', 'introspect': 'True', 'use_colors': 'True', 'invocation_command': 'dbt ', 'fail_fast': 'False', 'log_path': '/home/ubuntu/dbt-projects/dbt-profiles/dbt_customer_bi/customer_bi/logs', 'warn_error_options': 'WarnErrorOptionsV2(error=[], warn=[], silence=[])', 'log_cache_events': 'False', 'quiet': 'False', 'partial_parse': 'True', 'version_check': 'True', 'indirect_selection': 'eager', 'profiles_dir': '/home/ubuntu/.dbt', 'empty': 'None', 'no_print': 'None', 'send_anonymous_usage_stats': 'True', 'cache_selected_only': 'False', 'printer_width': '80', 'debug': 'False'}
[0m10:48:07.883370 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'project_id', 'label': '3f7cf17a-223c-48b3-b2a6-6d4bb66d7231', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8c339b8fc0>]}
[0m10:48:07.895081 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:48:07.895999 [info ] [MainThread]: Warning: No packages were found in packages.yml
[0m10:48:07.896704 [debug] [MainThread]: Resource report: {"command_name": "deps", "command_success": true, "command_wall_clock_time": 0.21561281, "process_in_blocks": "528", "process_kernel_time": 0.130428, "process_mem_max_rss": "103088", "process_out_blocks": "8", "process_user_time": 1.193922}
[0m10:48:07.897114 [debug] [MainThread]: Command `cli deps` succeeded at 10:48:07.897014 after 0.22 seconds
[0m10:48:07.897474 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8c32c148d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8c32c14f30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8c32ef3e50>]}
[0m10:48:07.897771 [debug] [MainThread]: Flushing usage events
[0m10:48:08.765976 [debug] [MainThread]: An error was encountered while trying to flush usage events
